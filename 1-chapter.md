---
title: "Plain Text: The Poetics of Human-Computer Interaction (Sample Chapters)"
subtitle: "Chapter 2: Literature Down to the Pixel"
author: "Dennis Tenen"
style: csl/chicago-note.csl
bibliography: plain-text.bib
toc: true
documentclass: article
cover-image: images/steno.png
header-includes:
- \usepackage{ftnxtra}
- \usepackage{titlesec}
- \usepackage{fancyhdr}
- \pagestyle{fancy}
- \newcommand{\sectionbreak}{\clearpage}
- \rhead{DRAFT (please do not distribute)}
- \lhead{}

---

# Chapter 1: Literature Down to the Pixel

## 1.1 WYSINAWYG (What You See Is Not Always What You Get)

I do not trust the text appearing before my eyes. Wendy Hui Kyong Chun calls
magnetic storage the "enduring ephemeral," which "creates unforeseen
degenerative links between humans and machines" [@chun_enduring_2008, 148]. If
the floating gate transistor,[^ln1-gate] where my text now lives, can be called
the enduring ephemeral, I will call my liquid crystal display, where the text
shows itself, fading persistence. This is not to believe (fearing the wrath of
textual thinkers like Jerome McGann, Johanna Drucker, and Katherine Hayles)
that text on the screen lacks materiality. Not at all. Only that it seems to
drift from surface to surface, beyond the field of vision, in ways that erode
trust in the general permanence of the literary system. If we are destined to
dwell on surfaces, I do not know to which surface to attach my attention: the
screen or the floating gate.

I have before me at least two inscriptions: one as circuit state on my hard
drive, and another as crystal phase on the screen. They denote the same idea,
but in two distinct places. How did this duplicity come to be? And what impact
does it have on the life of a mind? In an attempt to answer, I offer the
following three historical preconditions for digital textuality. They are not
meant as history, but rather as signposts to mark the journey ahead. The schema
proceeds roughly as follows: first, content and control couple at the visible
surface of removable storage media (ticker tape and punch card); second,
inscription splits into input and output, retreating into magnetic storage to
reemerge at last on screen, in the same guise, but also, as we suspect, somehow
transformed: less solid and concealing something beneath the phantasmal shell
of its former appearance.[^ln1-denning] We will not travel the length of the
route in this chapter. Here we only need only to set the grounds that begin the
journey.

Where does text reside? If you are reading this book in print, you can point to
the page and be fairly certain of the answer: here, on the page. If you are
reading the book on a *device*, things get more complicated. You can point to
the screen and yet it would not be enough to say that the text resides there.
The screen will go dark when the device loses power, but the text will
persevere within, stored at another physical location. Deep within a device,
the text will be embedded into into some sort of a storage medium. Where the
digital text resides affects even those who prefer to read in print. Like most
literature today, print materials are also "born digital." At some point of
time, they occupy the same duplicitous position between storage medium and
pixel.

Few readers today will be able to give an account of a pixel's passage from
keystroke to ink or pixel. The vast machinery that brings literature to life
hides from the reader's view. Literature conceals. Even on the level of
stylistics, writing well is meant to evoke a sense of ease where it is usually
the product of painstaking labor. Let us hold on to that intuition for the
duration of this chapter: writing conceals labor.

Literature hides the labor of readers and writers. In another sense, there is
also the concealment of raw, physical energy that "goes into the word." The
work it takes to get a rock up a mountain gives it potential energy to roll
back down. By definition, potential energy relates to the possibility of "work
done." The rock can pull something up the hill as it is falling down, for
example. What of the energy that went into the writing of these sentences? I
spent a few *joules* compressing the springs under my keyboard. Many more went
into my laptop as electric current. Why? To overcome matter's passive impedance
of advancing change. To preserve and to communicate this message *amperes*, the
unit of electrical flow, struggled with *ohms*, the unit of electrical
resistance.

There is energy too in the librarian's effort to archive a book, in the
churning of a search engine to make it accessible to a reading public, and in
the work of a truck driver who ferries information by road. The sum of these
energies, which seem significant now in their accumulation, went into the
writing of these words. Much of it dissipated as heat and light. And some of
it---just a small part---left an imprint somewhere. It left a trace of ink on a
page, magnetized a piece of tape, or shifted the configuration of the floating
gate buried within the solid state drive. Finally, the potential energy
remaining within the word began a chain of electrical reactions in the reader's
brain. The mental effort or reading and writing amounts to "knowledge
work."[@liu_laws_2004] Ultimately, writing serves to connect the electrical
circuitry of at two neurological systems. Reading leaves the brain in a
different state than before. Writing, in this light, involves the transmission
of neural activity between hosts. When considered as a physical process,
literature is a type of electroencephalography (EEG), or electric head writing.

![The place of linguistic structure among "the facts of language". Saussure's communication circuit [@saussure_course_1986, 11].](images/saussure.jpg)

Ferdinand de Saussure's famous communication circuit (pictured here) follows
from head to head. The task of *Plain Text* will be to complicate that picture.
The book traces the passage of a thought into an *inscription* etched into a
*device*. At each stage, we will observe the process of encoding or decoding,
by which one type of substance---a thought---is converted into another type, a
word or the oscillation of an electrical current. I will present here a type of
a materialist reading practice, which pays attention to the physical properties
of the communication act. I will give that practice a name, microanalysis, and
a purpose: to uncover the energy gained or lost in the transformation from code
to code and from medium to medium. I am concerned with energy because too often
our foundational models of reading and interpretation take the text as a fixed
given. Even in our most destabilizing readings, like Jacques Derrida's "Plato's
Pharmacy", the materiality of the text is assumed to persevere. "We will take
off here from the *Phaedrus*," Derrida writes [@derrida_dissemination_1981,
66]. "The reader can count the lines" at the "precisely calculated center of
the dialogue" [@derrida_dissemination_1981, 67]. Plato's dialog has for Derrida
a definate beginning, middle, and end.  Derrida asks the reader "to return to
the text," even when the text "remains forever imepreceptible"
[@derrida_dissemination_1981, 63 & ].[^ln1-tranlsate] When Derrida talks in
this usual way of a literary scholar, he uses the word "text" in two distinct
ways: one, as an abstract metnal constract---the one that is forever
impreceptible, and the physical one, which has a definite and stable
topography.

In the vision of post-structural poetics, the text comprises quicksand of
unstable signifiers. In other words, we can only define a word with other
words. Derrida writes about the "dissimulation of the woven texture," and "a
web that envelops a web," and "an organism, idefinitely reenerating its own
tissue behind the cutting trace, the decision of each reading"
[@derrida_dissemination_1981, 63]. In *Plain Text*, I want to challenge the
stability of the text as a physical artifact, and not just a mental construct.
By the end of the book, we will see also that much is at stake in the material
affordances of the literary artifact. The political struggle for meaning-making,
that is the very opportunity to engage in the activity, begins with the text as
a physical object. 


[^ln1-translate]: "In our discussion of this text we have been using an
authoritative French translation of Plato, the one published by Guillaume Bude.
In the case of *Phaedrus*, the tranlation is by Leon Robin. We will continue to
refer to it, inserting Greek text in parenthesis [@derrida_dissemination_1981,
71]."

reveal the labor it
physically takes to ferry inscriptions across space. In searching for the
grounds of digital textuality, we will discover that the duplicity of text

### A. Removable storage media and automation, 1725--1964.[^ln3-loom]

"You must acknowledge that this is readable without special training," reads
the schematic illustration to a Goldberg 1911 patent, simply titled
"Controller." "My invention relates to all controllers," Goldberg writes.
Furthermore, the object of his invention is "to provide a mechanism operable by
a control sheet which is legible to every person having sufficient education to
enable him to read." Goldberg illustrates his invention in attaching to his
patent "a control sheet in which the control characters are in the form of the
letters of the ordinary English alphabet"  [@goldberg_controller_1915]. Rather
than using ticker tape, Goldberg uses perforations that form letters. On
Goldberg's control sheets, the language of machines and the language of humans
coincide.

![Goldberg's Control Cards [@goldberg_controller_1915].](images/control-2.png)

The Controller never caught on, but the patent makes it clear that Goldberg,
among others, was aware of the problem: the mechanization of type, automation,
and remote control required specialized training. With the advent of the
automated telegraph, content meant for people was now being intermixed with
machine-controlling code. To combat mutual unintelligibility, Goldberg imagines
using cards, perforated in the shape of the English alphabet. Besides carrying
(human-readable) content, the perforations do "double duty" to mechanically
manipulate the machine's "blocks," "handles," "terminal blades," and "plungers"
[@goldberg_controller_1915]. Early paper-based storage media, from Morse
code-based ticker tape systems, to the telegraphs of Hughes and Baudot, and to
punch cards that powered weaving looms, player pianos, and census tabulators
coupled message and control. The era of ticker tape punch cards can be thought
to end with the mass-market introduction of IBM's Magnetic Tape/Selectric
Typewriter in 1964.

### B: Magnetic tape, 1888--1968

"Historically unforeseen, barely a thing, software's ghostly presence produces
and defies apprehension," Wendy Chun writes in her *Programmed Visions*, an
influential monograph that continues to shape the field of software studies.
She quotes several prominent computer scientists and media historians to the
same effect. But what gives software its ephemeral quality? Embossed onto
ticker tape or punched into the card, early software protrudes through the
medium. In the age of the telegraph, the largest barrier to the comprehension
of software was encoding. But once the cipher is known and the format
identified, the inscription makes itself visible to view. Early programmable
media could hardly be called ephemeral or immaterial. Anecdotes circulate in
the digital humanities circles of Father Roberto Busa, an early (post-WWII)
pioneer in the field of computational philology, carting his punch cards around
Italy on a truck.[^ln3-busa] Code before its electromagnetic period was
burdensome, fragile, unwieldy, and, most of all, visible.

![IBM Mag Card II, introduced in 1969 for use in the Magnetic Card/Selectric
Typewriter (MC/ST) in 1969. "A simple relationship could be maintained between
a typed page and a recorded card" [@may_ibm_1981, 744]. Image by Pointillist
under GNU Free Documentation License, Version 1.2.](images/ibm-card.png)

The principles of magnetic recording were developed by Oberlin Smith (among
others), the American engineer who also filed several inventions related to
weaving looms at the end of the nineteenth century. In 1888, inspired by
Edison's mechanical phonograph, Smith made public his experiments with an
"electrical method" of sound recording using a "magnetized cord" (cotton mixed
with hardened steel dust) as a recording medium. These experiments were later
put into practice by Valdemar Poulsen of Denmark, who patented several
influential designs for a magnetic wire recorder [@smith_possible_1888;
@poulsen_method_1900; @engel_1888-1988_1988; @thiele_magnetic_1988;
@daniel_magnetic_1998; @vasic_coding_2004].

In 1964, IBM combined magnetic tape storage with its *Selectric* line of
electric typewriters, introducing the Magnetic Tape Selectric (MT/ST) line of
typewriters to the mass market. Writing for the *Encyclopedia of Library and
Information Science* in 1992, Daniel Eisenberg mentions the MT/ST as one of the
first word processors, defined by the ability to record strokes and to print
them onto paper *as a separate operation* [@eisenberg_word_1992]. The
separation of input and output allows for word processing as such. An article
in the *IBM Journal for Research and Development* explains that the real
significance of the MT/ST workstation was in the introduction of new "power
typing" technologies: "For the first time the typist could type at 'rough
draft' speed, 'backspace and strike over' errors, and not worry about the
pressure of mistakes made at the end of the page" [@may_ibm_1981, 742].

We may think of word processing as a temporal extension of the page. Words on
magnetic storage media begin to exist in the ephemeral state, giving the typist
an opportunity to edit and emend *before* commitment to paper, in its immutable
form, and as a separate operation, removed from the immediate process of
inscription. The very invention of word processing thus corresponds to the
decline of text into ephemera. What was visible through a hole punch on ticker
tape, was now submerged into tape. The tape no longer afforded human legibility
nor comprehension. Encoding used by MT/ST retained the familiar (from the
earlier sections) underlying structure (7-bit encoding, in this case) which, on
tape, ceased to be recoverable by the naked eye. The inscription lay literally
beyond (human) sense. We lack the perceptual apparatus to perceive "magnetic
domains" and "polarities"[^ln3-magnet] that take place of visible alphabets.
Magnetic storage remains, for all unassisted intents and purposes, a black
slate.

### C: Screen, 1968--today

By decoupling input and output, magnetic storage and solid state media afford
the injection of time and space, in arbitrary intervals, between the process of
inscription and comprehension. Content, coupled with control code, sinks
beneath the matte surface of electrical charge. The final movement in the
emergence of automated discourse reintroduces the illusion of immediacy into
the process of inscription. Text, invisible in its material substratum,
reappears on the screen, but, crucially, it no longer corresponds to its mirror
inscription. This property is as perilous as it is liberating: perilous,
because the flows of power and control can now be submerged under the
shimmering surface of the screen, and liberating, because loosely coupled to
their material substratum texts become both more fluid and more portable.
Plainly put, the systematic barriers to copying, sharing, exchanging, editing,
remixing, and disseminating texts are reduced to a minimum.

Ersatz isomorphism (between disk storage and screen image) leads to the
reception of digital text as an ephemeral artifact. With the illusory role of
the screen in mind, I propose 1968 as the year in which the contemporary
textual condition takes its present form. On December 9, 1968 Douglas
Engelbart, then founder and primary investigator at the NASA- and ARPA-funded
Augmentation Research Center lab at the Stanford Research Institute, gave what
later became known colloquially as "the mother of all demos
[@tweney_mother_2008]" before an audience of roughly one thousand or so
computer professionals attending the Fall Joint Computer Conference held at the
Convention Center in San Francisco [@rogers_demo_2005]. The demo announced the
arrival of almost every technology prophesied by Vannevar Bush in his
influential 1945 piece for *The Atlantic* [@bush_as_1945]. Speaking a little
over an hour,  through a headset, in a prerecorded address, Engelbart features
functional (live) prototypes of the following: graphical user interfaces, video
conferencing, remote camera monitoring, links and hypertext, version control,
text search, image manipulation, windows-based user interfaces, digital slides,
networked machines, mouse, stylus, and joystick inputs, and "what you see is
what you get" (WYSIWYG) word processing.

!["NOW IS THE TIME FOB." Schematics for a "display system"
[@engelbart_x-y_1970].](images/engel.png)

In his report to NASA, which sponsored research on "intellect augmentation"
along with DARPA, Engelbart describes his lab as a group of scientists
"developing an experimental laboratory around an interactive, multiconsole
computer-display system" and "working to learn the principles by which
interactive computer aids can augment the intellectual capability of the
subjects" [@engelbart_human_1969, 1]. Cathode Ray Tube (CRT) displays were
central to this research mission. In one of many patents that came out of
"intellect augmentation" laboratory, Engelbart pictures the "display system" as
a workstation that combines a typewriter, a CRT, and a mouse. The system is
frozen in mid-action, with the words "THE TIME IS NOW FOB" prominently
displayed on the screen. Although Engelbart does not explain the message, the
system's user is evidently in the process of editing a sentence and about to
correct the nonsensical FOB into a FOR. Engelbart writes, "One of the
potentially most promising means for delivering and receiving information to
and from digital computers involves the display of computer outputs as visual
representation on a cathode ray tube and the alternation of the display by
human operator in order to deliver instructions to the computer"
[@engelbart_x-y_1970].

The CRT closes the circuit between human and machine, with a few caveats. In
practice, the short-lived screen-less word processors (like the IBM MT/ST)
necessitated for the cognitively arduous task of continuously keeping the
underlying document structure in the mind's eye. The CRT lifts that burden by
unfolding the structure topographically, allowing for spatial navigation along
the document---restoring, in a sense, the natural affordances of print. Data
becomes visible again. Moreover, represented in the shimmer of the cathode ray,
it attaches itself lightly to the retina. Users trying out this way of writing
for the first time report that the screen liberates them from the material
confines of print textuality. One user, possibly Engelbart himself, writes the
following:[^ln3-follow]

[^ln3-follow]: I reproduce the text verbatim and preserving the line breaks,
since formatting is an important part of the reported experience.

```
    1B2B1 "To accommodate and preserve a thought or
    piece of information that isn't related to the work
    of the moment, one can very quickly and easily
    insert a note within the structure of a file at such
    a place that it will nether get in the way nor get
    lost.

    1B2B2 "Later, working in another part of the file,
    he can almost instantly (e.g. within two seconds)
    return to the place where he temporarily is storing
    such notes, to modify or add to any of them.

    1B2B3 "As any such miscellaneous thought develops,
    it is easy (and delightful) to reshape the structure
    and content of its discussion material.
```

Engelbart, interested in collecting empirical phenomenological accounts of the
system, records what must count as several of the most evocative passages to
appear on the pages of a NASA technical report. In the "Results and Discussion"
section an anonymous user continues to report:

```
1B4 "I find that I can express myself better, if I can
make all the little changes and experiments with wording
and structure as they occur to me." [Here the user
experiments a little with using structural decomposition
of a complex sentence.]
```

A deconstruction indeed follows, as the author begins to deviate from the
conventions of the technical report. The numbered passages, and unexpected
enjambment, heightens the staccato quality of the prose, which at times reaches
towards the lyric:


```
    1B4A "I find that I write faster and more freely,

        1B4A1 "pouring thoughts and trial words on the
        screen with much less inhibition,

        1B4A2 "finding it easy to repair mistakes or wrong
        choices

            1B4A22 "so while capturing a thought I don't
            have to inhibit the outpouring of thought and
            action to do it with particular correctness,

        1B4A3 "finding that several trials at the right
        wording can be done very quickly

            1B4A3A "so I can experiment, easily take a look
            and see how a new version strikes me--and often
            the first unworried attempt at a way to express
            something turn out to be satisfactory, or at
            least to require only minor touch up.

        1B4A4 "Finding that where I might otherwise
        hesitate in search of the right word, I now pour out
        a succession of potentially appropriate words,
        leaving them all the while the rest of the
        statement takes shape. Then I select from among
        them, or replace them all, or else merely change the
        list a it and wait for a later movement of the
        spirit.
```

When input and output coincide in time, as they do on paper, mistakes can be
costly. The writer must commit to making an inscription and, once made, the
inscription gains permanence in a way that is difficult to correct. One can
erase, removing a layer of physical material, or cover up, adding a layer of
white ink to repair the damage. Engelbart's anonymous writer reports a feeling
of freedom from such commitment to physical medium. He or she can simply
"backspace" and start over. The contemporary reader may take such things for
granted now, but imagine trying to write in that way for the first time.
Writing "comes easy," becomes "uninhibited," and it "pours out" experimentally.
Rather than manipulate language mentally, the writer "pours" the words onto the
screen and then "selects" the right one, without hesitation. The highly
hierarchical and blocky paragraph structure, along with its repetitive refrain,
"finding" and "I find that," gives the prose a hypnotic drive forward, which
matches the reported experience of liberation. Anonymous continues:

```
    1B4B "I find that,

        1B4B1 "being much more aware of

            1B4B1A "the relationships among the phrases of a
            sentence,

            1B4B1B "among the statements of a list,

            1B4B1C "and among the various level and members
            of a branch,

        1B4B2 "being able

            1B4B2A "to view them in different ways,

            1B4B2B "to rearrange them easily,

            1B4B2C "to experiment with certain special
            portrayals,

                1B4B2C1 "not available easily in unstructured data

                1B4B2C2 "or usable without the CRT display,

        1B4B3 "and being aware that

            1B4B3A "I can (and am seeking to) develop still
            further special conventions and computer aids

            1B4B3B "to make even more of this available and
            easy,

        1B4B4 "all tend to increase

            1B4B4A "my interest and experimentation

            1B4B4B "and my conviction that this is but a
            peek at what is to come soon.
```

The passages are too contrived to be spontaneous admissions of phenomenological
experience. Despite the experimental structure, the passages contain a
well-formed rhetorical message advancing key elements of Engelbart's research
program, which aimed to develop new data structures in combination with new
ways of displaying them. Yet I cannot help but be carried away by the fluency
of the prose and by the sheer audacity of the project. Here's someone who has
not only glimpsed the future, but has also brought it into being. The
contemporary author can drag and drop passages around with more facility, but
he has not himself structured his cognitive environment. In Engelbart's terms,
someone else has augmented the author's intellect, in ways that may or may not
fit the individual psyche. That feeling of effortless textuality cannot
therefore be taken at face value, by the unreliable phenomenological accounts
alone. To bring his system into being, Engelbart convened what he called a
"bootstrap community," which through recursive self-improvement could lift
itself up towards a smarter, more efficient, and as the report's lexicon
betrays, a more human way of working. To accomplish this, the group crafted new
input and output devices. They wrote new programming languages, compilers to
interpret them, and debuggers to troubleshoot. They invented word editors and
format control languages. Here's how Engelbart diagrams a *part* of his
text-manipulation language in the same report:

!["State--chart portrayal of part of text--manipulation control language"
[@engelbart_human_1969, 36].](images/engel-edit.png)

The diagram shows much attention to the detail of and love for the writing
craft. But there is also much complexity. It is near impenetrable. In building
their own tools, Engelbart's team lifted themselves up by the bootstraps. But
it was not the machine that lifted them up---it was the process of creating the
machine. The very metaphor of bootstrapping suggests the impossibility of using
one's bootstraps to pull others out of the Platonic cave. As a side effect of
that effort, text, before readily apparent on the page, now enters a complex
system of executable code and control structure. The perception of material
lightness of textual being comes at the price of legibility. Would new authors
find the same ease in the complication of the mechanism? I suspect not unless
they become an active part of a "bootstrapping community" of their own.

As the chapter is about to take on the intellectual history of document
structure, I propose we keep the above three devices at hand: Goldberg's
Controller, the IBM MT/ST, and Engelbart's Time Fob. In the first of these,
text structure and machine control lie before us for inspection. The second one
is mute: a black slate. The last of these belongs to what Peter Denning calls
the "third generation" of computer systems, an assemblage of storage, input,
and output devices that continues to shape the contemporary human encounter
with text. In this period, the inscription fractures into traces of disparate
kind: some that fade from view to appear elsewhere in the system in a more
enduring form, as an arrangement of floating gates; some that persist in the
flicker of the cathode ray or the flow of the liquid crystal, but only as long
as the device draws power; and yet others that simply dissemble to create the
illusion of effortless textuality.

## How will we find it?

## "microanalysis," 

the closest possible kind of reading
that pays attention to the material contexts of knowledge production.

Such textual proliferation comes at a cost. Until recently, the bread and
butter of literary scholarship has been close reading. Close reading, like
critical thinking, is an idea more easily performed than explained, because the
details become contentious as soon as they are formalized.[^ln2-close] I will
tread carefully, therefore, by committing myself to a broad definition of close
reading that involves some notion of deliberate and reflective practice aimed
at deep comprehension. The Partnership for Assessment of Readiness for College
and Careers (PARCC), the organization responsible for the controversial
Common Core standards being implemented in schools across the United States
today, promotes close reading as "thorough," "methodical," and "analytical"
practice that "enables students to reflect on the meanings of individual words
and sentences; the order in which sentences unfold; and the development of
ideas over the course of the text, which ultimately leads students to arrive at
an understanding of the text as a whole" [@parcc_parcc_2012, 7]. The general
movement here is from "text" on the page to "work" (text as a whole, in the
language of the report). The model of textuality implicit in the institutional
project of close reading assumes an environment of received literary canons,
naturally accessible to the human intellect. For the duration of the "Gutenberg
galaxy," the age of print, a well-educated person might have been expected to
internalize some several hundred or perhaps thousands of major texts
constituting the canon. Close reading was honed to operate on that naturalized
scale.

The expansion of the textual field has radically increased the cognitive
demands of literary engagement. The pipeline between text and work has
lengthened considerably. On the one side, the matter of canon formation can no
longer be relegated to stable, long-term systems of social filtering. Seen from
the perspective of a literary interface, the database, the social stream, and
the search engine are tools for dynamic, "on the fly," generative
canon-formation. Consider the task of finding an unknown (to me) factoid
online, about philosophy in the times of Andalusian Spain, for example. Where
in the past I might have started with a subject catalog compiled by librarians,
today I construct a search query, using resources that I believe will return a
reasonably authoritative list of texts on the subject. The search engine, in
effect, replaces (or rather complements) centuries-long processes of
canon-formation. A near-instantaneous list of results now becomes my ephemeral,
but nevertheless authoritative, collection of relevant literature.

Each text in the returned list still requires the instrumentation of close,
analytical interpretation. However, the same discipline of critical and
reflective deliberation exercised on the level of an individual text needs to
also be exercised on the level of procedurally generated search engine results:
Where to search? Using what engine? How to construct the query? What are the
implicit biases of the system?  The academic question of canon-formation
transforms into a (not yet critical) practice of rapid, iterative, generative
canon-making. Whatever ideals motivate close reading between "text" and "work"
surely must drive the process on the level of dynamic corpus composition.

Examples of macroanalysis.  The various practices of distant reading arise from
the condition in which canons are no longer accessible, in their entirety, to
the unaided (natural) human intellect. These include distant reading and
macroanalysis in literary studies [@jockers_macroanalysis_2013;
@moretti_distant_2013], culturomics in economy [@aiden_uncharted:_2014],
e-discovery in law [@scheindlin_electronic_2004; @scheindlin_electronic_2009],
automatic essay evaluation in education [@shermis_handbook_2013], and medical
informatics in medicine [@shortliffe_biomedical_2013], among others. At the
foundations of these nascent disciplines is a shared toolkit of statistical
natural language processing [@manning_foundations_1999; @jurafsky_speech_2008],
automatic summarization [@radev_centroid-based_2004; @nenkova_pyramid_2007],
machine learning [@rasmussen_gaussian_2006; @flach_machine_2012], network
analysis [@opsahl_node_2010; @szell_measuring_2010; @takhteyev_geography_2012],
and topic modeling [@wallach_topic_2006; @blei_probabilistic_2012].

*Plain Text* compliments distant reading on yet another scale of analysis. The
book is structured along a trajectory that begins at the material foundations
of modern textual technology, moving from the stratum of circuit and silicon to
reach up, through layers of abstraction (files, folders, and operating
systems), towards the reader. In following that path, I aim to make visible
material conditions of knowledge production that give rise higher order
phenomena like pixels, letters, words, books, and literature. In developing
such an approach to the study of textual phenomena, my book introduces a method
of literary microanalysis. Where distant reading perceives patterns across
large-scale corpora, microanalysis breaks literary systems down to their minute
constituent components.

![Micro, macro, and close reading.](images/micro.png)

Where distant reading and macroanalysis are concerned with text aggregates,
microanalysis, of the type suggested (but not carried out to its logical
conclusion) by Kittler and Latour and Woolgar, occupies the other side of the
text--work equation. Note that contrary to what one may intuit, distant reading
often happens prior to close reading and not the other way around. In most
cases, the reader must find the text before analyzing it. Canons, corpora,
collections, libraries, and archives are all aggregates that mediate the
process of discovery. Microanalysis, too, constitutes a study of mediation, but
on the level of physical minutiae otherwise not readily observed in cursory
exploration. The instruments of microanalysis may coincide with computational
tools, designed to find hidden patterns lurking above or beneath a given
document.[^ln2-iarkho] The micro-instrumentation might also include a
screwdriver, a binding needle, or a soldering iron: sharp tools that help to
pry open and to scrutinize otherwise magical textual black boxes.

The pre-modern model of literary transmission assumes relatively short physical
distances separating authors, texts, and readers. Were I to write these words
in front of you, on paper, I would simply pass the page into your hands. In
receiving this text, you could be fairly certain that no third party meddled in
the process of passing the note. When communicating in this way, it makes sense
to talk about notions of strong "authorial intent" and "fidelity to the
original," because authors and their works are thought to occupy the same
contiguous space and time. The advent of cheap mechanical reproduction of print
(for the sake of brevity let's say the printing press) has introduced a range
of devices that mediate between the author and the reader. Distance, time, and
mediation subsequently weaken facile notions of authorial fidelity or intent.
At the very least, we know that editorial practices, publishing markets, and
communication technologies can introduce an unintended element of noise into
the channel. At the maximal break down of the communication act, long-distance,
asynchronous communication is susceptible to "man-in-the-middle" attacks, by
which a third party maliciously alters the content of communication
[@needham_using_1978].

Changing material conditions of textual transmission push against familiar
literary critical ideas. For example, as the mechanical reproduction of print
weakens the material basis for authorship attribution, the notion of authorship
itself undergoes change. That is not say that the author is dead, as Barthes
would have it. Authors continue to live and to collect royalties from the sale
of their works. The weakening of the authorship function merely makes certain
ways of talking about things like "authorial intent" and "fidelity to the
original" difficult to sustain. Massively collaborative writing projects like
Wikipedia and procedural narrative generation (machine writing) further erode
ideas of authorial production based on individual human agency. Yet, it would
be a mistake to believe that the myth of autopoiesis (the literature that
"writes itself," writing that writes, discourse that speaks, etc.)[^ln2-varela]
can displace the myth of the author. A discipline of close attention to the
atomic particulars of encoding, transmission, storage, and the decoding of text
at the site if its application to the human condition ultimately aims to
reclaim subjective agency, in motion. This may seem strange at first: to
recover the subject in the physical minutiae of the literary--technological
encounter. Yet the point of contact is crucial, for it is here that the subject
seems to disappear, in a compressed moment of time that needs to be unpacked
with some precision.

Come back to vocab. Not the difficulty of obscurity unfamiliar. Ima guide you
through it. I care.


In an ASCII-rendered plain text file, byte count corresponds to character
count.[^ln-char] Let these words soak in for a moment. I will spend the rest of
the book unpacking this idea. We will have to come to terms with what "plain
text," "ASCII," and "bytes" really represent. But for now, a common-sense
understanding of the terms should be enough to form the intuition that texts
and characters are units of meaning that make sense to humans; and that bytes,
by contrast, "make sense" for machines. Remarkably, the two modes of
meaning-making gain a measure of equivalence under the singular condition of
plain text (and even then, not always, and with many caveats).[^ln-human],
[^ln-meaning]


## Microanalysis opens the gap between text as it is embedded into storage media
and text as it is seen on the screen. 


## Gerard Genette and Nelson Goodman on the nature of textual copies,

## "reading" from the perspective of a copy machine

which "sees" pages
as monolithic images. 

We often think of literary works as completed products. *To the Lighthouse,*
*Hamlet,* *Moby Dick* seem to circulate as complete, fixed entities. Textual
critics, in charge of creating a work out of a multiplicity of texts, remind us
that the work is a carefully-crafted illusion. The text multiplies, always
threatening to undermine the unity of the work. Think of something substantial
you have written recently: a paper or a proposal perhaps.

"Computer assisted tracing of text evolution"

Goodman's "Art and Authenticity." Benjamin. Immanence and transcendence.
Goodman: thinking about the perfect copy. But there is a confusion here: fake
art is not necessarily about a copy, it is about provenance. Autographic vs.
allographic art. Whether forgery is possible. Digital vs. analog art. Analog
art is medium bound. Digital abstracts from the material.

Duplicates in Computer Science. Almost 30% of webpages are "very similar to
other pages" [@fetterly_evolution_2003].

It is a commonplace sentiment now that the printing press brought with it the
Protestant Reformation [@dittmar_information_2011, @febvre_coming_2010,
@mcluhan_gutenberg_1962, @eisenberg_word_1992]. Movable type is responsible for
at least a part of that story. The mechanization of print is what turned
writing from an autographic into a supremely allographic art. The Gutenberg
press helped place a copy of the bible into the hands of every ordinary
believer. That meant also minimizing the role of the priest as a privileged
interpreter. Without books in hand Martin Luther could not have written "all
who are in the church are priests" [@hillerbrand_protestant_2009]. Movable type
lifted text from rarefied media of parchment into that of the reified and
disposable leaflet. But until well into the middle of the 20th century, the
printing press remained in the realm of big technology: expensive, centralized,
unwieldy, and relatively easy to suppress. The liberatory potential of media
independence comes into its full fruition after the age of the typewriter.
After a historically brief flowering in the wild, our daily textual practice is
once again in danger of returning to controlled, media-bound, and
tightly-regulated conditions of existence.

Several important developments in the twentieth century aided in the liberation
of text from its material contexts. The first is the copy machine.  It is an
entirely unglamorous job to ponder the phenomenology of reading from the point
of view of a photocopier. Yet the impact of the humble Ditto and Xerox machines
has only recently began to get the scholarly attention it deserves. In the
United States, the copy machine placed a smaller version of Gutenberg press
into every office. And it allowed for the flourishing of a vibrant samizdat
(self-publishing) scene, usually connected to underground culture movements
without access to conventional publishing: punks, skinheads, computer- and
game- geeks, feminists, fans of science fiction and queer-core music
[@klanten_behind_2011, @piepmeier_girl_2009, @duncombe_notes_2008].

The word "xerox" firmly entered the English lexicon by the
mid-1960s[^ln11-xerox] along with stencil duplicators, mimeograph machines,
microfiche, spirit duplicators,[^ln11-spirit] hectographs, and dot-matrix
printing. A whole industry of photocopying still exists to seamlessly peel the
content away from one material medium and plaster it into another.

From the "point of view" of a photocopier, the text exists only as pure form.
The electro-photographic process at the basis of modern photocopying (invented
in the 1930s, but with patents dating back to the middle of the 19th century)
describes documents in terms of "electric charge patterns," "photoemissive
layers," and "sheet surfaces"  [@carlson_electron_1940,
@carlson_electrophotography_1942, @carlson_graphic_1953]. The photocopier is a
device for total graphesis and extreme surface reading.[^ln11-ooo]. There is no
word or content for the photocopier: only paper and image. The photocopier's
"job", if you will, is to lift a picture from one page and to impress it into
another. It does so by temporarily imaging text (as picture) onto a
photo-sensitive drum. Ink powder sticks to the electrically charged surfaces of
the drum in correspondence to the shapes on the page. The shapes are then
rolled onto a new page, re-inscribing the image into the new medium. The text
is never immaterial. But for a moment it finds shape as an invisible electric
charge. The transformation should not be reduced to some sort of mystical,
metaphysical moment. On the contrary, in instrumental terms we observe the
relatively mundane process of media independence.

Let's linger on that moment of remediation. The process of textual
transcription by hand involves a similar transformation. When a scribe copies a
page from the bible, for example, he also "lifts" content from paper.  Not
unlike a photocopier, the human stores text in the electro-chemical pathways of
the brain, before reproducing it once again into another medium. And like a
photocopier, the human can perform the copy function mechanically, without
comprehension and on a purely formal, visual level. In fact, the historical
record is full of errors that suggest that pre-modern scribes wrote at widely
varying levels of literacy [@goudsmit_illiterate_1974,
@bucking_training_2007].[^ln11-literacy] In the Muslim tradition, the prophet
Muhammad himself is sometimes called "the unlettered prophet," to emphasize
fidelity and lack of mediation in the transcription of the message
[@calder_ummi_1990, @gunther_muhammad_2002, 1-26]. In other words, the Qur'an
claim to authenticity relies on the prophet's ability to copy the message
"verbatim." On the conservative Muslim view, anything but a faithful
transcription of the original text amounts to an interpretation, not
self-identical to the letter of the text (a translation, for example)
[@tibawi_is_1962, @fatani_translation_2005, @wilson_first_2009].[^ln11-quran]

Where the photocopier placed a printing press into every office, the printer
placed one on every desk. Falling cost of print.

Finally, the text exists on the screen and magnetic media only, where the cost
of reproduction approaches zero.


## the crisis of authenticity

It is quite odd, if you think about it, to believe that objects somehow retain
an aura or a trace of their history: "a handkerchief used by Princess Diana,"
or "the cigar box owned by President Kennedy." In some real sense, the
historical patina covering these objects is only imagined. President Kennedy's
fingerprints and the tears of Princess Diana have long disappeared from their
physical medium. Their trace cannot be reduced to "mere" physical
characteristics: that is, to actual scratches, stains, or molecules,
recoverable from the object. Rather, the object had to "be there" in some way,
acquiring a patina of authenticity. "Having been there" is what gives the
object value, sentimental or otherwise, in a condition that refuses formal
reduction to a set of merely material properties. The high price contemporary
society places on authentic artifacts is therefore a type of magical thinking.
Walter Benjamin's famous essay on the work of art in the age of mechanical
reproduction appears to both celebrate and lament the demise of such magic.
"The technique of reproduction detaches the reproduced object from the domain
of tradition," wrote Benjamin. "By making many reproductions it substitutes a
plurality of copies for a unique existence. And in permitting the reproduction
to meet the beholder or listener in his own particular situation, it
reactivates the object reproduced. These two processes lead to a tremendous
shattering of tradition which is the obverse of the contemporary crisis and
renewal of mankind" [@benjamin_work_1969, 217-253]. And yet, for Benjamin, the
easy reproducibility of modern art forms like film and photography also
"emancipates the work of art from its parasitical dependence on ritual"
[@benjamin_work_1969, 217-253]. One could read Benjamin's famous essay as a
piece of technological nostalgia, or as an expose on the magic and superstition
latent in modernist art. Crisis or emancipation? Benjamin feels both, and in
doing so he captures the metaphysical anxiety of the modern age. We desire for
our Vermeers to stay special but also for a Vermeer to hang in every home,
peering back at us from a multitude of anonymous reproductions.

The modern world faces the challenge of authenticity, which we have only began
to assimilate: politically, economically, and philosophically. The ubiquitous
holographic certificate of authenticity on a luxury handbag or on a "Digital
Versatile Disk" (how antiquated the DVD sounds already!) is a cheap band-aid
hiding the immaterial nature of these artifacts. Fashion and software companies
are reluctant to talk publicly about piracy,[^ln11-counter] because the so
called "knock-offs" are essentially exact duplicates, often "leaked" from the
very same factories producing the original.

Note that it makes no sense to talk about "what was meant" by the designer of
the handbag, the way textual critic may talk about authorial intention. Nor
would it make sense to "correct" some perceived flaw in the painting by
Vermeer. The practice parallel to "textual criticism" in visual arts is
restoration. Although similar to textual criticism in its aims to recover some
notion of the original, crucially, the practice of art restoration takes place
without reproduction. Unlike a critical volume, which essentially reprints the
work in its reconstructed entirety, restoration happens at the site. All
"unauthorized" copies constitute a forgery that competes for attention with the
original. One could say that for painting and handbags "matter is synonymous
with meaning," but the sentiment strains at the limits of critical vocabulary
in the tradition that gives us only the binaries of form and content.

The context-dependence of an artifact is not even medium-specific, as
Goodman would suggest. Texts are more allographic than handbags and paintings,
but Vermeers and Louis Vuitton purses are also in some sense perfectly
reproducible, like texts. They are texts, and I do not mean it in some
post-structuralist notion of "everything is a text."[^ln11-derrida]

Technologies like 3D scanning and printing increasingly reduce physical objects
to their textual representation, as is the case with the stereo lithographic
document (STL) format that describes objects for 3D printing.  Here, for
example, I reproduce code that describes "a surface" of a functional gun (the
code made available online by *Defense Distributed*, a self-described civic
liberty activist group):

```
22=(
BOUNDED_SURFACE()
B_SPLINE_SURFACE(1,2,((#34104,#34105,#34106),(#34107,#34108,#34109)),
 .UNSPECIFIED.,.F.,.F.,.F.)
B_SPLINE_SURFACE_WITH_KNOTS((2,2),(3,3),(15.1641909236141,15.3257778193699),
(2.99329024107099,3.14159265358979),.UNSPECIFIED.)
GEOMETRIC_REPRESENTATION_ITEM()
RATIONAL_B_SPLINE_SURFACE(((1.,0.997252058758362,1.),(1.,0.997252058758362,
1.)))
REPRESENTATION_ITEM('')
SURFACE()
);
```

This piece of code exists in its Platonic ideal *before* the physical artifact.
As costs of reproduction approach zero, so does the line between an object and
its description. Imagine a world where to think "gun" is also to seamlessly
bring one into existence. The erosion of boundaries between object and idea has
a pronounced effect on the governance of goods, felt precisely at the
distinction between the autographic and the allographic. Governments currently
regulate the manufacture and the distribution of guns as artifacts, but how
does one regulate guns as texts and ideas? Any such effort amounts to
censorship. What was first regulated under the regime of property law, now
increasingly falls under the regimes of intellectual property and rights to
speech. Where does that leave the practice of textual criticism? In such a
world there can be no distinction between works and texts. Unlike millions of
reproduced Vermeers (which no doubt occupy this world in the form of postcards,
posters, and photographs), text is text. It lies before us in glorious
flatness, as thick as a sheet of paper.

Text as I have already began to define it Chapter One, is a digital mode of
representation, defined, at least in part, by a relative lack of "stickiness"
to its physical medium. Textual artifacts therefore exhibit the dualism between
text and work to a much greater extent than other forms of art and
communication. A Vermeer is always a work. Not so with *Hamlet*. "Hamlet" may
indicate a category of relate, or, an individual instance of a given work.
Philosophers Nelson Goodman and Catherine Elgin explain this peculiarity by
making the distinction between "works of art whose identity depends on their
history of production" and works whose identity does not. The former they call
"autographic" and the latter "allographic" [@goodman_reconceptions_1988, 65].
These terms will be useful for us throughout the book. Architecture and
painting, in this view, are strongly autographic disciplines. An exact copy of
a Vermeer or of a building by Frank Lloyd Wright could never rise to the status
of the original. The originals have what Walter Benjamin called an "aura or
authenticity." No amount of precision, down to the molecular level, could
substitute for the historical patina of the original artifact.

### 5.4 Problem of Drafts

Text as vector. Barthes: text as "an organism which grows by vital
expansion"[@barthes_work_1978, 161].  "The metaphor of the Text is that of the
*network*" [@barthes_work_1978, 161].

Internet is not rhyzomatic. Mediation is at stake. " But the bottom line is
that dumb-pipe email is unmediated, and therefore it's a business that Google
wants to get out of as soon as it can."

" It's also why I believe Google will kill Gmail as soon as it comes up with a
mediated alternative everyone loves. Of course, Google may offer an antiquated
"Gmail view" as a semi-obscure alternative to the default "Inbox"-like mediated
experience."

Problem of Drafts and Versions Documents as vectors. Not completed things.
Files as cognitive scaffolding for collective memory. Vissman.

Science lecture vs. humanities. Stuff that is known. Stuff that is not known

What does it really mean to "know" something? Or to have "read" a book? I have
had the following conversation countless times. Someone asks, "Have you read
Nabokov's *Pale Fire*," and I respond, "Yes I have." But there is a world of a
difference between reading it yesterday, last week, or ten years ago. The book
as an interface leaves an organic trace in the human mind---an imprint that
begins to fade as soon as it is created.

How much knowledge is there in the world? According to a recent paper published
in Science, 295 exabytes (or billion gigabytes).[@hilbert_info_2012;]

Whatever consciousness is, it is propelled forward on a thin edge of material
substratum. Reality exists only in the now, where the past is a memory and the
future only a possibility. Imagine whispering something to a friend. It does
not have to be very complicated, something like "I love you" or "I miss you."
The whisper dissipates as soon it is uttered. The percussion of the speaker's
breath creates temporary order: giving shape and pushing air molecules into
waves of pattern and form. That order begins dissipating as soon as it is
created. Within milliseconds, molecules return to their natural state of chaos.

To steel ourselves against entropy, we change the substratum from air to stone.
It's molecules are more stable. Etched in stone will be the same message. It
will last longer now. But it will also take longer to create. You will have to
carry around your tablet and chisel. The message etched in stone will outlive
the whisper and indeed will

Repercussion? How we preserve ourselves today affects the future. Appointment
with self.

Leroi-Gourhan, André.

The book as an interface between human and ?. We must see it in the context of
interfaces.

Wikipedia as a Turing complete language. The power of versioning. Text as a
vector. The problem of annotation. What are we annotating? Annotation solutions
force a platform. Is Hamlet a platonic object? A family of related objects?
Standard English editions in Europe.


related to the plummeting costs of digital
reproduction. The supposedly "dead" authors continue to draw checks from
copyrighted work. 

## Legal fictions rise to veil the emerging affordances of
textual technology.


# Not integrated yet.

## Everything is lit (connect to computational as lit)

1. Methodologically, materialism of the sort described above has naturally led me
to sociological approaches to the study of knowledge formation. In a study on
the construction of scientific knowledge, Bruno Latour and Steve Woolgar
perform what they call an ethnography of a science lab by exposing the passage
of ideas from applied experiment to literary production. For Woolgar and Latour
a laboratory functions as a factory of sorts, ingesting matter and artifact to
produce fact and inscription, beyond simplistic notions of knowledge-making and
discovery [@latour_laboratory_1986]. A cite of production. 

Why am I readin Ltour. Becaue latour gives us a way to see much lit.

Although much cited in the study of culture, science, and technology,
*Laboratory Life* has had less of an impact on the study of literature, even
though Latour and Woolgar borrowed much the other way around. In the 1986
postscript to the American edition, they remark on the broad trend in literary
theory toward treating texts as objects of interpretation, disavowing the kind
of criticism aimed at the "real meaning of texts," at "what the text says,"
"what really happened," or "what the authors intended"
[@latour_laboratory_1986, 273]. Similarly, the authors wanted to aim research
in the social study of science at the "contingent character of objectification
practices." Science, in their view, is itself a type of literary system,
"generating texts whose fate (status, value, utility, facticity) depends
subsequently on interpretation" [@latour_laboratory_1986, 273]. Throughout the
book, Woolgar and Latour use the term literature to mean something "inscribed,
printed, or published," a combination of verbs that coincides with "literature"
consistently throughout [@latour_laboratory_1986, 47-53]. Viewed as a work of
literary theory, *Laboratory Life* reifies the research program began by the
post-structuralists. Where Jacques Derrida could remark offhandedly that
"everything is a text,"[^ln2-derrida] Woolgar and Latour make concrete the
literal transformation of matter into text. No longer was literature an
illustration of something that happens in the laboratory: Latour and Woolgar
showed the laboratory itself to function as an inscription-making machine.

The conventional way to understand Woolgar and Latour brings the interpreter
into the laboratory, breaking the illusion of scientific objectivity: instead
of "discovering" facts, scientists construct them (therefore, *social
constructivism*). Paradoxically, as Latour and Woolgar worked to undermine the
empirical unity of science (turning it into a kind of hermeneutics), they
performed literary analysis as an empirical method. To "read" the laboratory,
researchers observed the space in person, embedded into the fabric of its
everyday activity. As ethnographers, Woolgar and Latour were interested in
discovering models of objectivity (and science) that emanate from the practice
doing science localized to the particular laboratory under observation. A
theory of knowledge formation therefore becomes visible through ethnographic
description. In the *Laboratory Life* the reader is asked to examines a range
of evidentiary material along with published work. Floor plans, research
samples, workflow diagrams, dialog transcriptions, time sheets, and logbooks
all become a part of the epistemology.

To view Woolgar and Latour as scholars of literature (and not just of
laboratories) is to open up the floodgates of textuality. The laboratory, the
courthouse, the hospital---these institutions cannot be reduced to text, but
the lesson of *Laboratory Life* is that they are, in part, literary systems.
Beneath every scientific discovery, every popular song, every film, radio, or
television serial, every art or architectural project, every political office,
every legal judgment, every restaurant meal, every doctor's prescription, every
website, and every machine manual---beneath that giant and perplexing pile of
human activity lies a complex network of circulating textual documentation.
Woolgar and Latour show that literary scholarship can involve more than just
reading books, but also making visible that textual undercurrent: following it
and mapping its movements across rooms and offices, all the while exposing the
mechanisms of its mutation and locomotion. It is an exciting prospect: to view
the hospital as a library, where cadres of physicians and administrators
convert human stories into diagnoses, into controlled vocabularies, into files,
into billable codes, into inscriptions, and into archives. Or to see the legal
system as a process of literary discovery, where legions of attorneys and
paralegals pore over terabytes of textual material, transforming inscription
into evidence. That is not all these institutions are, but they are also that;
and there is much work to be done there for a scholar of literature.

On an average day in 2008, at home, an average American read around 100,500
words a day. At 250 words per page, that is around 402 printed pages.  Between
the years of 1980 and 2008, the consumption of information in bytes---a measure
that would obviously privilege storage-heavy content like sound and
video---grew at a modest 5.4 percent per year. Reading, in decline until the
advent of the internet, has tripled in the same period.  Reading in print
accounted for 26 percent of verbal information consumed in
1960. That number fell to 9 percent in 2008, but the consumption of words
      digitally increased to 27 percent of total consumption, which means that
reading has increased its share of the overall household attention span
[@bohn_how_2009; @hilbert_worlds_2011; @hilbert_info_2012]. The first decade of
the twenty-first century saw a 20 percent increase in library visitation
[@u.s._institute_of_museum_and_library_services_public_2010]. According to
UNESCO data, literacy rates continue to rise globally, as the world calibrates
imbalances of access along the lines of gender and geography
[@huebler_adult_2013]. By a conservative estimate, the number of scientific
publications grows at about 4.7 percent per year, which means that the amount
of published research roughly doubles every 15 years or so, and the figures are
much higher in some fields [@archambault_welcome_2005; @crespi_empirical_2008;
@larsen_rate_2010]. The number of books published in the United States almost
tripled from 2005 to 2009 [@bowker_u.s._2009].

All measures point to a drastic expansion of the textual field. At the same
time, we are often told that reading of short stories, novels, poetry, and
plays appears to be at grave risk [stephens_rise_1998; @merrin_television_1999;
@nea_reading_2004; @golumbia_cultural_2009; @fish_digital_2012;
@marche_literature_2012].[^ln2-internet] Something does not add up. As a
society we are reading, writing, and publishing more each year. The
reconciliation of vocabularies locates the source of the perceived metaphysical
threat in the culture's profound alienation from the material contexts of
literary production. The language of literary theory is supremely tuned to deal
with symbols. Only after long reflection, on the order of centuries, do we
begin to note the sheer strangeness of the textual interface: the medieval
palimpsest, the stylus, and the goat parchment. Yet we cannot afford to care
less about the keyboard and the screen today that we do about ink and paper.  A
battle is quietly brewing for the soul of computing and hence for the control
of our basic reading and writing apparatus.

Where literacy thrives, the very nature of what constitutes a literary text (of
any kind) has changed significantly with the advent of ubiquitous computing and
its related techniques involving the internet, integrated circuits, magnetic
storage, miniaturization, and search and suggest algorithms. By many accounts,
the second half of the twentieth century saw a major technological shift
affecting the production, access, and distribution of knowledge, in a shift
comparable in its consequences to those accompanying the spread of the
Gutenberg printing press in the fifteenth century [@johns_piracy:_2009;
@kovarik_revolutions_2011]. Matters always central to the field of literary
studies remain vitally important to the functioning of modern society. Literary
studies, when conceived broadly, provide fertile grounds for collaboration
between diverse disciplines interested in the changing dynamics of narrative,
interpretation, language, form, prosody, composition, dialog, discourse, story,
genre, authorship, influence, and text. It is in this core conceptual cluster
of operational terms that I want to locate a notion of poetics capable of
addressing not only the machinations of poetry and prose, but also of pixel,
code, string, script, inscription, list, document, file, record, project, and
archive.

## 5.2 Copy Text (textual criticism)

The division of text into form and content makes a difference in more
utilitarian ways as well. For example, imagine the challenge of compiling all
known commentaries on Shakespeare's *Hamlet*, by hand or programmatically. The
problem of what edition to use as the "base" for such a project presents itself
immediately. In the words of Barbara Mowat and Paul Werstine, the editors of
*Folger Digital Texts*, "readers assume that there is a single text for the
plays: what Shakespeare wrote. But Shakespeare's plays were not published the
way modern novels or plays are today: as a single, authoritative text. In some
cases, the plays have come down to us in multiple published versions,
represented by various Quartos (Qq) and by the great collection put together by
his colleagues in 1623, called the First Folio (F). There are, for example,
three very different versions of *Hamlet*, two of *King Lear*, *Henry V*,
*Romeo and Juliet*, and others. Editors choose which version to use as their
base text, and then amend that text with words, lines or speech prefixes from
the other versions that, in their judgment, make for a better or more accurate
text" [@mowat_textual_2012]. In other words, where the readers expect to
encounter *Hamlet* as a single, unified idea, they instead encounter a
multiplicity of slightly diverging textual "witnesses." Errors in transcription
and editorial interventions accumulate. The proliferation of unlicensed copies,
translations, and imitations further adds to the complexity of the problem.

Editors of classical literature will often solve the problem of diverging
copies by designating one version as canonical, preserving the formal
characteristics of the given version down to individual line breaks, as is
often done with Platonic dialogs. It then becomes possible to collate
commentaries, connecting each to individual words and passages at their
canonical location. The problem remains in the abstract: which version to count
as canonical? The act of deciding on what constitutes an authoritative edition
can quickly become a contentious issue, particularly when the text in question
carries religious or political significance.

A whole field of textual criticism exists in the service of negotiating
"critical" editions which in some way preserve the struggle to reconstruct a
measure of authority. But what should count for being authoritative? A number
of logical possibilities find as many ardent adherents. For some, the act of
deriving the authoritative text lies in reconstructing authorial intent, using
the author's notes, for example, or from first-hand witness testimony. Others
place a premium on reconstructing and analyzing the writing process itself, in
what has been dubbed as "genetic criticism" [@deppman_genetic_2004, 1-36]. Yet
another approach will compare multiple editions of the same text to derive a
possible meta-text, preserving all extant variations. Yet others will simply
rely on their judgment to produce what they believe is a "true" text based on a
number of personal and eclectic considerations [@bowers_rationale_1950,
@bowers_shakespeares_1954, @mcgann_radiant_2004]. Such approaches to textual
criticism have at least this one thing in common: behind the fuzzy profusion of
textual material they perceive the outlines of a single, unified work. As G.
Thomas Tanselle (a prominent textual scholar) puts it, "one must be able to
distinguish the work itself from attempts to reproduce it."[^ln11-tanselle] For
a textual critic of his kind, all real-word variants and reproductions are
suspect because possibly corrupt in some way. Despite its careful attention to
the materiality of the text as artifact, textual criticism in this vein remains
a classically idealistic pursuit. The work exists as an ideal form in the realm
of the ideal.

[ln11-tanselle]:  "Whatever concept of authorship one subscribes to, the act of
reading or listening to receive a message from the past entails the effort to
discover, through the text (or texts) one is presented with, the work that lies
behind" [@tanselle_rationale_2010, 13-18.].

Restoration and Anti-restoration. Eugene-Emmanuel Viollet-le-Duc (for) and
William Morris and John Ruskin (against). Also Teoria del restaruo by Brandi.

"To restore an edifice means neither to maintain it, nor to repair it, not to
rebuild it; it means to reestablish it in a finished state, which may in fact
never have actually existed at any given time." [@price_historical_1996, 314;
orig from @viollet-le-duc_foundations_1990, 195]

"It is for all these buildings, threfore, of all times and styles, that we
plead, and call upon those who hve to deal with them to put Protection in the
place of Restoration, to stave off decay by daily care, to prop a perilous wall
or mend a leaky roof by such means as are obvioiusly mant for support or
convering, and show no pretence of other art, and othrwise to resist all
tampering with either the favric or ornament of the buildings as it stands; if
it has become 

But whose who make the change wrout in our day under the name of Restoration,
while professing to bring back a building to the best time of its hisory, have
no guide but each his own individual whim to point out to them what i s
admirable and what contemptible; while the very nature o their task compels
them to destroy something and to supply the gap by imagining what the earlier
builders should or might have done." Data munging. Irreversible changes.
@price_historical_1996, 320; orig in "William Morris, "The Principles of the
Society as Set Forth Upon its Foundation," Builder 35 August 1877.

Ruskin lamp of memory.

"It is impossible, as impossible as to raise the dead, to restore anything that
has every been great or beautiful in architecture [...] That spirit which is
given only by the hand and eye of the workman, never can be recalled. Another
spirit may be given by another time, an it is then a new building; but the
spirit of the dead workman cannot be summoned up, and commanded to direct other
hands, and other thoughts. And as for direct and simple copying, it is palpably
impossible. What copying can there be of surfaces that have been worn half an
inch down [@staniforth_historical_2013, 2]?"

"But it is said there may come a necessity for restoration! Granted. Look the
necessity full in the face and understand it on its own terms. It is a
necessity for destruction. Accept it as such, pull the building down, throw its
stones into neglected corners, make ballast of them, or mortar, if you will'
but do it honestly, and do not set up a Lie in its
place"[@staniforth_historical_2013, 3].

On the Restoration of The Circumcision by Signorelli. "At Volterra, over the
alter of a Company in the Church of S. Francesco, he painted in fresco the
Circumcision of Our Lord, which in considered beautiful to a marvel, although
the Infant, having been injured by damp, was restored by Sodoma and made much
less beautiful than before. And, in truth, it would be sometimes better to
leave works half spoilt, when they have been made by men of excellence, rather
than to have them retouched by inferior masters." 1550 Tim's Vermeer.

picture cleaning as removing as removal of "varnishes and other encrustations,
by which a paininting may be obscured, but by which it is usulaly also in some
measur protected from injury." [@bomford_issues_2004, 60] Original in the
Report from the Select Committee on the National Gallery, London 1853, vi-xi.

Colliveau set about [restoring]this picture with uncommon solenity, because it was his
own. He began with the most delicate pumice stone, and very soon got into the
walls of th building, but was astonished to find the apearance of bricks and
mortar still, although not guite soneat; he soon cleared away the wall on
discovering a room beyond, and pictures, furniture, &c. most beautiful.
Gracious powers! how was he agitated; forgetting totally the mischief he was
commiting, his elbow went like a fiddler's; every thing vanished.---However,
finding a bed, the curtains of which (a dark green) were drawn close, he went
to work upon it ver tenderly; presently a pot de chambre came  in sight!---His
whole souldwas absorbed; his face all one ghastly grin; his legs (like German
flutes) tottered under him. Not that great philosopher Guffin Ragba, was more
agitated while cutting the leather from off his bellow, to disover how wind was
make, than was Monsieru Collifeau while scroring off the bed cloaths; at last
he found a lady asleep, very beautiful, because very fat, and a Dutch woman.
Now this part I did not see, and I believe but very few besides himself; but
alas! hew was soon to be at the end of his discoveries. Having got a glimpse of
some gilding, he redoubled his efforts, when he found it was an inscription; it
was in Dutch, and nearly thus, as translated by Mr. Peter La Cave:---"Now
caitiff, meditate on th havoc though hast made throughout thy lie, and go thang
thyself. In this picture, wretch! though have destroyed, what to the end of the
world, can never be replaced." [@bomford_issues_2004, 44-45]. Originally from
Julio Caesar Ibbetson, An Accidence, or Gamut, of Painting in Oil and water
Colous . London: Darton and Harvey, 1803, 5-14.

"The sculptor wastes away th rudo block til he has accomplished the desired
form. not so the painter. he builds up his forms from a blank surface, and
hides, as he rogresses all the preliminary layers upon which the etxternal
colours depend for durability and lustre. And hence, not on the meaning and
spririt of the work must be understood; the restorer must also be familiar with
he naure of the materials and the manner of their employment.
[@bomford_issues_2004, 70]. Original from @merritt_dirt_1854, 67-72


The wound
that heals.

Just giving you a taste of restoration vs. preservation. What would


[^ln11-counter]: A documentary on counterfeit goods produced by BBC4
interviews Christophe Zimmerman, Senior Technical Officer at World Customs
Organization, who deplores the industry's lack of cooperation with law
enforcement [@bbc4_counterfeiting_2013].



We have now traveled from the pixel down to magnetic storage media and back
from the storage media to the screen. The passage opens up space between
visible content, media, and the imposed forms that govern any and all higher
notions of literary activity "floating" above this nominally "digital" layer.
Technology does not determine the literary space: it has only the potential to
hide implicit mechanisms of machine control, or, to offer possibilities for
transformation not otherwise available to other forms of textuality. Loosely
coupled to its material contexts, text can continue its relentless drive from
matter to idea and into other matters as long as its passage is not hampered by
regimes that prohibit further sharing, remixing, and transmediation. I say nothing
yet of the potential necessity of such regulation. Under certain conditions, in
the name of privacy, security, or property rights, it may become necessary to
flatten out and to treat text as more of an analog, media-bound modality of
communication, limited in its ability to move across minds and cultures. But,
it is also in our broadly human, civic interest to keep such mechanisms of
constraint visible to view, under continual scrutiny of critical, close, and
closest possible reading.


## Hyper Erudition

Writing on the state of
twenty-first century poetics Craig Dowrkin speaks about poetry's "Malthusian limit."
"Bound by discreptant rates of production and consumption, the readerly economy
of poetry in the twenty-first century cannot avoid a catastrophic calculus: the
rate of consumption quickly hits an arithmetic limit (any one person can only
read so much), but the rate of production is increasing geometrically"
[@dworkin_seja_2008, 9].

Flattening of the knowledge realm.
https://medium.com/the-physics-arxiv-blog/the-extraordinary-growing-impact-of-the-history-of-science-642022a39d67


Impact on the study of textual artifact. If are are to take the hypothesis of
extended cognition seriously, we must treat the tools of cognitive augmentation
as seriously as we would our own mental development, in the traditional sense.

The flattening of the knowledge realm. It is not the absence of topography. It
is our ability to readily perceive topography, and to place ourselves within
that discourse. The knowledge of the mechanic vs. the knowledge of a surgeon.
The surgeon should get more money because of the subjective experience of
cutting people up. That part does not transfer. The technical literature is
actually quite understandable. Incommensurability of subjective experience.
Commensurability of knowledge. The fundamental problem of modern world is the
problem of expertise.

http://word.mvps.org/faqs/general/wordvswordperfect.htm
http://wptoolbox.com/tips/MSWordToWP.html
modal vs. other kinds of processing

## 2.4 Content + Control

With the site more broadly defined, we can begin the work of recovering the
material contexts of literary production. I want to foreshadow now what will
become the punchline of the whole book: namely that the ghost in the machine
cannot be reduced to some vague notions of value---these amount to a
distraction, which lies plainly upon the surface. The conspicuous ghostly
apparition (indicating the absence of a living subject) is agency itself,
which, for now, necrotically attaches itself to dead things like systems,
mediums, and automated discourses. If all this talk of value, ghosts, and
necromancy sounds familiar, the reader will not be surprised when I advance
Karl Marx as a preeminent theorist of the algorithm and someone able to help us
reason through Kittler's computational conundrum. On towards Marx then (who
will appear much later), through the machine that, although intimately familiar
to every writer and consumer of text by touch, remains alien and alienating in
its hidden possibilities, soothing and threatening all at once.

The personal computer governing the production of textuality today emerged from
an amalgam of automated devices, chief among them the telegraph, the
typewriter, and the calculator. In his seminal 1937 paper "On Computable
Numbers," Alan Turing describes an automatic machine (*a*-machine) capable of
transposing the problem of calculability into "effective calculability." Where
all previous calculators are special-purpose mechanisms, engineered to augment
a specific type of computation, Turing thought of his device as a universal
"*a*-machine." In theory (and it was conceived as a thought experiment, at
first), it could imitate any other mechanism for calculation, because its
internal composition could be reduced to a dynamic representational state (as
opposed to a fixed, mechanical one). The device would receive input by means of
a paper ribbon containing discrete symbolic inputs. At its bare minimum, the
device would need only the ribbon, a means of "reading," and the means of
"writing" the symbols onto the tape. Mechanically, its movement could be
restricted to one axis or to the movement of the tape through the mechanism.
What makes such a device a *universal* machine is its ability to internalize
symbol as part of its mechanism. Symbolic input, in this case, can represent
computable data (letters and numbers), but it can also function as control code
("move left," "erase mark") altering the movement of the mechanism.

In fact, the Turing machine does not properly differentiate between content and
control code. Its "internal state" (Turing uses terms of sentient awareness
throughout) "is determined by *m*-configuration *q<sub>n</sub>* and the scanned
symbol," where the *m*-configurations are defined as "a finite number of
conditions" of a machine, which, in Turing's words, may be compared to "a man
in the process of computing a real number" [@turing_computable_1937, 231]. The
initial configuration plus the scanned symbol define the next internal state of
the machine. The material and the symbolic fuse into one: here lies the essence
of the universal Turing machine (UTM). Software is also hardware. In being able
to internalize configuration as symbol, the *universal* Turing machine can in
effect imitate all other special-purpose Turing machines. This transition of
symbols into machine states effectively defines modern programming. A universal
machine, unlike other, definitive, single-purpose and limited-state mechanisms
(a clock for example), contains the ability to take on differing internal
symbolic configurations. It can imitate a clock, an abacus, a scale, and, with
time, Turing thought it could imitate humans as well.

This ambiguity between hardware and software leads to some confusion in the
critical literature, as evidenced by Lev Manovich's playful response to
Kittler's "there is no software" argument. If I understand it correctly,
Kittler's short but often cited essay picks up the thread of Kittler's earlier
work to posit what he calls a "postmodern writing scene." "We do not write
anymore," writes Kittler: "human-made writing passes through microscopically
written inscriptions which, in contrast to all historical writing tools, are
able to read and write by themselves" [@kittler_there_1995]. According to this
schema, Kittler sees the paper-bound design blueprints of the first integrated
microprocessor as the last "real" piece of writing. Everything written after
that point is hardware (because software is hardware at that "microscopic"
level).

Manovich inverts Kittler's argument into "there is only software," by which he
means that in a pragmatic sense, the affordances of a given medium are
determined by software. A printed page begins to differ from a screen only when
the readers are able to effect something on the screen that they could not on
paper. To this end, Manovich encourages his readers to become active developers
of software, rather than its passive consumers [@manovich_there_2011, 274]. In
that, Manovich reasserts the possibility of writing in the silicon age. Kittler
(who passed in 2011) could perhaps object to that line of reasoning in
maintaining that chip architecture (the last written work) still determines (as
foundation) all higher levels of textuality "floating" above the silicon
bedrock. And no amount of learning to code would give an ordinary subject the
resources required to write in silicon---a process so advanced and expensive as
to be limited to a handful of international chip manufacturers. In opening a
successive nested series of black boxes, the post-silicon writer hits the
impenetrable bedrock of chip architecture. In such conditions, is it even
worthwhile to follow Manovich's call for new literacies? Is writing still
possible? If "there is no software," then no. Else if everything is software,
then one can answer to the affirmative.

This ontological complication has its roots deep in Cartesian dualism. The
question of where do brains end and minds begin remains unresolved in cognitive
science, for example. Similarly, at some imperceptible point software
disappears into hardware. But before we ourselves get lost in that liminal
space between matter and idea, let us recover a measure of oddity found in the
now ubiquitous operation of Turing machines. First, note that Turing's original
formulation happens at the level of a thought experiment. (Turing does not
begin to build actual machines until his move to Princeton in 1936.) A
universal Turing machine comes to life initially as an idea that can take on
the structure of other ideas expressed symbolically. Second, note that though
Turing describes his machine in the language of mathematics (where his most
significant contribution lies), his description also contains the bare minimum
of a mechanical device. No matter how symbolic a Turing machine aspires to be,
no matter how ascendant to the realm of the ideal, it still needs a bare
minimum of physical matter to function. And Turing's paper does contain the
canonical description of that bare physical minimum.

With the above two observations in mind, we can view abstracted universal
Turing machines, as implemented in the Wireworld universe (a cellular automaton
simulation), for example, or in Minecraft (a procedurally generated sand-box
world-exploration game), as recursive, second-order ideational constructs,
built on top of first-order physical mechanisms (a personal computer, in the
case of Wireworld and Minecraft). We know this, because all *n+* order Turing
machines are limited in computational power by the physical capabilities of
that bottom-most device (the physical machine writing the simulation). The
simulated UTM cannot outperform (in terms of cycles per second, instructions
per cycle, or its capacity to hold a number of instructions) the machine doing
the simulation. If we disregard the dizzying levels of recursion (a Turing
machine, simulating a Turing machine, simulating a Turing machine and so on),
we can begin to examine the turtle at the bottom, which has its head in the
symbolic and its feet firmly in the material world.

Literature in computer science tends to confront universal Turing machines as
algorithms: in other words, as virtual, second-order symbolic representations.
As consummate thinkers on the level of the symbol, computer scientists and
literary scholars (unlike, say, electrical engineers or book binders) rarely
need to pay heed to that strange bottom-most turtle.[^ln2-bottom] Yet it is
impossible to entirely disassociate the implementation from the idea. In his
review of Turing's "On Computable Numbers" paper, Alonzo Church, the American
mathematician whose work anticipated Turing's (independently) in several
important aspects, wrote that "a human calculator, provided with pencil and
paper and explicit instructions can be regarded as a kind of a Turing machine"
[@church_computable_1937, 42-3; also cited in @petzold_annotated_2008, 63].
Disregarding the broader, metaphysical implications of that statement, note for
now the persistence of two essential implements required for the minimally
viable operation of the Church--Turing human and machine calculators. Pen and
paper persevere and assert themselves through the
abstraction.[^ln2-abstraction]

Forced to confront the universal Turing machine *as a mechanism*, the historian
must acknowledge that it borrows from a number of extant designs, which,
together and incrementally, give the UTM its physical form. A media history of
the Turing machine as device differs from its intellectual history as symbolic,
mathematical abstraction in interesting and instructive ways.[^ln2-turing]

![Universal Turing machine as an idea. "Nick Gardner's Wireworld multiplier,
via a Turing machine."](images/turing-idea.png)

Although Turing's original article did not include any drawings, he describes
his "*a*-machine" in terms of paper tape medium, scanning, erasing, "writing
down notes," "external operators," and "moves" of the mechanism. From the
engineering standpoint, Turing's machine can be reduced (1) to the mechanism
for "reading" and "writing" symbols onto and from paper storage media, and (2)
to the paper tape storage medium itself (as seen in Mike Davey's
reconstruction). In the United States, these elements would find prior art in
mechanisms such as the "Numeral adding or subtracting attachment for
type-writing machines" [@daugherty_numeral_1894], "Combined
Type-writing and Computing Machine" [@degener_combined_1911], "Computing
Attachment for Typewriters" [@wright_computing_1914], "Computing Mechanism"
[@wright_computing_1915], and "Combined Type-writing and Adding Machine"
[@ellis_combined_1914] among others.

![Universal Turing machine as a device. Reconstructed by Mike Davey, circa
2012. Displayed in the Collection of Historical Scientific Instruments,
Harvard University. Go Ask A.L.I.C.E. Exhibition, 9/11-12/20/212. Photograph by
Rocky Acosta under CC Attribution 3.0 license.](images/half-turing.jpg)

By the end of the nineteenth century a number of lesser mechanisms anticipate
the functional elements of Turing's machine. And by 1936, when Turing
publishes his paper on computable numbers, these inventions not only anticipate
the modern computer, but are brought to mass market in the widespread
manufacture of computing scales, dial recorders, electric tabulating machines,
and computing typewriters made by companies like Underwood Computing Machine,
Electromatic, and International Business Machines (IBM). Rather than a single
eureka moment, the invention of the universal machine should be viewed as a
gradual historical process that culminates in Turing's universal (and minimally
viable) specifications.

When viewed in the context of literary, book, and textual history (instead of
the history of mathematics, for example), the universal Turing machine signifies a
culmination of several broad, long-term trends that begin with the invention of
writing itself. Since much has been written on the first two of these periods,
I will outline them briefly with the intention of elucidating the third in
greater detail:

1. The mechanization of type and the accompanying normalization of language
begins with the diffusion of movable type in China and Europe, circa 1040
(China) and 1450 (Germany) [@mcluhan_gutenberg_1962; @he_diffusion_1994;
@needham_shorter_1994, 1-34; @febvre_coming_2010]. With the invention of
movable type and the typewriter, the variability of hand-written script can be
normalized to a limited set of discrete characters. The process of
normalization continues today as contemporary technologies like natural
language processing and optical character recognition struggle to bring
non-regularized writing systems under the computational umbrella.

2. The development of distant writing (telegraphy) and remote communications,
which, although extant in many early societies (as smoke and mirror signals,
for example) accelerates dramatically at the beginning of the nineteenth
century [@shaffner_telegraph_1859; @beauchamp_history_2001;
@standage_victorian_2014]. When combined with the mechanization of type,
telegraphy amplifies the range and the speed of geographical displacement
enabled by the circulation of printed matter. The telegraph further weakens the
authorial function in transposing writing from its immediate physical
environment across vast geographical distances. The advent of telecommunication
lengthens the chain of technological mediation between author and reader. In
the absence of the identifying "hand," the telegraph clerk's and the censor's
mark cannot be distinguished from the author's (a common literary plot device
in the fiction of the period). The notion of a telegram's "fidelity" therefore
becomes an attribute of the communication channel as a whole (instead of an
attribute attached to authorial intent).

3. Finally, the rise of Turing machines in the late nineteenth and early
twentieth centuries elevates print from an intelligence-conveying medium to one
of communication and control. "Content" intended for humans is now routinely
mixed with "control codes" intended to alter the operation of the receiving
device.

It is this last and still current moment of historical development that is
least theorized. A dominant tradition in the history of the human sciences
follows Louis Althusser and Michel Foucault (and, to a lesser extent, Jacques
Derrida)[^ln2-derr] in analyzing mechanisms of structure and control in terms
of ideology, discourse, and representation. Where for Althusser "every science
is a science of ideology,"[^ln2-alt] Foucault posits his archeology of the
human sciences on the level of "positive unconscious," revealing unspoken
"rules" that govern the formation of "theories, concepts, and objects of study"
on the level of "scientific representation" [@foucault_order_1994, xi].

There are a number of theoretical niceties here (that the reader may skip, if
not interested in the history of Marxist thought). For example, the concept of
ideology, in the orthodox Marxist schema of things, exists as a "political
superstructure [...] to which correspond definite forms of social
consciousness." In the Marxist model, ideology floats *above* the material
base---the dominant mode of production---that "determines the general character
of the social, political and spiritual processes of life"
[@marx_contribution_1904, 11]. Althusser's influential contribution to this
world view was to insist that ideology, although secondary to the mode of
production, nevertheless assures its continual reproduction as a means of
continuing subjugation and political control. Simply put, for Althusser, the
ideological apparatus of the modern capitalist state ends up being "installed
in the dominant [social] position" [@althusser_lenin_2001, 85-127]. In a
similar (although not equivalent) rhetorical move, Foucault envisions that
beneath the "epistemological level" of human activity there exists an
"archaeological layer," where representation "governs the mode of being of
language, individuals, nature, and need itself" [@foucault_order_1994, 209].
Ideology and representation, once determined, now determine. The rhetorical
jujitsu inverts Hegel yet again, to put Marx back on his head.[^ln2-flip] My
short history of the universal Turing machine runs counter to this top-heavy
tradition of reading Marx in the inverse. As a device, the Turing machine fuses
control and communication. Control is meant here bluntly, as physical circuit
control, at the level of the mechanism and not at the attenuated level of
ideology or representation.

A number of inventions at the end of the nineteenth century pertain
specifically to "circuit-controlling devices controlled by a traveling
perforated strip or tape" [@cuttriss_telegraphy_1893]. Prior to perforated
tape, the transmission of messages by telegraph required the presence of a
skilled operator, able to transcribe messages from text to Morse code, and into
the physical motion of a lever-operated circuit. In this system, the human
operator acted as a mute interface between text and machine. The transcription
of text into signal, and back onto paper, required the real-time presence of
human encoders and decoders. The perforated tape decoupled the human from the
machine. In US1187035 (1916) on "Telegraphy" Albert and Ralph Bumstead explain,
"the object of our invention is to provide a system of telegraphy which does
not require skilled operators for the transmission and reception of messages"
[@bumstead_telegraphy_1916]. Instead, the message was transcribed into
perforation via mechanical means and then fed into the device.  The tape
mechanics of the typewriter could then be coupled with the electrics of the
telegraph, with perforated tape acting as a mediator between the two "worlds"
of mechanics and electricity. A number of devices emerged at the time with the
aim of transforming the mechanical action of the typewriter into perforation,
and, consequently, perforation into script, completing the circuit between
automated "encoding" and "decoding." As one device converts human input into
mechanical states, and into signal, another device converts the signal into
mechanical states and into human-legible messages.

What began as a trickle at the end of the nineteenth century ended in a flood
at the beginning of the twentieth. A multitude of devices capitalized on the
control capabilities of removable storage media. These included machines for
tape-controlled telegraphic transmission [@wheatstone_improvement_1874;
@murray_tape-controlled_1905; @bumstead_telegraphy_1916], tape-controlled
printing [@creed_printing_1911], printing telegraphs
[@hallden_printing-telegraph_1929], and remote broadcast programming devices
for radio and television content [@vriendt_program_1934; @brown_automatic_1936;
@brown_selective_1936]. With the invention of punch cards and perforated tape
(also used in textile looms, as early as 1725), a message meant for another
human became also a physical medium---bumps and holes---used to animate the
mechanical movement of the transmission apparatus.

For example, of the 33 asserted claims in the Bumstead brothers' patent, the
first 13 relate to the "transmission of intelligence [...] adapted to initiate
a succession of electrical impulses all of which have a character representing
significance, a receiver adapted to detect variations in time intervals
elapsing between successive impulses, a plurality of interpreting relays
selectively actuated by said receiver, and a printed mechanism responsive for
the combined action" [@bumstead_telegraphy_1916, 12-13]. Up to this point, the
patent describes a device for transmission information, from type to print.
Starting with clause 14, the language changes to describe a more general
"telegraph system," involving "an impulse transmitter having means to vary the
time intervals between successive transmitted impulses, each time interval
having a character *representing significance*, a receiver responsive to said
devices, and signal, *distributing*, *interpreting*, and recording devices
responsive to the contiguous significant time intervals defined by the impulses
[emphasis mine]" [@bumstead_telegraphy_1916, 14]. For the automated telegraph
the control code and the message are one: that is, the arrangement of
perforation on the ticker tape affects the internal mechanical configuration of
the machine. Another way to put this would be to say that the state of the
tape-driven telegraph machine at any given time is also, in part, a message
meant for the human recipient.  The printing telegraph of Brothers Bumstead,
along with their subsequents and antecedents, contain all the necessary parts
for a Turing machine: discrete symbolic language, removable storage medium, and
a device that can alter its internal state based on the reading and writing
passing symbols.

By 1905, Murray (of the Murray telegraph and alphabet) can write that "if we
disregard the small class of telegrams that merely express emotions, *the
essence of telegraphy is control* [emphasis mine]." He goes on to write that
"telegraph systems, therefore, belong not to the class of producing or
distributing, but to the class of controlling mechanisms"
[@murray_setting_1905, 556].

With this history in mind, I am interested in reformulating the metaphysical
anxiety about literature and value formation in terms of text and control. If
you are reading these words on a screen, my message has reached you through a
long chain of transformations: from the mechanical action of my keyboard, to
the arrangement of electrons on magnetic storage media, to the modulation of
fiber-optic signal, to the shimmer of the flowing liquid crystal. Many, many
third parties were involved in that act of communication. And some part of that
message was used to control the electrical circuits of the device in your lap,
in your hand, near to your eye, embedded, or embodied. Close reading limited to
the parsing of content and representation risks missing the machinations of
naked circuit control. It looks like you are reading a book, but this book
changes its internals as it receives instructions to inform and to control. For
now I ask only, Who controls the device? But were a book also a pill or fused
with the neural circuitry of the brain, would you know what and whom you were
reading?


[^ln2-gurevich]: Kittler mistakingly attributes "Algorithms in the World of
Bounded Resources" to Brosl Hasslacher. The author is rather Yuri Gurevich,
Principle Researcher at Microsoft Research and then a professor at the
University of Michigan. Hasslacher's essay entitled "Beyond the Turing Machine"
appeared in the same volume of collected essays, @herken_universal_1988.

[^ln2-bottom]: For example, in the Open Systems Interconnection (OSI) model of
communication, the top-most layer of protocols and interface method is called
the "application layer" and the bottom-most layer the "physical layer"
[@peterson_computer_2007, 26-28]. Timothy Colburn and Gary Shute describe it as
being "responsible for encoding bits onto a transmission medium, whether wires,
fiber optics, or radio broadcast, in ways that maximize the transmission rate
and minimize sensitivity to noise [@colburn_abstraction_2007, 181].

[^ln2-abstraction]: This is a topic of some contention in the literature. In
his influential paper on the topic, James Moor includes the immateriality of
software as one of the "three myths" of computer science. "As a practical
matter, what we regard as computer instructions, and consequently what we
regard as computer programs, is determined by computers available," he writes
[@moor_three_1978, 215]. Nurbay Irmark argues that software is instead a purely
abstract artifact, akin to a musical work [@irmak_software_2012]. See also
@turner_programming_2013; @colburn_software_1999.

[^ln2-turing]: The intellectual history of the Turing machine is well
established, in multiple works on the subject. It follows the Greek Diophantus,
René Descartes, Georg Cantor, David Hilbert, Gottlob Frege, Bertrand Russell,
Kurt Gödel, Ludwig Wittgenstein [@petzold_annotated_2008;
@herken_universal_1988; @grattan-guinness_development_1981].

[^ln2-alt]: "We have to think (in a completely novel way) the relation between
a science and the ideology [...] the fact that such an investigation confronts
us with the observation that every science, in the relationship it has with
ideology it emerged from, can only be thought as a 'science of ideology, would
disconcert us, were we not forewarned of the name of the *object* of knowledge,
which can only exist in the form of ideology" [@althusser_reproduction_2014,
46].

[^ln2-derr]: See @derrida_writing_1978. I am alluding particularly to
statements like "ethnology-like any science-comes about within the element of
discourse," and "this moment was that in which language invaded the universal
problematic; that in which, in the absence of a center or origin, everything
became discourse-provided we can agree on this word-that is to say, when
everything became a system where the central signified, the original or
transcendental signified, is never absolutely present outside a system of
differences. The absence of the transcendental signified extends the domain and
the interplay of signification ad infinitum" (278-294).

[^ln2-flip]: There is a long-standing joke in Marxist literature that involves
flipping Hegel, who prioritized the transcendent spiritual over the physical
and material forms of life, over "back to his feet." See for example
@marx_marx-engels_1978: "The form of wood, for instance, is altered, by making
a table out of it. Yet, for all that, the table continues to be that common,
every-day thing, wood. But, so soon as it steps forth as a commodity, it is
changes into something transcendent. It not only stands with its feed on the
ground, but, in relation to all other commodities, it stands on its head, and
evolves out of its wooden brain grotesque ideas, far more wonderful than
'table-turning' ever was" (320). See also @engels_ludwig_1941: "Thereby the
dialectic of the concept itself became merely the conscious reflex of the
dialectical motion of the real world and the dialectic of Hegel was placed upon
its head; or rather, turned off its head, on which it was standing before, and
placed on its feet again" (44).

[^ln1-dirt]: Think of text as substance and literature as surface. The first is
like dirt where the second is gound. We play with one on top of the other.

[^ln1-gate]: Solid state memory devices like flash memory store information in
capacitor "circuit states." This by contrast with electromagnetic storage that
works by modulating electrical charge over a magnetic surface. Solid state
capacitor storage was used in the earliest computers
[@kahng_semipermanent_1967, 1296], but was prohibitively expensive to
manufacture until well into the twenty-first century, when solid state drives
began to replace electromagnetic storage on consumer devices. In an early
(1967) paper on "A Floating Gate and Its Application to Memory Devices," Kahng
and Sze explain: "A structure has been proposed and fabricated in which
semipermanent charge storage is possible. A floating gate is placed a small
distance from an electron source. When an appropriately high field is applied
through an outer gate, the floating gate charges up. The charges are stored
even after the removal of the charging field due to much lower back transport
probability [...] Such a device functions as a bistable memory with
nondestructive read-out features. The memory holding time observed was longer
than one hour" [@kahng_floating_1967, 1288]. See also @horton_experimental_1962
and @frohman-bentchkowsky_fully_1971 on "floating gate avalanche injection."

[^ln1-denning]: These stages correspond roughly to the "three generations of
electronic computing" outlined in Peter Denning's "theory of operating systems"
[@denning_third_1971].

### Works Cited
