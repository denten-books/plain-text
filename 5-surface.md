# Chapter 5: Recondite Surfaces

The scholar Wendy Hui Kyong Chun views magnetic storage as an "enduringly
ephemeral" medium, which "creat[es] unforeseen degenerative links between
humans and machines" [@chun_enduring_2008, 148]. If the floating gate
transistor, where my text now lives, can be called enduring and ephemeral, I
will call my liquid crystal display, where the text shows itself, fading and
persistent.[^ln2-gate] This is not to suggest that digital text transcends its
material contexts. Only to observe that it drifts from surface to surface,
beyond the field of vision, in ways that erode trust its general permanence.
If we are to dwell on surfaces, I do not know where to look: the screen or the
floating gate.

Where does text reside? We know the answer for print: it is there, stuck to a
page. Computational media complicate our ability to situate inscription. We
point at screens, but without certainty; their surfaces remain in flux. They
are sites of fleeting projection, which emanates from hidden storage media.
Digital text passes through multiple filters and transformations on the way to
the surface. Though we observe the inscription on screen only, it embodies a
complicated figure, stretched across at least two sites. The sign strains and
splits: between the configuration of magnetic pulse and circuit state at one
end, and in the phases of liquid crystal on screen at the other. Both
locations afford distinct constraints to interpretation. Familiar actions like
"reading" and "writing" imply distinctly differing operations depending on the
surface.

Thus when Michael Heim refers to the "ephemeral quality" of the electronic
text or when Pamela McCorduck describes it as "impermanent, flimsy, malleable,
[and] contingent" they are correctly identifying a facet of digital
inscription at its site of projection. McCorduck tells the story of a
rabbinate court, which, when interpreting a law that prohibits observant Jews
from erasing God's name, deemed that words on a screen do not constitute
writing, and therefore sanctioned their erasure [@mccorduck_universal_1985,
51; also quoted @heim_electric_1987, 192].

Conversely, when scholars like Johanna Drucker, Katherine Hayles, or Matthew
Kirschenbaum respond to Heim and company with hardened materialism, they are
also rightly locating properties of digital inscription, at the site of its
archival immanence. The two schools of thought disagree because they speak
neither of the same phenomenon nor at the same site. The former group
highlights ephemeral, transcendent qualities of the projected word. The latter
foregrounds the "uniquely indelible nature of magnetic storage," "drives,
tracks, and disks," "fundamental physical support," and "material substrates
of computing" [@kirschenbaum_mechanisms:_2008, 51;
@drucker_performative_2013, para. 2]. Screen and hard drive partition the sign
between surface and depth, projection and archive. At the site of projection
one speaks of texts that are---I have kept a list of adjectives when reviewing
the literature---*ephemeral*, *shimmering*, *electric*, *flimsy*,
*contingent*, *malleable*, *impermanent*, *flowing*, *transcendent*,
*ghostly*, and *radiant*. At the archival site, scholars describe inscription
as *enduring*, *solid*, *persistent*, *permanent*, *indelible*, *hard*,
*immanent*, *lasting*, *palpable*, and *concrete*.

The seeming immateriality of digital media has real-world effects: the costs
of replication plummet, for example. Computational text can be copied
effortlessly, with minimal cost and near-perfect fidelity.
Hyper-reproducibility of the sort was unthinkable for most of the twentieth
century. Lowered costs of copying further reduce barriers to access and
conveyance. Symbols that adhere lightly to their medium are easy to store and
transport. Public knowledge works like search engines and massive open online
encyclopedias subsequently claim to organize "all of the world's knowledge."
Such projects unfold by the logics of immateriality: total archives, big data,
and universal accessibility.[^ln1-google] When the sign appears to weigh
nothing, one imagines its weightless aggregate, available immediately and
everywhere. The perceived weightlessness of text has other side-effects as
well. A text that adheres lightly to its medium is difficult to contain.
Bureaucracies struggle to restrict it: in speech, sharing, or dissemination.

Despite appearances, text cannot be unmoored from its material foundations.
Rather, it splits in two: with one half arising visibly, a weightless and
apparent screen simulation, and the other half, opaque, etched into hefty and
hidden material contingencies. The material substrates of computational text
likewise carry real-world consequences. First and foremost they are hidden
from view. Drives and tapes reside inside of boxes encased in plastic and
aluminum. If only because they carry electrical current and are thus
flammable, they are regulated. They contain heavy and rare metals, often
hazardous to touch or ingestion. They become toxic when discarded. Flash
memory cards, USB sticks, solid state storage devices, and magnetic disk
drives contain circuitry to prevent unauthorized access and resist tempering.
Within them, text intertwines with machine internals, sealed hermetically and
hermeneutically, in a way that resists human interpretation. It is inscription
not compatible with humans, machine literature, a hazard.

Form and content lie flat in print. Print interfaces are paper thin. Ink
adheres to paper in a way that pixels do not to screens. Textual fissure
complicates the traditional structuralist distinctions between form and
content. The computed inscription occupies at least two distinct sites, each
entailing drastically differing affordances for interpretation. Were we to
untangle the tightly wound coil of the circuit, we would find ample distance
between hard drive and screen. Some readers measure that distance in inches,
as when reading documents locally for example, stored on their own computer;
and in miles when reading documents remotely, stored on far away "cloud"
servers. Where print is flat, digital inscription is stratified across
multiple incongruent planes.[^ln2-hayles]

How did this multiplicity come to be? And what effect does it have on the life
of the mind? In this chapter I will make visible the gap between projected
sign and archived inscription. I will begin by providing a historical account
of a letter's passage from paper to pixel. My narrative arc proceeds in three
stages, which I summarize here and expand upon later.

First, with the advance of telecommunications, we observe an emerging divide
between human-readable text and machine-readable code. Removable storage media
like ticker tape and punch cards embodied a machine instruction set, meant to
actuate mechanisms which in turn produced human-legible inscriptions.
Unintelligible (to humans without special training) control codes "driving"
the machine were thereby mixed with plain text, the content of communication.
Inscription split between sites of storage, which archives an expanded machine
instruction set, and projection, which displays human alphabets.

Second, where ticker tape and punch cards were legible to the naked eye,
magnetic tape storage made for an inscrutable medium, inaccessible without
instrumentation. In the 1950s and 1960s, machine operators worked "blindly,"
using complicated workarounds to verify equivalence between input, storage,
and output. Writing began to involve multiple "typings," "printings," and
specialized magnetic reading devices to establish the correspondence between
input, storage content, and output of entered text. The physical properties of
electromagnetic inscription situate it, in practice, beyond human sense.

Finally, the appearance of Cathode Ray Tube (CRT) displays in the late 1960s
restored a measure of legibility lost to magnetic storage media. The sign
reemerged on screen. Crucially, it now framed a simulacrum of archived
inscription. Typing a word on keyboard produced one sort of a structure on
tape or disk and another on screen. The two relate contingently, without
necessary equivalence. The lay reader has no means to ensure the
correspondence between visible trace and stored mark. An opaque "black box" of
automated word "processing"---rules for transmediation---intercede between
reading and writing.

A number of textual machines will illustrate the above history. Three
mechanisms mark the journey: the Controller patented by Hyman Goldberg in
1911; Magnetic Reader introduced by Robert Youngquist and Robert Hanes in
1958; and, for a lack of a better name, Time Fob, introduced by Douglas
Engelbart in 1968. Goldberg attempted to heal the rift between human and
machine alphabets by inventing a mechanical punch card that could move minds
and levers alike. His device traced an alphabet understood by humans and
machines both. Youngquist and Hanes gave human operators a glimpse into the
hidden world of magnetic polarities and electric charges. Engelbart's Time Fob
belongs to what Peter Denning, a prominent computer scientist, has called the
"third generation" of computer systems. It is an assemblage of storage, input,
and output technologies that continue to shape our contemporary encounter with
text today.[^ln2-denning] Together, these devices tell a story of a fissure at
the heart of our contemporary textual predicament.

## A. Programmable Media

To begin, some background on telegraphy. The turn of the twentieth century was
a pivotal period in the history of letters. It saw the languages of people and
machines enter the same mixed communications stream. Artificial, fixed-length
alphabets like the Baudot code paved the way for the automation of language.
The great variety of human scripts was reduced to a set of discrete and
reproducible characters. So regularized, type was converted into electrical
signal, sent over great distances, and used to program machines remotely. But
these new affordances came at a price of legibility. Initially, a cadre of
trained machine operators was required to translate human language into
machine transmittable code. Eventually, specialized equipment automated this
process, removing the human from the equation. Machines could communicate with
other machines without human intervention, triggering a chain of cascading
events that reverberate today: from algorithmic financial trading to the war
between junk (spam) mail generators and their filters.

The advent of programmable media---punch cards and ticker tape---coupled
human-compatible alphabets with machine control code. Reduced to a discrete
and reliably reproducible set of characters, natural languages could now be
conveyed as electric signals. In such a transitive state language became more
mobile than ever before. It was transmitted efficiently across vast distances.
The mechanization of type introduced new control characters into circulation,
that affected machine state changes at a distance. Initially, such state
changes were simple: "begin transmission," "sound error bell," "start new
line." With time, they developed into what we now know as programming
languages. Content meant for people was being routinely intermixed with code
meant to control machine devices. Such early remote capabilities were quickly
adapted to control everything from radio stations to advertising billboards
and knitting machines [@hough_wired_1931; @adler_knitting_1933;
@casper_remote_1934].

Language compressed and pushed through the wires underwent a number of
transformations. Donald Murray, the inventor of the popular Murray telegraph
alphabet, conceived of telecommunications in terms of time and space.
Advancing a self-professed "metaphysics of telegraph signalling alphabets," he
described spatial writing symbols that "appeal to the eye," and temporal,
"telephonic" signals that "appeal to the ear." Paradoxically, space signals
(like words on a billboard) occupy little space, but persist in time. "For
instance," Murray wrote, "a signboard may extend over 10 feet and 100 years"
[@murray_setting_1905, 36]. Time signals, by contrast, dilate in space where
they contract in time. "A Morse signal in a wire may extend over half a
second," Murray wrote, and 500 miles [@murray_setting_1905, 86].

Physical structures like sentences and paragraphs that appear on a page take
shape in the reader's mind. Unlike a painting, a paragraph cannot be perceived
wholly and at once; it must be reassembled mentally. The printed word thus
extends in time. By contrast, the electric signal is near instantaneous. It
claims space however in transmission. A perceptive reader of electric letters
reconstructs the sign's spatial along with its temporal characteristics. The
digital inscription gains a new dimension which extends away and beyond the
reader's field of vision. Elongation in space compounds elongation in time to
complicate the tactics of reading.

The electronic letter's emergent physics presented new technological and
administrative challenges. As telegraphy spanned national boundaries,
agreements were needed to standardize conventions for equipment and message
encoding [@international_telegraph_union_journal_1899, 82-91]. These were
handled on a regional, ad-hoc basis until 1865, when the International
Telegraph Union (ITU) was created. The International Telegraph Conference
(ITC) in Paris, held between March 1 and May 17 of 1865, adopted, among other
things, the use of a modified Morse code character set, containing 33 Latin
letters (including characters from the French, German, and Spanish alphabets),
10 numbers (0--9), 14 punctuation marks (including a fraction bar), and 10
control codes (including "end of service," "attention," and "error")
[@international_telegraph_union_documents_1865]. In specifying the conversion
tables for the Morse alphabet, the 1865 ITU rules required a silence equal to
three dots (or one dash) to indicate the space between two letters, and a
silence equivalent to four dots (later changed to seven) to indicate a gap
between words.

Although Morse code is commonly imagined as a binary code, comprising ones and
zeroes, it is technically ternary, because it uses three elements: dots,
dashes, and silences. In Morse code, characters are of variable length: one
dash for "t," for example, and six dots for the number six. The transmission
of variable length codes thus required the presence of human operators who
could "translate" from natural to machine language using "keys" which when
depressed vertically would complete an electric circuit to produce the signal.
The key interfaced between disparate conduits---paper and wire---that hosted
human and machine languages.

Writing in 1929 for the journal *American Speech*, Hervey Brackbill preserved
some of the specialized language associated with Morse code culture. "Morse
telegraphy is commonly referred to as a 'game,'" he wrote, "and the operator
'works a wire.'" Operators tapping keys used "bugs," which where insect-like
machines that had "long slender levers and springs." Bugs had trade names like
"Lighting Bug," "Gold Bug," and "Cootie" (for a small model)
[@brackbill_telegraphers_1929, 288].

Operators using "straight keys" achieved speeds upwards of 25--30 words per
minute, limited by the shortest possible length of the smallest transmitted
unit (a dot), fixed by the American and International Morse Code conventions
to 1/24 of a second in duration.

At the time, companies like Vibroplex began manufacturing semi-automatic keys,
which made use of horizontal switches capable of emitting a rapid succession
of dots to one side of the action, and dashes to the other
[@martin_telegraphic_1904; @boyd_telegraph-key_1916]. A semi-automatic bug was
said to "run away" when adjusted for too high of a speed. Vibroplex keys would
also greatly alter the "fist," or the operator's individual transmission
style, by which telegraphers could be previously recognized. They allowed for
speeds approaching 50 words per minute and above, when not following the
minimum signal length specifications [@mcnicol_american_1913, 207;
@u.s._bureau_of_labor_statistics_displacement_1932; @halstead_genesis_1949].
Despite such improvements, Morse code was showing its age. Although the
convention stipulated fixed length, the actual length of silences between
meaning-carrying units varied greatly with the vagrancies of transmitting
media. Cross talk between wires and weather interference was common;
communicating in Morse still required a human operator for efficient ciphering
and deciphering. In a long-chain of time-sensitive transformations between the
message and its recipient, the human posed a limiting factor.

Telegraph operators working a "hand sender" often developed partial paralysis
in their wrists or arms. The condition was commonly known as "glass arm"
[@brackbill_telegraphers_1929, 288;
@u.s._bureau_of_labor_statistics_displacement_1932]. Senders were called
"hams" or "bums" when they "fell down" or made frequent errors
[@brackbill_telegraphers_1929, 288]. To "put someone under the table" in
sender's lingo was to transmit faster than a receiver could transcribe. The
"reader" was "burnt up" when he fell behind. He had to "break," or interrupt
the sender to ask for repetition [@brackbill_telegraphers_1929, 289]. To
"paste" someone was to deliberately burn him up.[^ln4-men]

[^ln4-men]: See @brackbill_telegraphers_1929, 288-9. Operators at the time
were all almost exclusively male.

The 1908 ITC in Lisbon, ratified two additional alphabet standards for
international use: Hughes and Baudot. Both were developed in response to
Morse's limitations and allowed for fully autonomous telegraph operation. The
Hughes telegraph, an 1855 design modification of the 1846 American Royal E.
House model, was a capricious machine that relied on a tuning mechanism to
transmit individual characters. It was inspired by the player piano and even
looked like one, complete with a keyboard and rotating drum. Its sending
device struck a tone which, when transmitted by electrical current, initiated
the rotation of a similar drum in the receiving apparatus "at the pleasure of
the distant operator." A length of time---between the initial synchronization
signal and struck chord---corresponded to a letter, which the machine then
printed to tape using a letter wheel. Hughes referred to his invention as a
"Compound Magnetic and Vibrating Printing Instrument," a name that hints at
its fragility. When the sending and receiving drums fell out of sync, the
message became impossible to decipher [@hughes_improvement_1856;
@hayles_print_2004, 145-147; @noll_evolution_2007, 20-21]. Precise
coordination between the various drums and their operators was of paramount
importance.

The number and size of telegraph cables further limited the system's
"information density." Independent developments in communication technology
led to a range of techniques for sharing the same wire to send multiple
messages, known as "multiplexing." These fell into two categories:
time-division and pulse-amplitude modulation [@rowland_multiplex_1901;
@hausmann_telegraph_1915]. Pulse-amplitude modulation involved filling the
available space (bandwidth) with simultaneous signals of different types.
Imagine someone speaking loudly and quietly at the same time into the same
channel. In this way, all loud messages could be sorted to the one side and
all quiet ones to the other, which would effectively utilize the whole sonic
spectrum.[^ln3-multi] Initially, pulse-amplitude modulation was difficult to
implement. Cross-channel noise and device sensitivity hampered reliable
reception and decoding across multiple simultaneous wave frequencies.

Emile Baudot, among others, noticed that the prevailing Morse and Hughes
telegraph systems also failed to make full use of the *time* allotted for
message transmission. Hughes telegraphs in particular extensively used long
silences, which could have been condensed to improve bandwidth. Baudot-type
time multiplexers utilized synchronized rotating mechanisms at the sending and
receiving ends. These distributed units of time among multiple operators. One
operator could, for example, send messages on top of the minute and the other
at the bottom. In this way, receivers could sort messages based on time
signatures: all those sent on top of the minute would go into one pile, all
those on the bottom, into another. A "duplex" printing telegraph, of the
Murray type, involved a complicated synchronization device, known as the
"distributor," capable of orchestrating two distinct streams of transmission
along the same channel. Taking turns to transmit to the regular tolling of a
tuning bell, two operators would send separate messages along the same
channel. The receiving machine would then separate one message from the other
based on its time signature [@murray_setting_1905].

![Multiple Printing Telegraph, 1905 [@murray_setting_1905,
574].](images/multiplex.png)

It was important to maintain unison in such multiplexed systems. Time-shared
device operators had to know when it was their turn to type. The Baudot
multiplexer used time- or cadence- "tapper" mechanisms to indicate turns. In
some devices, they locked keyboards to prevent out-of-turn input. In a
quadruplex system, up to four operators could engage in an intricate dance of
fingers, keys, tappers, and signals, synchronized by the rotation of the
telegraph distributor.

Synchronization was also needed for Morse code devices, to distinguish
non-meaning carrying silence (the receiver is turned off) from the
meaning-carrying one (the receiver pauses to indicate a dot). The receiving
device needed a measure, a duration of silence to differentiate between "dots"
and spaces between words, which was indicated by silences of different length.
When the devices went out of sync, or if communication lagged for some reason,
the coherence of the message faltered. Sending and receiving machines had to
be tuned to a cadence of common information exchange. The system of operators,
transmitters, and receivers, was, in aggregate, tuned to a specific but
arbitrary rhythm by which certain messages made sense only in particular (also
arbitrary) units of time. In early telegraphy, these units of time were slower
than natural human time, enough so for the operator to remain idle. Later
systems increased the pace to a rhythm beyond natural human abilities of
comprehension, to a point where human operators could no longer decipher
signals without machine assistance. In the language of wiremen, bugs were
"running away" with the whole "game" [@brackbill_telegraphers_1929, 288]. A
human operator examining wire signals directly would find a jumble of data.
This, in fact, is a decent provisional definition of data: information beyond
human time, not amenable to unmediated interpretation.

Hundreds of alphabet systems were devised to speed up automated
communications. These evolved from variable-length alphabets like Morse and
Hughes, to fixed-length alphabets like Baudot and Murray. The systematicity of
the signal---always the same length, always at the same time---moved the dial
further from natural human languages, which rely on affect and variation, to
artificial codes, which necessitate consistency and reproducibility.

The discovery of *binary* arithmetic belongs to Gottfried Leibniz, who,
influenced by hexagrams of the *I Ching*, articulated his own system in his
1979 *Explication de l'Arithmétique Binaire* [@leibnitz_explication_1703].
Francis Bacon deserves credit for articulating a *fixed-length* binary code.
In the sixth book of his *De augmentis scientiarum*, an encyclopedic treatise
on the "partition of sciences," Bacon mentioned a "highest degree of cipher"
that could signify "all in all" [*omnia per omnia*].[^ln2-bacon] He proceeded
to describe a "fivefold," "bi-literarie" alphabet, which encoded each letter
of the English language using a five-character long string of As and Bs. The
letter A, for example, became "aaaaa." B became "aaaab," C "aaaba," and so on
to Z, rendered as "babbb." Unlike other known cypher systems, Bacon's
characters all took up exactly five spaces. "Neither is this a small matter,"
Bacon wrote:

> these Cypher-Characters have, and may performe: For by this *Art* a way is
> opened, whereby a man may expresse and signifie the intentions of his minde,
> at any distance of place, by objects which may be presented to the eye, an
> accommodated to the eare: provided those objects be capable of a twofold
> difference only; as by Bells, by Trumpets, by Lights and Torches, by the
> report of Muskets, and any instruments of like nature [@bacon_advancement_1987,
> 266].

Writing more than two centuries before electric telegraphy, Bacon eloquently
described its essence, which lay in expressing and signifying the human mind
at a distance.

The fixed-length property of Bacon's cipher, later implemented in the 5-bit
Baudot code, signaled the beginning of the modern era of serial communications
[@jennings_annotated_2004]. The Baudot and Murray alphabets were designed with
automation in mind.[^ln1-murray] Both did away with the "end of character"
signal that separated letters in Morse. Signal units were to be divided into
letters by count, with every five codes representing a single character.
Temporal synchronization was therefore unnecessary given the receiver's
ability to read the message from the beginning. A character was simply a unit
of space, divisible by five.

Additionally, the Murray code was more compact than Morse and especially more
economical than Hughes, which used up to 54 measures of silence to send a
signal representing double quotes.[^ln1-zero] The signal for "zero" in Morse
code occupied 22 measures. By contrast, all Baudot and Murray characters were
a mere five units in length, with the maximum of ten used to switch the
receiving device into "figure" or "capital letter" states (for the total of
ten units) [@murray_setting_1905; @beauchamp_history_2001 380-397].

<!-- ![Murray Keyboard Perforator, 1905. Note the QWERTY arrangement of the keys
[@murray_setting_1905].](images/murray-keyboard.png) -->

![Table of Alphabets, 1901 [@vansize_new_1901, 23].](images/alphabets-vansize.png)

Fixed-length signal alphabets drove the wedge further between human and
machine communication. Significantly, the automated printing telegraph
decoupled information encoding from its transmission. Fixed-length encoding of
messages could be done in advance, with more facility and in volume. The
prepared message could then be fed into a machine without human assistance. In
1905, Donald Murray wrote that the "object of machine telegraphy [is] not only
to increase the saving of telegraph wire [...] but also to reduce the labour
cost of translation and writing by the use of suitable machines"
[@murray_setting_1905, 557]. Baudot's and Murray's codes were not only
shorter, they were simpler and less error-prone, and thus resulted in less
complicated and more durable devices.

With the introduction of mechanised reading and writing techniques, telegraphy
diverged from telephony to become a means for truly asynchronous
communication. It displaced signal transmission in time as it did in space.
The essence of algorithms lies in their ability to delay execution: a cooking
recipe, for example, allows novice cooks to follow instructions without the
presence of a master chef. Similarly, delayed communication could happen in
absentia, according to predetermined rules and instructions. A message could
activate a machine that prints a log and another that trades stocks and
another that replies with a confirmation.

The new generation of printing telegraphs was "programmed" in this way using
removable storage media, similar to the way player pianos were programmed to
play music by means of music rolls. Just like perforated music rolls decoupled
music-making from its live performance, programmable media decoupled
inscription from transmission. In both cases, one could stroke keys now, only
to feel the effects later. Programmable media contained the instructions for
desired effects for later execution. Ticker tape and punch cards thus enabled
the advanced preparation of messages, which could then be "fed" into the
mechanism for transmission at rates far exceeding the possibilities of
hand-operated Morse telegraphy. Besides encoding language, the Baudot schema
left space for several special "control" characters. The "character space"
could thus further be expanded by switching the receiving mechanism into a
special "control mode" in which every combination of five bits represented an
individual control character (instead of a letter). Content and control became
intertwined and began to occupy the same spectrum of communication.

By the 1930s, devices variously known as "printer telegraphs,"
"teletypewriters," and "teletypes" displaced Morse code telegraphy as the
dominant mode of commercial communication. A 1932 U.S. Bureau of Labor
Statistics' report estimated more than a 50 percent drop in Morse code
operators between 1915 and 1931. Morse operators referred to the tele-typists
on the sending side as "punchers" and on the receiving side as "printer
men."[^ln2-printermen] The printer men responsible for assembling pages from
ticker tape were called "pasters" and sometimes, derisively, as "paperhangers"
[@brackbill_telegraphers_1929]. Teletype automated this entire process,
rendering punchers, pasters, and paperhangers obsolete. Operators could now
enter printed characters directly into the machine, using a keyboard similar
to the typewriter, which, by that time, was widely available for business use.
The teletype would then automatically transcode the input into transmitted
signal and then back from the signal onto paper on the receiving end.

As Bacon's early writings on the language arts suggest, the roots of
telegraphy lie in cypher-making and cryptography. It is no surprise, then,
that the encoding of human languages for machine use was intimately connected
to war-time, diplomatic, and otherwise clandestine communications. The
seemingly innocuous problem of machine translation was therefore inextricable
from questions of access and legibility: who gets to understand the encoded
message and when? To understand the effects a message would have in
transmission, one had to understand its encoding.

For example, the Final Protocol to the Telegraph Regulations, ratified in
Madrid in 1932 by the governments of more than seventy countries, included a
special provision delineating the difference between transmitting "plain" and
"secret" language. The protocols grouped "secret languages" into "code" and
"cypher" categories. "Plain language" was defined as words which present "an
intelligible meaning in one or more of the languages authorised for
international telegraph correspondence, each word and each expression having
the meaning normally assigned to it in the language to which it belongs"
[@itu_telegraph_1932, 12]. By contrast, "code language" was defined as
"composed either of artificial words, or real words not used in the meaning
normally assigned to them in the language to which they belong and
consequently not forming intelligible phrases" [@itu_telegraph_1932, 12].

The terms of the convention were binding. Codes were not permitted to contain
more than five characters and were charged, by contrast with plain text, at
6/10th of the agreed tariff rate. Upon request, senders were required to
"produce the code from which the text or part of the text of the telegram has
been compiled" [@itu_telegraph_1932, 13]. Otherwise, the language was
considered a secret "cypher," defined as "groups or series of Arabic figures
with a secret meaning." Participants agreed to accept and pass telegrams in
plain language through their jurisdictions. They were not, however, obliged to
accept or help deliver secret messages [@itu_telegraph_1932, 13].

Machine code thus occupied a gray area between plain text and cypher.
Theoretically, it was considered intelligible only when its compilation
sources were available to the transmitter. Without its sources, it became
equivalent to secret communication. In practice, the proliferation of
encodings and machine instructions had the effect of selective illiteracy.
Telegraph operators carried with them multiple cheat-sheets: small cards that
reminded them of each system's particularities. Telegraphy re-introduced the
problem of legibility into human letters. To speak in telegraph was to learn
arcane encodings, which required specialized training. A number of failed
communication schemas attempted to heal the rift between human and machine
alphabets.

![Goldberg's Control Cards [@goldberg_controller_1915].](images/control-2.png)

"You must acknowledge that this is readable without special training," Hymen
Goldberg wrote in the patent application for his 1911 Controller
[@goldberg_controller_1915, sheet 3]. The device was made "to provide [a]
mechanism operable by a control sheet which is legible to every person having
sufficient education to enable him to read." In an illustration attached to
his patent, Goldberg pictured a "legible control sheet [...] in which the
control characters are in the form of the letters of the ordinary English
alphabet" [@goldberg_controller_1915, 1]. Goldberg's perforations did the
"double duty" of carrying human-readable content and mechanically manipulating
machine "blocks," "handles," "terminal blades," and "plungers"
[@goldberg_controller_1915, 1-4]. Unlike other schemas, messages in Goldberg's
alphabet could be "read without special information," effectively addressing
the problem of code's apparent unintelligibility.

The inscription remained visible at the surface of Goldberg's control sheet,
as a perforated figure, punched through the conduit. Whatever challenges punch
cards and ticker tape presented for readers, these were were soon complected
by the advent of another medium, even more mute and inscrutable, the magnetic
tape.

## B. Textual Laminates

"Historically unforeseen, barely a thing, software's ghostly presence produces
and defies apprehension," Wendy Chun wrote in her *Programmed Visions*, an
influential monograph that continues to shape the field of software studies
[@chun_programmed_2011, 3]

One could hardly call early programmable media ephemeral. Anecdotes circulate
about Father Roberto Busa, an early pioneer of computational philology, who,
in the 1960s, carted his punch cards around Italy in his truck.[^ln2-busa]
Codified inscription, before its electromagnetic period, was fragile and
unwieldy. Just like writing with pen and paper, making an error on ticker tape
entry required cumbersome corrections and sometimes wholesale re-entry of
lines or pages. On the surface of ticker tape, the inscription still made a
strong commitment to the medium. Once committed to paper it was
near-immutable. Embossed onto ticker tape or punched into the card, early
software protruded through the medium.

In the age of telegraphy, encoding stood in the way of code comprehension.
Morse code and similar alphabet conventions at least left a visible mark on
the paper. They were legible if not always intelligible. Once the machine
encoding was identified---as Morse, Baudot, or Murray---it could be translated
back into natural language using a simple lookup table.

Magnetic tape changed the commitment between inscription and medium. It
provided a temporary home, where the word could be altered before being
committed to paper. At the 1967 symposium on electronic composition in
printing, Jon Haley, staff director of the Congressional Joint Committee on
printing, spoke of "compromises with legibility [which] had been made for the
sake of pure speed in composition and dissemination of the end
product."[@technology_electronic_1967, 48] A new breed of magnetic storage
devices allowed for the manipulation of words in "memory," on a medium that
was easily erased and rewritten. The magnetic charge adhered lightly to tape
surface. This "light touch" gave the word its new-found ephemeral quality. But
it also made inscription illegible. In applications like law and banking,
where the fidelity between input, storage, and output were crucial, the
immediate illegibility of magnetic storage posed a considerable engineering
challenge. After the advent of teletype, but before screens, machine-makers
employed a variety of techniques to restore a measure of congruence between
invisible magnetic inscription and its paper representation. What was entered
had to be verified against what was stored.

The principles of magnetic recording were developed by Oberlin Smith (among
others), an American engineer who also filed several inventions related to
weaving looms. In 1888, inspired by Edison's mechanical phonograph, Smith made
public his experiments with an "electrical method" of sound recording using a
"magnetized cord" (cotton mixed with hardened steel dust) as a recording
medium. These experiments were later put into practice by Valdemar Poulsen of
Denmark, who patented several influential designs for a magnetic wire recorder
[@smith_possible_1888; @poulsen_method_1900; @engel_1888-1988_1988;
@thiele_magnetic_1988; @daniel_magnetic_1998; @vasic_coding_2004].

Magnetic recording on wire or plastic tape offered several advantages over
mechanical perforation. Tape was more durable than paper; it could fit more
information per square inch; and it was reusable. "One of the important
advantages of magnetic recording," Marvin Camras, a physicist with the Armour
Research Foundation, wrote in 1948, "is that the record may be erased if
desired, and a new record made in its place."[@camras_magnetic_1948, 505] Most
early developments in magnetic storage were aimed at sound recording. The use
of magnetic medium for data storage took off in earnest around the 1950s
[@dee_magnetic_2008, 1775]. That said, some early developers of
electro-magnetic storage and recording technology already imagined their work
in dialog with the long history of letters. Addressing the Franklin Institute
on December 16th of 1908 Charles Fankhauser, the inventor of the
electromagnetic "telegraphone," said:

> To transport human speech over a distance of one thousand miles is a
> wonderful achievement. How much more wonderful, then, is the achievement
> that makes possible [...] its storage at the receiving end, so that the
> exact sentence, the exact intonation of the voice, the exact timbre, may be
> reproduced over and over again, an endless number of times.
> [@fankhauser_telegraphone_1909, 37-8]

Comparing magnetic recording to the invention of the Gutenberg press,
Fankhauser added that:

> It is my belief that what type has been to the spoken word, the telegraphone
> will be to the electrically transmitted word. [...] As printing spread
> learning and civilization among the peoples of the earth and influenced
> knowledge and intercourse among men, so I believe the telegraphone will
> influence and spread electrical communication among men."
> [@fankhauser_telegraphone_1909, 40]

In that speech, Fankhauser also lamented the "evanescen[ce]" of telegraph and
telephone communications. The telephone, he rued, fails to preserve "an
authentic record of conversation over the wire"
[@fankhauser_telegraphone_1909, 39-40]. Fankhauser imagined his telegraphone
being used by "the sick, the infirm, [and] the aged":

> A book can be read to the sightless or the invalid by the machine, while the
> patient lies in bed. Lectures, concerts, recitations---what one wishes, may
> be had at will. Skilled readers or expert elocution teachers could be
> employed to read into the wires entire libraries
> [@fankhauser_telegraphone_1909, 44].

Anticipating the popularity of twenty-first century audio formats like
podcasts and audio books, Fankhauser spoke of "tired and jaded" workers who
would "sooth [themselves] into a state of restfulness" by listening to their
favorite authors [@fankhauser_telegraphone_1909, 45]. Poulsen saw his
"electric writing" emerge as "clear" and "distinct" as "writing by hand," "an
absolutely legal and conclusive record."[@fankhauser_telegraphone_1909, 41]
Where written language was lossy and reductive, electromagnetic signals, he
hoped, would hold high fidelity to the original.

In 1909, Fankhauser thought of magnetic storage as primarily an *audio*
format, which combined the best of telegraphy and telephony. Magnetic *data*
storage technology did not mature until the 1950s, when advances in composite
plastics made it possible to manufacture tape cheaper and more durable than
its paper or cloth alternatives. The state of the art relay calculator
commissioned by the Bureau of Ordinance of the Navy Department in 1944, and
built by the Harvard Computation laboratory in 1947, still made use of
standard issue telegraph "tape readers and punchers," adapted for computation
with the aid of engineers from Western Union Telegraph Company.[^ln2-punch] It
was equipped with a number of Teletype Model 12A tape readers and Model 10B
perforators, using 11/16-inch wide paper tape, partitioned into "five
intelligence holes," where each quantity entered for computation took up
thirteen lines of code [@staff_description_1949, 30]. Readers and punchers
were capable of running at 600 operations per minute. Four Model 15
Page-Printers were needed to compare printed characters with the digits stored
on the ticker tape print register. The numerical inscription in this setup was
therefore already split between input and output channels: with input stored
on ticker tape and output on printed page.

The Mark III Calculator which followed the Harvard Computation Laboratory's
earlier efforts was also commissioned by the Navy's Bureau of Ordinance. It
was completed in 1950. Its "organization" ("architecture," one would say
today) did away with punch cards and ticker tape, favoring instead an array of
large electromagnetic drums along with reel-to-reel tape recorders. The drums,
limited in their storage capacity, revolved at much faster speeds than tape
reels. They were used for fast, temporary, internal storage. The drums'
surface was coated with a "thin film composed of finely divided magnetic
oxides of iron suspended in a plastic lacquer, and applied to the drums with
an artist's air brush" [@staff_description_1952, 1]. Mark III employed
twenty-five such drums, rotating at 6900 rpm and each capable of storing 240
binary digits.

[^ln3-multi]: Technical literature makes a distinction between space- and
frequency- division multiplexing. On some level, space-division multiplexing
simply involves the splitting of a signal into multiple physical channels
(wires). Frequency-division better "fills" the space of a single channel.

In addition to the fast "internal storage" drums, the "floor plan" included
eight slow "external storage" tape-reader mechanisms. Tape was slower than
drums, but cheaper. It easily extended to multiple reels, thus approaching the
architecture of an ideal Turing machine, which called for "infinite tape." In
practice, the tape was not infinite, but merely long enough to answer the
needs of military computation. Unlike stationary drums, tape was portable.
Operators could prepare tape in advance, in a different room, at the allotted
Instructional Tape Preparation Table. The information on tape would then be
synced with and transferred to a slow drum. In the next stage, the slow drum
accelerated to match the higher rotating speeds of more rapid internal
storage, and transferred again there for computation. Mark III was further
equipped with five printers "for presenting computed results in a form
suitable for publication." The printers were capable of determining the
"number of digits to be printed, the intercolumnar and interlinear spacing,
and other items related to the typography of the printed page"
[@staff_description_1952, 34-5]. One can imagine the pathway of a single
letter or digit as it crosses surfaces, through doorways and interfaces,
gaining new shapes and temporalities with each transition.

Electromagnetic signals were transcoded into binary numerical notation. To
transfer characters onto tape, operators sat at the "numerical tape
preparation table," a separate piece of furniture. Data were stored along two
channels, running along tape's length. Operators entered each number twice,
first into Channel A and then into B. This was done to prevent errors, since
they worked blindly, unable to see whether the intended mark registered
properly upon first entry. An error bell would sound when the first quantity
did not match the second, in which case an operator would reenter the
mismatched digits. To "ensure completely reliable results," one of the five
attached Underwood Electric teletypes could further be used to print all
channels and confirm input visually [@staff_description_1952, 35 & 143-88].

The potential for incongruence between recondite data formats and their
apparent representation posed a problem in the decades preceding screen
simulation. In a 1954 patent, filed on behalf of Burroughs Corporation, Herman
Epstein and Frank Innes described an "electrographic printer," involving an
"electrical method and apparatus for making electrostatic images on a
dielectric surface by electrical means which may be rendered permanently
visible" [@epstein_electrographic_1961, 1]. The electrographic printer
anticipated the modern photocopier in that it proposed to use dusting inks to
reveal the static charge. Rather than encoding its data into another
representation like the Baudot code, it traced human-legible letter shapes
directly onto tape. A small printing head would convert binary input into a
five-by-seven grid of electromagnetic charges to render the English alphabet.
Such magnetic shapes could then be be made apparent by combining them with a
"recording medium" having the "correct physical properties to adhere to the
electrostatic latent images" [@epstein_electrographic_1961, 2]. A light
dusting of powder ink would reveal the latent magnetic inscription. Tape and
paper configurations would thus achieve a measure of literal analogy.

!["Images formed by a negative voltage," from Electrographic Printer by
Epstein and Innes
[@epstein_electrographic_1961]](images/magnetic-alphabet-epstein.png)

Advances in magnetic storage found their way into businesses and home offices
a decade later. In 1964, IBM combined magnetic tape (MT) storage with its
Selectric line of electric typewriters (ST), some of the first machines to
reliably transform a keyboard's mechanical action into binary electric signal.
Selectrics were used for input in a variety of early computing platforms
because of their relative ubiquity [@eisenberg_word_1992]. In coupling
electromagnetic tape storage with keyboard input, the MT/ST became one of the
first personal word processors. Built on a simpler architecture than its
supercomputer cousins, the machine used a single tape read and write
mechanism. An advertisement in the American Bar Association Journal, circa
1968, called it the $10,000 typewriter, "worth every penny." Where typists
previously had to stop and erase every mistake, the IBM MT/ST allowed them to
"backspace, retype, and keep going." The mistake was altered on magnetic tape,
"where all typing is recorded and played back correctly at incredible speed"
[@association_aba_1966, 998].

MT/ST architecture inherited the problem of legibility from its supercomputer
predecessors: information entered into tape was similarly invisible to the
typist. In addition to being encoded, electric alphabets were written in
"magnetic domains" and "polarities," which lay beyond human
sense.[^ln2-magnet] One therefore had to verify input against stored
quantities to ensure correspondence. But the stored quantity could only be
checked by transforming it into yet another inscription. Verification itself
could be prone to error, because storage could not be accessed directly
without specialized instruments.[^ln5-witt]

[^ln5-witt]: Recall Wittgenstein's broken reading machines, which exhibited a
similarly recursive problem of verification. To check whether someone
understood a message one has to resort to another message, and so on.

Mark III engineers attempted to solve this problem by asking operators to
input quantities several times over. Another class of solutions involved
making the magnetic mark more apparent. For example, Youngquist and Hanes
described their 1962 "magnetic reader" as a "device for visual observation of
magnetic symbols recorded on a magnetic recording medium in tape or sheet
form." Magnetic recording tape, they wrote,

> is often criticized because the recorded signals are invisible, and the
> criticism has been strong enough to deny it certain important markets. For
> example, this has been a major factor in hampering sales efforts at
> substituting magnetic recording tape and card equipment for punched tape and
> card equipment which presently is dominant in automatic digital
> data--handling systems. Although magnetic recording devices are faster and
> more troublefree, potential customers have often balked at losing the
> ability to check recorded information visually. It has been suggested that
> the information be printed in ink alongside the magnetic signals, but this
> vitiates major competitive advantages of magnetic recording sheet material,
> e.g., ease in correction, economy in reuse, simplicity of equipment,
> compactness of recorded data, etc." [@youngquist_magnetic_1961, 1].

The magnetic reader consisted of two hinged plates. Youngquist and Hanes
proposed to fill its covers with a transparent liquid that would host
"visible, weakly ferromagnetic crystals." When sandwiched between the plates,
a piece of magnetic tape would activate crystals, which would in turn align
with the embedded inscription. The top plate would ultimately reveal the
signal's "visibl[e] outline."[@youngquist_magnetic_1961, 1]

!["The positioning of a magnetic recording card for visual observation of
symbols recorded thereon," from [@youngquist_magnetic_1961,
1]](images/youngquist.png)

From the lack of acknowledgement in the literature, we can surmise that
magnetic readers did not find widespread use. The problem remained: tape and
paper were physically incompatible. Data plowed into a broad sheet's expanse
had to be transformed along the length of a narrow groove. The next generation
of IBM Magnetic Selectric typewriters added a "composer" control unit that
attempted to bridge the structural differences between media. The IBM Composer
restored some of the formatting lost in transition between paper and plastic.
For example, it added capabilities for margin control and text justification.
The original IBM Composer unit justified margins (its chief innovation over
the typewriter) by asking the operator to type each line twice: "one rough
typing to determine what a line would contain, and a second justified typing"
[@morgan_ibm_1968, 69]. After the "first typing," an indicator mechanism
calculated the variable spacing needed to achieve paragraph justification. The
formatting and content of each line thus required separate input passes to
achieve the desired result in print.

+-------------------+---------------+-------------------------------+
|  Model            | Year          | Technologies                  |
+===================+===============+===============================+
| Selectric         |               | - electric                    |
| Typewriter        | 1961          | - binary code                 |
|                   |               | - replaceable font element    |
+-------------------+---------------+-------------------------------+
| MT/ST             | 1964          | - magnetic tape               |
|                   |               | - Selectric typewriter        |
+-------------------+---------------+-------------------------------+
| Selectric         | 1966          | - justification               |
| Composer          |               | - spacing                     |
|                   |               | - typesetting                 |
+-------------------+---------------+-------------------------------+
| MT/SC             | 1968          | - magnetic tape               |
|                   |               | - Selectric typewriter        |
|                   |               | - Composer                    |
+-------------------+---------------+-------------------------------+
| MC/ST             | 1969          | - magnetic card               |
|                   |               | - Selectric typewriter        |
+-------------------+---------------+-------------------------------+

Table: Generations of IBM Selectric line of word processors.

IBM's next generation Magnetic Tape Selectric Composer (MT/SC) combined a
Selectric keyboard, MT/ST's magnetic tape, and a Composer's formatting
capabilities. Rather than having the operator type each line twice, the MT/SC
system printed the entered text twice: once on the "input station printout"
which showed both the content and control codes in red ink, and the second
time as the final "Composer output" printout, which presented the final
typeset copy. Output operators still manually intervened to load paper, change
font, and include hyphens. The monolithic page unit was thereby further
systematically deconstructed into distinct layers of content and formatting.
Like other devices of this generation, the IBM MT/SC suffered from the problem
of illegible storage. Error checking on paper via multiple printouts was aided
by a control panel consisting of 11 display lights. The configuration of
lights could be used to "peek" at the underlying data structure
[@bishop_development_1968].

In an attempt to achieve greater congruence between visible data and data
archived on the magnetic medium, IBM briefly explored the idea of using
magnetic cards instead of tape. On tape, information had to be stored
serially, as one long column of codes. Relative arrangement of elements could
be preserved, it was thought, on a magnetic card, which dimensionally
resembled paper. The 1968 patent for "Data Reading, Recording, and Positioning
System" describes a method for arranging information on a storage medium
"which accurately positions each character recorded relative to each previous
character recorded" [@clancy_data_1970, 1]. In 1969, IBM released a magnetic
card-based version of its MT/ST line, dubbed the MC/ST. Fredrick May, whose
name often appears on word processing related patents form this period, would
later reflect that a "major reason for the choice of a magnetic card for the
recording of medium was the simple relationship that could be maintained
between a typed page and a recorded card." The card resembled a miniature
page, making it a "unit of record of storage for a typed page."[@may_ibm_1981,
743] The Mag Card was short-lived as well because of its limited storage
capacity and capricious feeding mechanism [@may_ibm_1981]. Though it offered a
measure of topographic analogy between tape and paper, it remained
inscrutable, like all magnetic media.

![IBM Mag Card II, introduced in 1969 for use in the Magnetic Card/Selectric
Typewriter (MC/ST) in 1969. A simple relationship could be "maintained between
a typed page and a recorded card" [@may_ibm_1981, 743]. Image by Pointillist
under GNU Free Documentation License, Version 1.2.](images/ibm-card.png)

The structure of textual artifacts---from a simple leaflet to a novel in
multiple volumes---has remained remarkably stable since the invention of
movable type. One rarely finds a sentence that spans several paragraphs, for
example. Nor would a contemporary reader expect to find pages of different
sizes in the same tome. Long-standing historical conventions guide the
production of printed text. Likewise, semantic and decorative units on a page
exist within a strict hierarchy. No book of serious non-fiction, for example,
would be typeset in cursive font. Unless something out of the ordinary
attracts their attention, readers tend to gloss the "inconsequential" details
of formatting in favor of content. The material contexts of a well designed
book fade from view during reading.

For a few decades after the advent of magnetic storage media, but before the
arrival of screen technology, the sign's outward shape disappeared altogether.
It is difficult to fathom now, but at that time---after the introduction of
magnetic tape in the 1960s but before the wide-spread advent of cathode ray
tube displays in the 1980s---typewriter operators and computer programmers
manipulated text blindly. Attributes like indent size and justification were
decided before ink was committed to paper.

In the 1980s, an engineer would thus reflect on the 1964 MT/ST's novelty:

> It could be emphasized for the first time that the typist could type at
> "rough draft" speed, "backspace and strike over" errors, and not worry about
> the pressure of mistakes made at the end of the page [@may_ibm_1981, 742].

The MT/SC further added a "programmable control" unit to separate inputs from
outputs. Final printing was then accomplished by:

> mounting the original tape and the correction tape, if any, on the
> two-station reader output unit, setting the pitch, leading, impression
> control and dead key space of the Composer unit to the desired values, and
> entering set-up instructions on the console control panel (e.g., one-station
or > two-station tape read, depending on whether a correction tape is present;
> line count instructions for format control and space to be left for
> pictures, etc.; special format instructions; and any required control codes
> known to have been omitted from the input tape). During printing the operator
> changes type elements when necessary, loads paper as required, and makes and
> enters hyphenation decisions if justified copy is being printed
> [@bishop_development_1968, 382].

The tape and control units thus intervened between keyboard and printed page.
The "final printing" combined "prepared copy," "control and reference codes,"
and "printer output" [@bishop_development_1968, 382; @may_ibm_1981]. Historical
documents often mention three distinct human operators for each stage of
production: one entering copy, one specifying control code, and one handling
paper output. These three could hypothetically work in isolation from one
another. The typist would see copy; typesetter, formatting and control codes;
printer, interpolated results.

Researchers working on these early IBMs considered the separation of print
into distinct strata a major contribution to the long history of writing. An
IBM consultant went as far as to place the MT/SC at the culmination of a grand
"evolution of composition," which began with handwriting and continued to wood
engraving, movable type, and letterpress. "The IBM Selectric Composer provides
a new approach to the printing process in this evolution," he wrote. He
concluded by heralding the IBM Composer era, in which people once again write
books "without the assistance of specialists" [@frutiger_ibm_1968, 10].
Marketing language notwithstanding, the separation of the sign from its
immediate material contexts is a major milestone in the history of writing and
textuality.

The move to magnetic and later solid state storage media would have tremendous
social and political consequences for the republic of letters. Magnetic
storage reduced the costs of copying, in a sense freeing the word from its
more permanent material confines, creating the illusion of ephemerality. Yet,
the material properties of magnetic tape itself continued to prevent direct
access. Magnetic storage created a new illiteracy, separating those who could
read and write at the site of storage from those who could only observe
passively, at the shimmering surface of archival projection.

The above schematics embody textual fissure in practice. By tracing signals
through their machines we are able to reconstruct a multiplicity of
inscription sites. These are not metaphoric but literal localities, which
stretch the sign across manifold surfaces. Where pens, typewriters, and hole
punches transfer inscriptions to paper directly, early electromagnetic devices
compounded them obliquely, into a laminated aggregate. The propagation of
electrical signal across space required numerous phase transitions between
media: from one channel of tape to another, from tape to drum, from a slow
drum to a fast one, and from drum and tape to paper. On paper, the inscription
remains visible in circulation; it disappears from view on tape, soon after
key press. Submerged beneath a facade of opaque oxide, inscriptions thicken
and stratify. Where in the first stage of our history we saw machine and human
languages intertwine, we saw the mixture fracture and pass out of sight in the
second. Electromagnetic inscription heralds the era of *laminate textuality*.

## C. Legibility

The contemporary textual condition took its present form in the late 1960s.
Computers subsequently changed in terms of size, speed, and ubiquity. They
retain the same essential architecture today, comprising (i) programmable
media, (ii) electromagnetic storage, and (iii) screens.

Screens finally addressed the problem of electromagnetic legibility. The
electric charge "freed" inscription from its immutable contexts. An abstract
layer was created, where signs could be manipulated "in memory," without
permanence or commitment. That layer however remained opaque and difficult to
manipulate mentally. Screens added a much-needed window onto the abstraction.
Text's temporary home could now be pictured in its entirety, obviating the
need for double entry or frequent print outs. Screens interjected to mediate
between input and output. They flattened stratified complexity to facilitate
use. But with time, the image also subsumed the very nature of mediation.

On December 9, 1968 Douglas Engelbart, then founder and primary investigator
at the NASA- and ARPA-funded Augmentation Research Center lab at the Stanford
Research Institute, gave what later became known as the "mother of all demos"
to an audience of roughly one thousand or so computer professionals attending
the Joint Computer Conference in San Francisco [@rogers_demo_2005;
@tweney_mother_2008]. The flyer advertising the event read as follows:

> This session is entirely devoted to a presentation by Dr. Engelbart on a
> computer-based, interactive, multiconsole display system which is being
> developed at Stanford Research Institute under the sponsorship of ARPA, NASA
> and RADC. The system is being used as an experimental laboratory for
> investigating principles by which interactive computer aids can augment
> intellectual capability. The techniques which are being described will,
> themselves, be used to augment the presentation. The session will use an
> on-line, closed circuit television hook-up to the SRI computing system in
> Menlo Park. Following the presentation remote terminals to the system, in
> operation, may be viewed during the remainder of the conference in a special
> room set aside for that purpose [@engelbart_doug_1968].

The demo announced the arrival of almost every technology prophesied by
Vannevar Bush in his influential 1945 *Atlantic* essay, "As We May Think"
[@bush_as_1945]. During his short lecture, Engelbart presented functional
prototypes of the following: graphical user interfaces, video conferencing,
remote camera monitoring, links and hypertext, version control, text search,
image manipulation, windows-based user interfaces, digital slides, networked
machines, mouse, stylus, and joystick inputs, and "what you see is what you
get" (WYSIWYG) word processing.

In his report to NASA, Engelbart described his lab as a group of scientists
"developing an experimental laboratory around an interactive, multiconsole
computer-display system" and "working to learn the principles by which
interactive computer aids can augment the intellectual capability of the
subjects" [@engelbart_human_1969, 1]. Cathode Ray Tube (CRT) displays were
central to this research mission. In one of many patents that came out of his
"intellect augmentation" laboratory, Engelbart pictured his "display system"
as a workstation that combines a typewriter, CRT screen, and mouse. The
schematics show the workstation in action, with the words "NOW IS THE TIME
FOB" prominently displayed on screen. The user was evidently in the process of
editing a sentence, likely to correct the nonsensical FOB into a
FOR.[^ln5-timefob]

!["NOW IS THE TIME FOB." Schematics for a "display system"
[@engelbart_x-y_1970].](images/engel.png)

Reflecting on the use of "visual display systems" for human-computer
interaction, Engelbart wrote:

> One of the potentially most promising means for delivering and receiving
> information to and from digital computers involves the display of computer
> outputs as visual representations on a cathode ray tube and the alteration
> of the display by human operator in order to deliver instructions to the
> computer [@engelbart_x-y_1970, 1].

The first subjects to read and write on screen reported feeling freedom and
liberation from paper. An anonymous account included in Engelbart's report
offered the following self-assessment:[^ln2-follow]

```
    1B2B1 "To accommodate and preserve a thought or
    piece of information that isn't related to the work
    of the moment, one can very quickly and easily
    insert a note within the structure of a file at such
    a place that it will neither get in the way nor get
    lost.

    1B2B2 "Later, working in another part of the file,
    he can almost instantly (e.g. within two seconds)
    return to the place where he temporarily is storing
    such notes, to modify or add to any of them.

    1B2B3 "As any such miscellaneous thought develops,
    it is easy (and delightful) to reshape the structure
    and content of its discussion material.
```

Writing, which the typist previously perceived as an ordered and continuous
activity, she now performed in a more disjointed way. She delights in shaping
paragraphs that more closely match her mental activity. Screens thus restored
some of the fluidity of writing that typewriters denied. Writers could, for
example, pursue two thoughts at the same time, documenting both at different
parts of the file as one would in a notebook. Not constrained by the rigidity
of a linear mechanism, they could move around the document at will.

Engelbart recorded what must count as some the most evocative passages to
appear in a NASA technical report. Its "Results and Discussion" contains the
following anonymous contemplation:[@engelbart_human_1969, 50]

```
1B4 "I find that I can express myself better, if I can
make all the little changes and experiments with wording
and structure as they occur to me." [Here the user
experiments a little with using structural decomposition
of a complex sentence.]
```

A decomposition follows indeed. The author deviates dramatically from
technical writing conventions. Numbered passages along with unexpected
enjambment heighten the staccato quality of prose that reaches towards the
lyric:[@engelbart_human_1969, 50-1]

```
    1B4A "I find that I write faster and more freely,

        1B4A1 "pouring thoughts and trial words onto the
        screen with much less inhibition,

        1B4A2 "finding it easy to repair mistakes or wrong
        choices

            1B4A2A "so while capturing a thought I don't
            have to inhibit the outpouring of thought and
            action to do it with particular correctness,

        1B4A3 "finding that several trials at the right
        wording can be done very quickly

            1B4A3A "so I can experiment, easily take a look
            and see how a new version strikes me--and often
            the first unworried attempt at a way to express
            something turns out to be satisfactory, or at
            least to require only minor touch up.

        1B4A4 "Finding that where I might otherwise
        hesitate in search of the right word, I now pour out
        a succession of potentially appropriate words,
        leaving them all there while the rest of the
        statement takes shape. Then I select from among
        them, or replace them all, or else merely change the
        list a bit and wait for a later movement of the
        spirit.
```

When input and output coincide in time, as they do on paper, mistakes are
costly. Once inscribed, the sign gains permanence; it is difficult to emend.
An eraser can help remove a layer of physical material. Alternatively, writers
use white ink to restore the writing surface. Engelbart's anonymous subject
reports the feeling of freedom from such physical commitment. She can simply
"backspace" and start over. Words come easily because there are no penalties
for being wrong. Virtual space seems limitless and endlessly pliable.

The feeling of material transcendence---the ephemeral quality of digital
text---is tied directly to the underlying physical properties of screens and
electromagnetic storage. Screens expose the pliability of the medium, where
erasure is clean and effortless. Content can be addressed in memory and copied
at a stroke of a key. The numbered paragraphs suggest a novel system for
recollection. Data storage units become, in a sense, mental units. I am struck
by the distinctly phenomenological quality of technical description: the
editor does not merely resemble a page, it is, for the writer, a newly
discovered way of thought, which changes not only the writer's relation to
text, but to her own thoughts. The highly hierarchical and blocky paragraph
structure, along with its repetitive refrain, "finding" and "I find that,"
gives the prose a hypnotic drive forward. The cadence matches the reported
experience of discovery.

The writer continues:[@engelbart_human_1969, 51]

```
    1B4B "I find that,

        1B4B1 "being much more aware of

            1B4B1A "the relationships among the phrases of a
            sentence,

            1B4B1B "among the statements of a list,

            1B4B1C "and among the various level and members
            of a branch,

        1B4B2 "being able

            1B4B2A "to view them in different ways,

            1B4B2B "to rearrange them easily,

            1B4B2C "to experiment with certain special
            portrayals,

                1B4B2C1 "not available easily in unstructured
                data

                1B4B2C2 "or usable without the CRT display,

        1B4B3 "and being aware that

            1B4B3A "I can (and am seeking to) develop still
            further special conventions and computer aids

            1B4B3B "to make even more of this available and
            easy,

        1B4B4 "all tend to increase

            1B4B4A "my interest and experimentation

            1B4B4B "and my conviction that this is but a
            peek at what is to come soon.
```

The passages appear too contrived to be spontaneous. Despite its experimental
structure, the narrative advances key elements of Engelbart's research
program, which aimed to develop new data structures in combination with new
ways of displaying them. Yet I cannot help but be moved by the fluency of the
prose and by the sheer audacity of the project.

Engelbart's research into intellect augmentation created tools that augment
research. In an image that evokes Baron Münchhausen pulling himself out of a
swamp by his own bootstraps, Engelbart called his lab's methodology
"bootstrapping," which involved the recursive strategy of "developing tools
and techniques" to develop better tools and techniques [@engelbart_research_1968, 396]. The "tangible product"
of such an activity was a "constantly improving augmentation system for use in
developing and studying augmentation systems."[@engelbart_human_1969, 6]

It is an appealing vision, but only as long as it remained recursive. The lab
benefited from creating its own tools and methods. Engelbart also hoped that
his system could be "transferred---as a whole or by pieces of concept,
principle and technique---to help others develop augmentation systems for many
other disciplines and activities."[@engelbart_human_1969, 6] Undoubtedly,
Engelbart's ideas about intellect augmentation have had a broad impact on
knowledge work across disciplines. His vision loses the property of self
determination, however, when transferred outside the narrow confines of a lab
actively engaged in the transformation of material contexts of their own
knowledge production. Word processing today rarely involves a community
pulling itself by the bootstraps. It is augmentation enforced from without,
adhering to values and principles no longer comprehensible to the entity being
augmented.

To bring his system into being, Engelbart convened a community which through
recursive self-improvement could lift itself up towards a smarter, more
efficient, more human way of doing research. The group crafted novel
instruments for reading and writing. They engineered new programming
languages, compilers to interpret them, and debuggers to troubleshoot. The
system shows care and love for the craft of writing. But there is also
complexity. "This complexity has grown more than expected," Engelbart wrote in
conclusion [@engelbart_human_1969, 67]. The feeling of transcendence the
anonymous author describes in using the system engages a sophisticated
mechanism. The mechanism was not, however, what lifted the community. It was
the process of building it that does it. "The development of the Bootstrap
Community," Engelbart wrote, "must be coordinated with the capacity of our
consoles, computer service, and file storage to support Community needs, and
with our ability to integrate and coordinate people and
activities."[@engelbart_human_1969, 67] The development of the community must,
in other words, form a feedback loop with software development. It involves
training, critical self-reflection, and deliberation.

Modern word processors enable us to drag and drop passages with unprecedented
facility. We live in Engelbart's world, to an extent that we use his lab's
complex systems daily and in a smiler configuration: screens, keyboards,
storage. Today's computer users rarely comprise a self-determined
"bootstrapping community" however. The contemporary writer is bootstrapped
passively to whatever prevailing vision of intellect augmentation. The very
metaphor of bootstrapping suggests the impossibility of using one's bootstraps
to pull others out of the Platonic cave. Engelbart's liberatory research
program therefore left another less lofty imprint on the everyday practice of
modern intellectual life. Text, which before the advent of the cathode ray was
readily apparent on the page in all of its fullness, finally entered a complex
system of executable code and inscrutable control instruction. The material
lightness of textual being came at the price of legibility.

Short-lived screenless word processors of the early 1960s (like the MT/ST)
were difficult to operate, because typists had no means to visualize complex
data structures on tape. Screens helped by representing document topography
visually, restoring a sense of apparent space to otherwise opaque media. The
contemporary "digital" document may resemble a page on screen, but beneath it,
it is a jumble of bits, split into the various regions of internal memory.
Screens simulate document unity by presenting holistic images of paragraphs,
pages, and books. The simulation seems to follow the physics of paper and ink:
one can turn pages, write in margins, and make bookmarks. But the underlying
inscription remains in fracture. Simulated text cannot transcend matter.
Screens merely conceal its material properties while recreating others. The
act of continual dissemblage---one medium imitating the other---manufactures
an ephemeral illusion, by which pages fade in and out of sight, paper folds in
improbable ways, words glide effortlessly between registers of copy and paste.

In decoupling inputs from outputs, programmable media injects arbitrary
intervals of time and space into document architecture. Forces of capital and
control occupy the void as the sign acquires new dimensions and capabilities
for automation. Code and codex subsequently sink beneath the matte surface of
a synthetic storage medium. Screens purport to restore a sense of lost
immediacy: of the kind felt upon contact between pen nib and paper, as the
capillary action of cellulose conveys ink into its shallow conduit. Screens
are meant to open a window onto the unfamiliar physicalities of
electromagnetic inscription. They obviated, for example, the need for multiple
typings or print outs. Projected image should, in theory, correspond to its
originating keystroke. The gap separating input and output appears to close.
Crucially, the accord between archived inscription and its image cannot be
guaranteed. The interval persists in practice and is actively contested. Deep
and shallow inscriptions entwine. Laminate text seems weightless and ephemeral
at some layers of the composite, which allow for rapid remediation. At other
layers, its affordances are determined by its physics; at others, they are
carefully constructed to resist movement or interpretation. Alienated from
base particulates of the word, we lose some of our basic capacities to
interrogate embedded power structures.

[^ln2-gate]: Solid state memory technology, flash memory for example, store
information in capacitor "circuit states." This by contrast with
electromagnetic storage that works by modulating electrical charge over
a magnetic surface. Solid state capacitor storage was used in the earliest
computers, but was prohibitively expensive to manufacture until well into the
twenty-first century, when solid state drives began to replace electromagnetic
storage in consumer electronics [@kahng_semipermanent_1967, 1296]. In an early
(1967) paper on "A Floating Gate and Its Application to Memory Devices," Kahng
and Sze explain: "A structure has been proposed and fabricated in which
semipermanent charge storage is possible. A floating gate is placed a small
distance from an electron source. When an appropriately high field is applied
through an outer gate, the floating gate charges up. The charges are stored
even after the removal of the charging field due to much lower back transport
probability [...] Such a device functions as a bistable memory with
nondestructive read-out features. The memory holding time observed was longer
than one hour" [@kahng_floating_1967, 1288]. See also @horton_experimental_1962
and @frohman-bentchkowsky_fully_1971 on "floating gate avalanche injection."

[^ln2-printermen]: According to the U.S. Department of Labor statistics, women
comprised 24 percent of Morse operators in 1915 (before the wide-spread advent
of automated telegraphy). By 1931 women comprised 64 percent of printer and
Morse manual operators [@u.s._bureau_of_labor_statistics_displacement_1932,
514].

[^ln2-bacon]: This volume is also commonly translated as "Of the Dignity and
Advancement of Learning," following the Spedding edition. The first two books
of *The Advancement* appeared first in 1605. Together with books 6-9 published
in Latin in 1623 they are sometimes referred to as "Of the Advancement and
Proficience of Learning or the Partitions of Sciences," following the Oxford
1640 edition. I will follow the 1640 English edition here. Volume 10, in the
new Oxford Collected Works, containing *The Advancement*, was not out at the
time of my writing this book (2015).

[^ln2-brain]: We will later entertain the (real) possibility of
non-representational communication, suggested by early experiments in direct
brain-to-brain or brain-to-machine interfaces.

[^ln2-denning]: These stages correspond roughly to the "three generations of
electronic computing" outlined in Peter Denning's "theory of operating systems"
[@denning_third_1971].

[^ln2-busa]: See for example @hockey_history_2004, "Father Busa has stories of
truckloads of punched cards being transported from one center to another in
Italy" (para. 8).

[^ln2-magnet]: See for example @stefanita_magnetism:_2012, 1-69 and
@ohmori_memory_2015.

[^ln2-loom]: These dates, as is usually the case with periodization, are
somewhat arbitrary. I suggest 1725 as an inaugural date when the French textile
worker Basile Bouchon used "drill paper" to automate industrial drawlooms
[@koetsier_prehistory_2001, 593-595; @randell_history_2003]. The inaugural
honors could also go to the brothers Banū Mūsā, ninth-century automata
inventors from Baghdad; to Jacques De Vaucanson, who delighted the public with
his lifelike mechanisms in the mid eighteenth century
[@riskin_defecating_2003]; or to Joseph Charles Marie Jacquard, who improved
upon and popularized Bouchon's looms on an industrial scale around the same
time.

[^ln2-follow]: I reproduce the text verbatim and preserving the line breaks,
since formatting is an important part of the reported experience. See
@engelbart_human_1969, 48-9.

[^ln5-timefob]: The source for the cryptic phrase is likely
@weller_early_1918, 21 and 30: "We were then in the midst of an exciting
political campaign, and it was then for the first time that the well known
sentence was inaugurated,---'Now is the time for all good men to come to the
aid of the party;' also the opening sentence of the Declaration of
Independence [...] were repeated many times in order to test the speed of the
machine."

[^ln2-punch]: See @staff_description_1949, 1-40. "Two means are available for
preparing the functional tapes required for the operation of the
interpolators. First, when the tabular values of *f(x)* have been previously
published, they may be copied on the keys of the functional tape preparation
unit [...] and the tape produced by the punches associated with this unit,
under manual control. Second, as suitable control tape may be coded directing
the calculator to compute the values of *f(x)* and record them by means of one
of the four output punches, mounted on the right wing of the machine" (33-4).

[^ln2-hayles]: See also @hayles_print_2004.

[^ln1-google]: "Google's mission is to organize the world's information and
make it universally accessible and useful" (@google_about_2015).  See also
Johanna Drucker's discussion on "the totalizing drive of the digital" in
[@drucker_digital_2001, 145].

[^ln1-murray]: The Australian Donald Murray improved on the Baudot system to
minimize the amount of "designed to punch as few holes as possible," allotting
fewer perforations to common English letters (@murray_setting_1905, 567).

[^ln1-zero]: Twenty-eight measures to indicate the numerical "figure space" and
26 to indicate double quotes (which shared the encoding length with the letter
"z").

\newpage
