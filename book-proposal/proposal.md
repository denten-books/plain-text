---
title: "Plain Text: The Poetics of Human-computer Interaction (Book Proposal)"
author: Dennis Tenen

---
# Theme and Argument

Plain text is a file format and a frame of mind. A fundamental concept in the
development of computing, plain text stands in opposition to closed platforms,
rarefied knowledge, and black-box devices. Instead, it offers a vision of data
that is human-readable by design: portable, concise, and universal. This book
contains an argument for plain text. In doing so, it seeks to convene a
community interested in reflecting critically on the ideas, tools, and
practices that shape our daily encounter with computation. My argument starts
with foundational principles at the intersection of media theory and
information science. I ask: what is at stake in the difference between digital
and analog media?  What contains more information a novel or a block of wood?
What separates meaning, form, and formatting? How do pixels form into texts?
Where does data end and meta-data begin? To what extent media determine the
message? The formulation of these broadly theoretical concerns about the
poetics of human-computer interaction opens the way to a discussion about the
social impact of textual technology, as it relates to applied dynamics of
online agency, deliberation, consensus-making, and dissent.

A secondary aim of this volume is to convince computer "users" to view their
computational environments as a literary system of sorts. I mean a "literary
system" in opposition to what one might conventionally mistake for a "binary"
or "digital" one, however imprecise those terms are in everyday use. In
clarifying usage, I will ask mere "users" to become close readers, thinkers,
and makers of technology, able to apply the same critical acuity to reading
code and platform as they do to close reading of prose and poetry. Ultimately,
the book makes a case for the recovery of textual roots latent in the
mechanisms of modern computing.

# Field Significance

The book aims to redress a weakness in the field of Digital Humanities,
particularly as it relates to literary studies. Much scholarship in this space
is often criticized for being ahistorical or atheoretical, abandoning the deep
traditions of literary theory and criticism, even where such traditions would
help bolster the case for emerging scholarship. The nominally related field of
New Media Studies has the opposite problem. Although theoretically
sophisticated, it sometimes produces research far removed from the actual
practice of creating new media (the archetypal example given by Katherine
Hayles is one of a contemporary photography critic not familiar with the use of
the "layers" tool in Photoshop). By contrast, I situate *Plain Text* at the
intersection of theory and practice: somewhere between "technological
foundations for new media studies" and "philosophical bases for computing in
the humanities."

# Existing Literature

*Plain Text* makes a theoretical intervention in the cluster of media studies-
and digital humanities-related fields that include Science and Technology
Studies, Platform Studies, Critical Code Studies, and Media Archeology.
Recent comparable books in these fields include *Paper Knowledge* by Lisa
Gitelman published by Duke UP in 2014, *Coding Freedom: The Ethics and
Aesthetics of Hacking*, by Gabriella Coleman from Princeton UP in 2012,
*Mechanisms: New Media and the Forensic Imagination* by Matthew G. Kirschenbaum
from MIT Press in 2012, *Files: Law and Media Technology* by Cornelia Vismann
from Stanford UP in 2008 and several titles in the *Electronic Mediations*
series at Minnesota UP, which published Lori Emerson's *Reading Writing
Interfaces* in 2014.

My work differs from these volumes in that it is both more philosophical
(rather than strictly "theoretical") and more grounded in the fields of
software an electrical engineering. To give you an example of what I mean,
consider my first chapter, "Phenomenology of a Photocopier." It begins with the
discussion of the Hegelian distinction between "form" and "content," ending
with a case study from the history of word processing, in a technical
explanation of the way "content" and "form" are encoded into modern HTML and
Markdown documents. Similarly, my third chapter, "We Have Always Been Digital,"
begins with a discussion of "digital representation" as philosophers Nelson
Goodman and John Haugeland define it in the analytic tradition. I proceed by
testing their intuitions on the basis of something called the "Soap Opera
Effect" particular to modern Liquid Crystal Displays (LCDs) and in the related
"motion interpolation" technology." The resulting analysis clarifies the
various (and often conflicting) meanings of the word "digital" in Media Studies
and in the Digital Humanities.

Complimentary to both of these fields, I prefer to describe my work as
"computational culture studies," understood in two ways: first as the
study of "computational culture," and, second, as computational approaches to
"culture studies". *Plain Text* falls firmly into the first category, with some
elements of the book edging on light computational methods (particularly if you
consider the proposed digital companion, discussed shortly).

Although I do not mean to engage in the debate on disciplinary formation, it is
important for me to insist on the reciprocal motion between the constituent
elements of "computation" and "culture." Too often rhetoric around the digital
humanities resembles a one-way street, in which computational methods are
promised to reform the humanities unilaterally. Books like Alexander Galloway's
*Laruelle: Against the Digital* (UMinn Press, 2014), Matthew Fuller's *Evil
Media* (MIT, 2012) and Johanna Drucker's *What Is?* (Cuneiform Press, 2013)
represent the beginning of a critical counter-movement. But this response too
must be balanced against emendatory potential of the Digital Humanities
program. As was the case with the "linguistic turn" in the decades prior,
almost all fields of human knowledge are now experiencing a turn towards
computational methods, which offer new insight at previously unavailable scales
of analysis. (Witness the emerging fields of computational biology,
computational chemistry, computational linguistics, computational geometry,
computational archeology, computational architecture design, computational
philosophy, computational social science, and the list goes on.) In *Plain
Text* I stake out a middle ground between Stephen Ramsey's laudatory *Reading
Machines* (University of Illinois Press, 2011) and David Golumbia's disparaging
*The Cultural Logic of Computation* (Harvard UP, 2009), making the case for
transformative use of technology in the humanities, in a reciprocal way that
assures mutual enrichment.

# Audience and Market

As is the case for most of my work, *Plain Text* appeals to two key audiences.
The first comprises digital, media, and literary scholars interested in the
material aspects of knowledge production. The second is composed of "knowledge
workers" that do not usually view their everyday practice in its historical,
philosophical, or political contexts. Software developers, graphic designers,
system administrators, and project managers routinely affect technologies that
have deep cultural significance: from the ways in which we relate to our family
and friends to the formation of shared cultural archives. For this reason,
technical decisions like choosing a text editor, a filing system, or a social
networking platform cannot be adequately addressed in shallow instrumental
terms limited to efficacy, speed, or performance.

A former software engineer and now a literary scholar, my research bridges the
(perceived) gap between the "two cultures" of science and the humanities. My
courses at Columbia University, which include *Code & Poetry* (Fall 2014),
*Computing in Context* (Spring 2015), and *Foundations of Computing for
Journalists* (Summer 2014), attract a diverse body of students from multiple
disciplines (and particularly from departments of English, History, and
Computer Science). I lecture widely in language departments, in schools of
engineering, in front of publishers, architects, artists, and librarians. As a
founder of Columbia's Group for Experimental Methods in the Humanities, I
believe in critical engagement with technology and in exposing my students to
real-world problems. My group has ran workshops on online security for
activists, we are teaching digital literacy at Rikers Island, and we have
reached out to an online community of engineers to help us write media history
as a project in "citizen humanities."[^ln-xp] In the fall of 2014, the group's
activity became the basis for Columbia's fund raising efforts around digital
humanities, in the form of a proposal for the Center of Culture and
Computation. The proposal was met with wide approval from the university deans,
the provost, and the president. In January 2015, the group was encouraged to
apply for the presidential "global innovation" fund to organize workshops on
"digital justice" in Beijing, Mumbai, and in Amman, Jordan.

I am inspired in these endeavors by my colleagues at Columbia English and at
Harvard's Berkman Center for Internet and Society, where I am an active faculty
associate. I keep these manifold audiences in mind as I am finishing *Plain
Text.* The book exposes the intellectual foundations that bolster my research
teaching activities. I write to strengthen these projects and to give back to
the community that has supported me so generously. I hope to rely on the same
good will and on the same support networks in reaching out to promote my book.

In the course of my teaching career, I have been asked to create courses and
certificate programs for graduate students in the humanities interested in
computational studies, including courses at the Digital Humanities Summer
Institute at the University of Victoria and in the Lede program at Columbia's
Journalism School. Texts usually assigned in these environments are either
volumes published by technical presses for a professional audience or
theory-based readings in New Media Studies that are related to the task of
teaching the fundamentals of Computer Science in context only loosely. My book
covers the philosophical foundations of computing fundamentals, explaining not
just the how of Digital Humanities but the why. With the proposed technical
appendix and a possible companion site (explained in the next section), I hope
to supplement the main body of the work with a series of "experiments" that
illustrate theoretical concepts in practice, at the keyboard. Such blend of
theory and practice defines my method. As many major universities invest in
programs related to Digital Humanities, Computational Social Science, and
Computational Journalism,[^ln-dh] I hope for *Plain Text* to become a standard
text that introduces faculty and advanced graduate students to the notion of
critical practice in humanities computing.

[^ln-dh]: For example: In February 2015, U. Penn received $7,000,000 from Penn
Arts and Sciences Overseer to establish the Price Lab for the Digital
Humanities. In December 2014, Yale announced an award in the amount of
$3,000,000 from The Goizueta Foundation to inaugurate the Digital Humanities
Laboratory. UC Berkeley announced $2,000,000 from the Andrew W. Mellon
Foundation to advance digital humanities. Stanford has recently launched the
Center for Spatial and Textual Analysis. University of Michigan opened six new
tenure-track searches in a hiring cluster under the rubric of Public Humanities
in a Digital World in 2012. In May of 2014, Bard College was Awarded $800,000
from Andrew W. Mellon Foundation to support Experimental Humanities.

[^ln-xp]: A detailed account of these an other of projects can be found at
[xpmethod.plaintext.in/strains.html](http://xpmethod.plaintext.in/strains.html).

# Length and Format

I am writing the book as a traditional volume, expounding a sustained thesis
across nine chapters. I tend to write concisely--a style that complements the
subject matter. At this point, I am aiming for a manuscript of around 60-80k
words, which would allot around 5-7k words per chapter.

Although, ostensibly, a work on the history and philosophy of computational
culture, the book argues for the advancement of theory through practice. In
writing it, I continually test my intuitions against the reality of
contemporary computing devices: laptops, servers, and mobile phones. It is one
thing, for example, to maintain that "the media is the message," and quite
another to ask how different modalities like sound, image, and video are
encoded on level of the operating system (Chapter 4). Similarly, the difference
between binary and plain-text formats can be best explored by peeking "under
the hood" of an Adobe Acrobat file (as I do in Chapter 3).

Although not required for the understanding of the book, I would like to
include a supplementary appendix that expands on the theoretical insight from
each chapter with "experiments" that can be done in the command line (a textual
human-computer interface built into most operating systems). These exercises
could exist on paper (as an Appendix) or as a complimentary website. Training
in computational methods often begins with packaged tools that obscure the
underlying complexity of the method. For example, while writing a relatively
complex piece of code a journalist in my class once confessed to being confused
about the relationship between files and folders. *Plain Text* is a book
*about* files and folders: it is about textuality as encoded in specific ways
on machines that have a shared engineering pedigree. The hidden (but very much
intended) side-effect of *Plain Text* is a measure of technical proficiency
with Unix-based operating systems. Higher-level notions that address the
political reality of computation, be they "open access," "freedom of
information," or "online privacy", begin at that system level. The book's
Technical Appendix would help build a practical foundation to the arguments
advanced within.

The book assumes no prior knowledge. It can be read sequentially as a
conventional piece of scholarship in textual theory or new media studies. But
for those willing to take the plunge, I will often illustrate abstract
theoretical concepts by asking the reader to type some commands into their
terminals. Detailed instructions on how to set up this *augmented reading
environment*: related experiments, excercises, and explainations will be found
in the Technical Appendix.

In addition to the Appendix, the book contains several (8-12) tables and
illustrations. I intend to apply for the "first book" subsidy offered by
Harvard's Department of Comparative Literature to offset any costs associated
with the preparation of the manuscript.

# Annotated Table of Contents

## Part I: Text

The first three chapters constitute a part of the book concerned with
textuality itself, building the vocabulary necessary for a discussion of
textual technology in Part II and the resulting social structures in Part III
of the book.

### Chapter 1: Phenomenology of a Photocopier

The chapter deals with the confused history of the distinction between form and
content. I find that going back to Plato and Hegel, "form" is at times used to
indicate physical structure, and, at other times, to indicate immaterial
categories in the ideal realm. A critical treatment of a more contemporary
conversation on "surface" and "depth" of meaning reveals form as a mediating
concept between thought and matter. A case study in extreme surface reading, in
the bowls of a photocopier, opens the way to the distinction between print
(where matter, form, and content are literally fused) and screen (where the
three layers come apart, providing only the illusion of flattened textuality).

### Chapter 2: Literature Down to the Pixel

The second chapter, "Literature Down to the Pixel," begins the work of moving
from "high level" concepts like "text" and "literature" down to their atomic,
constituent elements like fonts and the modulation of electric current. I argue
here that the concern with value in literary criticism detracts from the
explicit movement of control and power intimately connected to digital
textuality. Unlike scholars in the foucauldian tradition (who often trace the
machinations of power through discourse), I concentrate my analysis on
mechanisms of control at the material roots of literary practice. In doing a
media history through primary sources on early development of modern computing,
I show the explicit admixture of content and code: one meant to communicate
messages to humans and the other to program universal machines. This history is
not entirely critical: rather, it reveals a genealogy of computing alternative
to the widely held notion of computer as a device for reductive "mathesis" (in
the words of Johanna Drucker). I argue that the Turing machine is anticipated
not just by the Babbage calculator, but in a series of advances in
communications, word processing, and media storage. A notion of text (as
opposed to number) is "baked into" the system.

### Chapter 3: We Have Always Been Digital

The "microanalysis" in the second chapter, gives way to the opposite movement:
from pixel, to text, and up to meaning. The third chapter, "We Have Always Been
Digital," begins with popular intuitions about the "look and feel" of digital
aesthetics. Discourse around the digital humanities employs the term most
central to its stated research program inconsistently and often without
theoretical reflection. A case study of television motion blur (and the related
"soap opera effect") undermines the initial ease with which notions of the
digital are overdetermined to stand in for a range of often conflicting
modalities. The next section deals with the analytic tradition of dissecting
media into analog and digital categories (via Nelson Goodman in particular). My
summary of that tradition shows that language and text are already "born
digital," that is, discrete and differentiated throughout (by the analytical
definition). Furthermore, digitality depends on "reliable processes of copying
and preservation." From that insight I take it that "being digital" is not an
ontological condition, but rather imposed structure that manifests in specific
material affordances. "Is it copyable?" becomes "can I copy it?" Digital being
ends up being an instrumental condition imposed onto the "user" of media (and
not property of the media itself). The chapter ends with a history of encoding,
where the distinction between binary and plain text formats supersedes the
dichotomy between digital and analog.

## Part II: Technology

Part II of the book explores the textual technologies that define literary
practice.

### Chapter 4: The Media Is Not the Message

This chapter argues against the common-place truism that equates medium with
message. Sources of that confusion and what's at stake. Encoding of the media
on magnetic storage. A revisiting of modalities written and oral.

### Chapter 5: Freedom of Information

Containing an argument against the "systems" definition of information advanced
by Shannon and Weaver. In what Shannon calls a "strange feature" of this
communication theory, information is defined as amount of "freedom" or entropy
in the system. By contrast, I want to insist on the agency (freedom) of the
sender and the receiver. Incidentally, we get some clarity on the differing
ways in which information is invoked in different discourses.

### Chapter 6: Bad Links

Why links are bad. The long history of intertextuality. The excitement of the
90s about it. The intertextual art of Gwern (a blogger that writes about
cryptography). Erudition and analogical thinking. The difference between
hard-coded and symbolic links. Snapshots, the Internet Archive, and the
future of Wikipedia.

## Part III: People

Part III broaches the social impact of computational culture.

### Chapter 7: Recursive Encounters with Oneself

Understanding the document as a vector. The problem of drafts and versions.
What is being transmitted through the vector? The appointment with one self:
Beckett and Sartre. Pipes and I/O serialization.

### Chapter 8: Shared Knowledge

What does it mean to "know" something? Extended cognition. Pen, typewriter, and
word processor (with detours to Kittler and Heidegger). Writing together.
Models of co-authorship (and why we should pay attention).  The massively
multi-authored online novel (*Wu Ming and Lo zar non è morto*).

### Chapter 9: Secrecy, Surveillance

Encrypted literature. Surveillance and counter-surveillance. Notions of
textuality as embroiled in contemporary ideas of privacy, secrecy, and
transparency.  Computation does not necessarily work for the
military-industrial apparatus (as argued by Golumbia, Lennon, and McPherson).
Recovering and preserving textuality in computing. Engineering for dissent.

## Tech Appendix

A series of "experiments" that illustrate key theoretical concepts form each
chapter in the command line, to create an "augmented reading environment."

# Relationship to Dissertation

The book bears a resemblance to the dissertation in the subtitle only. Several
paragraphs from the embargoed dissertation did make it into *Plain Text* in an
ad-hoc manner, but the book as a whole represents a completely new
framework and a new direction in my thinking about the subject. The bulk of the
re-written dissertation made it into another project, called *Algorithmic
Imagination*, to be completed in the winter of 2015/16.

# Schedule to Completion

As of today, the manuscript stands at 40k words. During the academic year, I
average around 250 words per day, a number that more than quadruples when freed
from teaching obligations.[^ln-progress] Assuming a manuscript of around 80k
words, a conservative estimate of my schedule would place the final draft
somewhere towards the middle of summer, 2015.

[^ln-progress]: I wrote a word-counter that beacons my progress online at
[xpmethod.plaintext.in/minimal-computing/plaintext.html](http://xpmethod.plaintext.in/minimal-computing/plaintext.html).

