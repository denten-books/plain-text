Laminate Text: Material Contexts of Digital Knowledge Production
Dennis Yi Tenen

Digital text,  at the basis of all computer-mediated epistemological activity,
appears to view at once an ephemeral and enduring phenomenon. It is, as Wendy
Hui Kyong Chun wrote, an "enduringly ephemeral" inscription that "create[es]
unforeseen degenerative links between humans and machines."[^1] At the site of
its projection, on "soft" screens, the text shimmers and wanes, suspended in
liquid crystal, in response to the modulation of an electric signal. At the
site of its storage, on "hard" drives, among "floating gates" and
ferromagnetic polarities, the text adheres to recondite surfaces persistently.
There, it multiplies and obstinately spreads, like silicon dust, filling the
available crevices with stored data, far beyond the immediate contexts of
inscription.

This inherent duplicity of electromagnetic inscription, as I argue here,
results in conflicting critical accounts in the scholarly literature.
Furthermore, it engenders a fundamental alienation from the material contexts
of digital knowledge production. The alienation ultimately threatens critical
disempowerment. In this essay, I outline a condition I name *laminate
textuality*, by which an inscription fractures to occupy multiple surfaces
concurrently. A historical archaeology of electromagnetic writing anchors the
more theoretical portion of the essay within the emerging affordances of
computational media, which take place, in part, at quantum scale, beyond the
reach of humans senses. Clandestine forces of capital and control subsequently
contest such newly found microscopic expanses, thus limiting the scope of
possible interpretive activity.

## Literary Composites

Little separates ink from paper in print. Ink permeates paper; the conduit is
firmly embedded into the medium. The amalgam of in and paper implies specific
physical properties and their related affordances: what the medium is relates
to what can be done with it.[^2] Print is stable on the scale of decades and
sometimes centuries. At the very least, once can be assured that a text will
remain the same as it passes from one pair of hands into another. As
interpreters of texts, we usually can (with some effort) make certain to find
ourselves, literally, on the same page and to insure that we are discussing
roughly the same piece of writing.

Traditional models of modernist hermeneutics often assume the relative
permanence of the medium.[^3] For example, Paul Ricoeur wrote about "the
possibility of transferring orders of long distances without serious
distortions" as related to the "birth of political rule exercised by a distant
state."[^4] Among other effects, Ricoeur attributed to the fixity of print
"the birth of market relationships" and economics, "the birth of justice" in
the fixity of the legal codex, "the constitution of archives" and therefore
the very possibility of history. He concluded to write that "such an immense
range of effects suggests that human discourse is not merely preserved from
destruction by being fixed in writing, but that it is deeply affected in its
communicative function."[^5]

The architecture of contemporary computational media---screens, hard drives,
and keyboards---cannot sustain the assumption of fixity. Electromagnetic
inscription occupies several available surfaces at once. It exists at once on
screen, itself a composite of glass panes and liquid crystal, and "in memory,"
as manifested in the arrangement of silicon and circuitry. The word is in the
wires. When reading digitally, on a personal computer for example, the
distance between these localities may extend a few inches, enough to cover the
space between a screen and a hard drive. In other contexts, when viewing text
on a commercial "electronic reader," that space may span continents as stored
data are transmitted over vast distances, between content "owners" and its
"users," who hold only a temporary right to peruse, limited in scope to
specific timelines and geographies. An electronic book borrowed from the New
York Public Library, for example, may be stored somewhere in New York, on
library servers. It disappears from my devices when the terms of my book
borrowing expire.

In this way, the digital sign is continually stretched, elongated, and
fractured across diverse material strata, subject to distinct local
affordances. A simple act of erasure on screen, as when one backspaces over a
word written in error, simulates a comparable action in print. However on
disk, metaphoric "erasure" takes on a different connotation, entailing an
entirely new set of operations, not congruent with the implied action on
screen or in print. The incongruence is often benign, particularly when
readers and writers are interested in handling surface representation,
operating on the level of words and ideas. However, in certain contexts, our
inability to perceive the mechanics of inscription at depth severely
undermines critical acuity. To take a blunt example, imagine a scholar or a
journalist who needs to redact unpublished materials in order to protect her
sources. Surface erasure in this case is insufficient to guarantee true
anonymity. Words erased on screen may persevere through deeper structures on
disk and among remote servers, in a way susceptible to malicious breach or
state-sponsored surveillance.

The affordances of laminate text depend not only on the medium, but also on
the contexts of its reception. The same "source text" may be transformed
according to its geography or the identity of its reader. The composition of
the sign changes as a text changes hands. A digital document may respond to a
reader's location, gender, age, or ethnicity. Think of an online newspaper, to
take a common example, where the very composition of headlines and stories are
often tailed to the reader. Such dynamic inscriptions contain the rules of
their own transformation. Content and control code intertwine to produce an
amalgamated artifact, which adapts to its context. The often covert presence
of control structures extend the reach of governance over the process of
interpretation. A government may censor speech through legislation, for
example, and prosecute those who express opinions contrary to the reigning
ideology. Similarly, mechanisms of censorship may be encoded into the fabric
of the laminate itself, through technologies that simply prevent prohibited
ideological formations from appearing on screen. Such technologies can also
limit the transmission and the reception of illicit material. Unlike those
censored explicitly, readers under the reach of algorithmic governance may not
be immediately aware of control mechanisms structuring their everyday
interpretive experience.

The stratified nature of digital inscription poses obvious challenges in the
political sphere. Our ability to mobilize against censorship or surveillance,
for example, is in peril when such mechanisms operate at a microscopic scale,
requiring specialized training and tools for interpretation. It also presents
a c challenge to the practice of literary hermeneutics more generally. How
does critical interpretive practice persist in conditions where readers can no
longer rely on the continuing stability of the medium? The theoretical problem
is not one where can choose to discuss texts as entities of pure immanence or
pure transcendence, enduring or ephemeral. We are confronted instead with
textuality that does not converge on a single location and with texts whose
multivalence is derived from their structural diffusion. The emerging
materialities of laminate text force us to reconsider long-standing critical
assumptions about the physics of inscription.

## Stratigraphy

Having summarized the basic theoretical concerns related to the emerging
material conditions laminate text, I would like to turn my attention now to
the historical archaeology of the medium itself. I borrow the term
"stratigraphy" from the field of archaeology proper, as opposed to media
archaeology where the term is used in its more evocative and metaphoric
sense.[^6] The art of analyzing sedimentary rock layers dates back at least to
the geological observations of the seventeenth century Danish scientist
Nicolas Steno, later extended by John Strachey in his *Observations on the
Different Strata of Earths and Minerals* (1727), and to the geological
cross-sections of William Smith and William Maclure, who drew beautifully
detailed cross-sections of the British and American landscapes [Figure 1].

![Stratigraphic map legend. "Sketch of the Succession of Strata and their
relative Altitudes." William Smith, "DELINEATION of the STRATA of ENGLAND and
WALES with part of SCOTLAND; exhibiting the COLLIERIES and MINES; the MARSHES
and FEN LANDS ORIGINALLY OVERFLOWED BY THE SEA; and the VARIETIES of Soil
according to the Variations in the Sub Strata; ILLUSTRATED by the MOST
DESCRIPTIVE NAMES," Section 16 (1815). Image in the public domain. Reproduced
from the collection of Sedgwick Museum of Earth Sciences, University of
Cambridge.](/image/smith-16.jpg)

The concept of stratigraphy is an apt borrowing when applied to the history of
computer technology, where the various layers of historical development are
often extant in one and the same device. In this manner, the legacy of machine
alphabets such as the Morse and Baudot alphabets is, in some real way, present
on modern devices through the ASCII and UTF-16 conventions. A
"conversational," text-based, model of human computer-interaction developed in
the 1960s, coexists with the later, graphic-based "direct-interaction" modes
of interaction in the innards of every modern mobile phone, game console, or
tablet.[^8] The same can be said about "low-level" assembly languages and
their alternatives like FORTRAN, LISP, and COBOL, continuously in use since
the 1950s alongside their modern descendants.

For the purposes of our conversation, which concerns the paradoxically
conflicting nature of electromagnetic inscription, at once enduring and
ephemeral, we may separate the formation of electromagnetic composites into
three historical periods, each leaving behind a distinctive media sediment.

First, with the advance of telecommunications, we observe the increasing
admixture of human-readable text and machine-readable code. Removable storage
media such as ticker tape and punch cards embodied a machine instruction set
meant to effect a mechanism, which in turn produced human-legible inscription.
Unintelligible (to humans without special training) control codes that
actuated machinery were thereby mixed with plain text, the content of
communication. Inscription split between the sites of storage, where an
expanded machine instruction set was archived, and projection, where the human
legible portion of the inscription was displayed.

Second, whereas ticker tape and punch cards were legible to the naked eye,
magnetic tape, a medium which supplanted paper, made for an inscrutable
substance, inaccessible directly without instrumentation. In the 1950s and
1960s machine operators worked blindly, using complicated workarounds to
verify equivalence between data input, storage, and output. Writing began to
involve multiple typings or printings. Specialized magnetic reading devices
were developed to make inscription more apparent and to establish a
correspondence between input, storage content, and output of entered text. The
physical properties of electromagnetic inscription also allowed for rapid
re-mediation. Tape was more forgiving than paper: it could be written and
re-written at high speeds and volume. However, the opacity of the medium has
also placed it, in practice, beyond human sense.

Finally, the appearance of cathode-ray tube (CRT) displays in the late 1960s
restored a measure of legibility lost to magnetic storage media. The sign
reemerged on-screen. Crucially, it framed a simulacrum of archived
inscription. Typing a word on a keyboard produced one sort of a structure on
tape or disk and another on-screen. The two related contingently, without
necessary equivalence. The lay reader lacked the means to ensure a
correspondence between visible trace and stored mark. An opaque black box of
automated word processing (rules for transmediation) began to intercede
between simultaneous acts of writing and reading.

I have selected a number of textual machines to illustrate this condensed
history. Three mechanisms mark the journey: the Controller patented by Hyman
Goldberg in 1911; the Magnetic Reader introduced by Robert Youngquist and
Robert Hanes in 1958; and, for a lack of a better name, the Time Fob,
introduced by Douglas Engelbart in 1968. Goldberg’s device was designed to
bridge the rift between human and machine alphabets in that it was a
mechanical punch card that could move minds and levers alike. His Controller
traced an alphabet understood by both humans and machines. Youngquist and
Hanes attempted to give human operators a glimpse into the hidden world of
magnetic polarities and electric charges. Engelbart’s Time Fob finally belongs
to what Peter Denning, a prominent computer scientist, has called the "third
generation" of computer systems. This third, and still current stage,
constitutes an assemblage of storage, input, and output technologies.[^8]
Together, these devices tell a story of a fissure at the heart of our
contemporary textual predicament.

## 1. Control Media

The turn of the twentieth century was a pivotal period in the history of
letters. It saw the languages of people and machines enter the same mixed
communications stream. Artificial fixed-length alphabets, such as the Baudot
code, paved the way for the automation of language. The great variety of human
scripts was reduced to a set of discrete and reproducible characters. So
regularized, type was converted into electric signal, sent over great
distances, and used to program machines remotely. These expanded textual
affordances came at a price of legibility. Initially, a cadre of trained
machine operators were required to translate human language into
machine-transmittable code.  Eventually, specialized equipment automated this
process, removing the human from the equation.

The advent of programmable media (punch cards and ticker tape) coupled
human-compatible alphabets with machine control code.[^9] Reduced to a
discrete and reliably reproducible set of characters, natural languages could
be conveyed as electric signals. In such a transitive state language became
more mobile than ever before. It was transmitted efficiently across vast
distances. The mechanization of type also introduced new control characters
into circulation capable of affecting machine state changes at a distance.
Initially, such state changes were simple: “begin transmission,” “sound error
bell,” or “start new line.” With time, they developed into what we now know as
programming languages. Content meant for people was being routinely intermixed
with code meant to control machine devices. Such early remote capabilities
were quickly adapted to control everything from radio stations to advertising
billboards and knitting machines.[^10]

Hundreds of alphabet systems vying to displace Morse code were devised to
speed up automated communications. These evolved from variable-length
alphabets such as Morse and Hughes codes, to fixed-length alphabets such as
Baudot and Murray codes. The systematicity of the signal--always the same
length, always at the same time--shifted text encoding further away from
natural human languages, which rely on affect and variation, and toward
artificial languages, which prioritize other attributes, such as consistency
and reproducibility.

The fixed-length property of Bacon’s cipher, later implemented in the 5-bit
Baudot code, signaled the beginning of the modern era of serial
communications.[27] The Baudot and Murray alphabets were designed with
automation in mind.[28] Both did away with the “end of character” signal that
separated letters in Morse code. Signal units were to be divided into letters
by count, with every five codes representing a single character. Temporal
synchronization was therefore unnecessary, given the receiver’s ability to
read the message from the beginning. A character was simply a unit of space
divisible by 5.

In addition, the Murray code was more compact than Morse code and especially
more economical than Hughes code, which, in the extreme, used up to fifty-four
measures of silence to send a signal representing double quotes.[29] The
signal for “zero” in Morse code occupied twenty-two measures. By contrast, all
Baudot and Murray characters were a mere five units in length, with the
maximum of ten used to switch the receiving device into “figure” or “capital
letter” states, for the total of ten units (Fig 4.2).[30]

Fixed-length signal alphabets drove the wedge further between human and
machine communication. Significantly, the automated printing telegraph
decoupled information encoding from its transmission. Fixed-length encoding of
messages could be done in advance, with more facility and in volume. Prepared
messages could then be fed into a machine without human assistance. In 1905
Donald Murray wrote that the “object of machine telegraphy [is] not only to
increase the saving of telegraph wire<3.>but also to reduce the labour cost of
translation and writing by the use of suitable machines.”[31] Baudot’s and
Murray’s codes were not only shorter but also simpler and less error-prone and
thus resulted in less complicated and more durable devices.

With the introduction of mechanized reading and writing techniques, telegraphy
diverged from telephony to become a means for truly asynchronous
communication. It displaced signal transmission in time as it did in space.
The essence of algorithmic control, amplified by remote communication devices,
lies in its ability to delay execution; a cooking recipe, for example, allows
novice cooks to follow instructions without the presence of a master chef.
Similarly, delayed communication could happen in absentia, according to
predetermined rules and instructions. A message could activate a machine that
prints a log and another that trades stocks and another that replies with a
confirmation.

The new generation of printing telegraphs was programmed in this way using
removable storage media, similar to the way player pianos were programmed to
play music by means of music rolls. Just as perforated music rolls decoupled
music making from its live performance, programmable media decoupled
inscription from transmission. In both cases one could strike keys now, only
to feel the effects of their impact later. Programmable media conserved the
logic of desired effects for later execution. Decoupled from its human
sources, that logic could then be compressed and optimized for speed and
efficiency. Prepared ticker tape and punch cards were fed into the mechanism
for transmission at rates far exceeding the possibilities of hand-operated
Morse telegraphy.

Besides encoding language, the Baudot schema left space for several special
control characters. The character space of the language could was therefore
expanded further by switching the receiving mechanism into a special control
mode in which every combination of five bits represented an individual control
character, instead of a letter. In this manner, human content and machine
control became intertwined and began to occupy the same spectrum of
communication.

By the 1930s devices variously known as printer telegraphs, teletypewriters,
and teletypes displaced Morse code telegraphy as the dominant mode of
commercial communication. A 1932 U.S. Bureau of Labor Statistics report
estimated a more than 50 percent drop in Morse code operators between 1915 and
1931. Morse operators referred to teletypists on the sending side as
“punchers” and those on the receiving side as “printer men.”[32] The printer
men responsible for assembling pages from ticker tape were called “pasters”
and sometimes, derisively, as “paperhangers.”[33] Teletype technology
automated this entire process, rendering punchers, pasters, and paperhangers
obsolete. Operators could enter printed characters directly into the machine,
using a keyboard similar to the typewriter, which, by that time, was widely
available for business use. The teletype would then automatically transcode
the input into transmitted signal and then back from the signal onto paper on
the receiving end.

Machine code thus occupied a gray area between plain text and cipher.
Theoretically, it was considered intelligible only when its compilation
sources were available to the transmitter. Without its sources it became
equivalent to secret communication. In practice, the proliferation of
encodings and machine instructions had the effect of selective illiteracy.
Telegraph operators carried with them multiple cheat sheets, small cards that
reminded them of each system’s particularities. Telegraphy reintroduced the
pre-Lutheran problem of legibility into human letters.

To speak in telegraph was to learn arcane encodings, which required
specialized training. A number of failed communication schemas consequently
attempted to bridge the rift between human and machine alphabets. “You must
acknowledge that this is readable without special training,” Hymen Goldberg
wrote in the patent application for his 1911 Controller.[38] The device was
made “to provide [a] mechanism operable by a control sheet which is legible to
every person having sufficient education to enable him to read.” In an
illustration attached to his patent, Goldberg pictured a “legible control
sheet [...] in which the control characters are in the form of the letters of
the ordinary English alphabet.”[39] Goldberg’s perforations did the “double
duty” of carrying human-readable content and mechanically manipulating machine
“blocks,” “handles,” “terminal blades,” and “plungers.”[40] Unlike other
schemas, messages in Goldberg’s alphabet could be “read without special
information,” effectively addressing the problem of code’s apparent
unintelligibility (Fig 4.3).[41]

[fig4.3]

The inscription remained visible at the surface of Goldberg’s control sheet,
as a perforated figure punched through the conduit. Whatever challenges punch
cards and ticker tape presented for readers, these were soon complicated by
the advent of  magnetic tape.

## 2. Opacity

One could hardly call early programmable media ephemeral.  Anecdotes circulate
about Father Roberto Busa, an early pioneer of computational philology, who in
the 1960s carted his punch cards around Italy in his truck.[43] Codified
inscription, before its electromagnetic period, was fragile and unwieldy. Just
like writing with pen and paper, making an error on ticker tape entry required
cumbersome corrections and sometimes wholesale reentry of lines or pages. On
the surface of ticker tape, the inscription still made a strong commitment to
the medium. Once committed to paper, it was nearly immutable. Embossed onto
ticker tape or punched into the card, early software protruded through the
medium.

In the age of telegraphy encoding stood in the way of code comprehension.
Morse code and similar alphabet conventions at least left a visible mark on
the paper. They were legible if not always intelligible. Once the machine
encoding was identified (as Morse, Baudot, or Murray), it could be translated
back into natural language using a simple lookup table.

Magnetic tape changed the commitment between inscription and medium. It
provided a temporary home, where the word could be altered before being
committed to paper. At the 1967 Symposium on Electronic Composition in
Printing, Jon Haley, staff director of the Congressional Joint Committee on
Printing, spoke of “compromises with legibility [that] had been made for the
sake of pure speed in composition and dissemination of the end product.”[44] A
new breed of magnetic storage devices allowed for the manipulation of words in
“memory,” on a medium that was easily erased and rewritten. The magnetic
charge adhered lightly to the tape surface. This light touch gave the word its
newfound ephemeral quality. But it also made inscription illegible. In
applications such as law and banking, where the fidelity between input,
storage, and output was crucial, the immediate illegibility of magnetic
storage posed a considerable engineering challenge. After the advent of
teletype but before cathode ray screens, machine makers used a variety of
techniques to restore a measure of congruence between invisible magnetic
inscription and its paper representation. What was entered had to be verified
against what was stored.

The principles of magnetic recording were developed by Oberlin Smith (among
others), an American engineer who also filed several patents for inventions
related to weaving looms. In 1888, inspired by Edison’s mechanical phonograph,
Smith made public his experiments with an “electrical method” of sound
recording using a “magnetized cord” (cotton mixed with hardened steel dust) as
a recording medium. These experiments were later put into practice by Valdemar
Poulsen of Denmark, who patented several influential designs for a magnetic
wire recorder.[45]

Magnetic recording on wire or plastic tape offered several distinct advantages
over mechanical perforation. Tape was more durable than paper; it could fit
more information per square inch; and it was reusable. “One of the important
advantages of magnetic recording,” Marvin Camras, a physicist with the Armour
Research Foundation, wrote in 1948, “is that the record may be erased if
desired, and a new record made in its place.”[46] Most early developments in
magnetic storage were aimed at sound recording. The use of magnetic medium for
data storage did not take off in earnest until the 1950s.[47] However, early
developers of electromagnetic storage and recording technology already
imagined their work in dialog with the long history of letters (and not just
sound).  In an address to the Franklin Institute on December 16, 1908, Charles
Fankhauser, the inventor of the electromagnetic telegraphone, spoke as
follows:

To transport human speech over a distance of one thousand miles is a wonderful
achievement. How much more wonderful, then, is the achievement that makes
possible<3.>its storage at the receiving end, so that the exact sentence, the
exact intonation of the voice, the exact timbre, may be reproduced over and
over again, an endless number of times.[48]

Comparing magnetic recording to the invention of the Gutenberg press,
Fankhauser added:

@ext:It is my belief that what type has been to the spoken word, the
telegraphone will be to the electrically transmitted word<4.>As printing
spread learning and civilization among the peoples of the earth and influenced
knowledge and intercourse among men, so I believe the telegraphone will
influence and spread electrical communication among men.[49]

In that speech Fankhauser also lamented the evanescence of telegraph and
telephone communications. The telephone, he rued, fails to preserve “an
authentic record of conversation over the wire.”[50] Fankhauser imagined his
telegraphone being used by

@ext:the sick, the infirm, [and] the aged<4.> A book can be read to the
sightless or the invalid by the machine, while the patient lies in bed.
Lectures, concerts, recitations--what one wishes, may be had at will. Skilled
readers or expert elocution teachers could be employed to read into the wires
entire libraries.[51]

@tae:Anticipating the popularity of twenty-first-century audio formats such as
{stat I this case if possible} podcasts and audiobooks, Fankhauser spoke of
“tired and jaded” workers who would “sooth [themselves] into a state of
restfulness” by listening to their favorite authors.[52] Fankhauser saw his
“electric writing” emerge as “clear” and “distinct” as “writing by hand,” “an
absolutely legal and conclusive record.”[53] Whereas written language was
lossy and reductive, Fankhauser hoped that electromagnetic signals would hold
high fidelity to the original.

@tx:In 1909 Fankhauser thought of magnetic storage as primarily an audio
format that would combine the best of telegraphy and telephony. Magnetic data
storage technology did not mature until the 1950s, when advances in composite
plastics made it possible to manufacture tape that was cheaper and more
durable than its paper or cloth alternatives. The state-of-the-art relay
calculator, commissioned by the Bureau of Ordinance of the Navy Department in
1944 and built by the Computation Laboratory at Harvard University in 1947,
still made use of standard-issue telegraph “tape readers and punchers” adapted
for computation with the aid of engineers from Western Union Telegraph
Company.[54] It was equipped with a number of Teletype Model 12A tape readers
and Model 10B perforators, using <fr>11/16</fr>-inch-wide paper tape,
partitioned into “five intelligence holes,” where each quantity entered for
computation took up thirteen lines of code.[55] Readers and punchers were
capable of running 600 operations per minute. Four Model 15 Page-Printers were
needed to compare printed characters with the digits stored on the ticker tape
print register. The numerical inscription in this setup was therefore already
split between input and output channels, with input stored on ticker tape and
output displayed in print.

The Mark III Calculator, which followed the Computation Laboratory’s earlier
efforts, was also commissioned by the Navy’s Bureau of Ordinance. It was
completed in 1950. Its organization or “floor plan” (“system architecture,” we
would say today) did away with punch cards and ticker tape, favoring instead
an array of large electromagnetic drums coupled with reel-to-reel tape
recorders. The drums, limited in their storage capacity, revolved at much
faster speeds than tape reels. They were used for fast, temporary internal
storage. A drum’s surface was coated with a “thin film composed of finely
divided magnetic oxides of iron suspended in a plastic lacquer, and applied to
the drums with an artist’s air brush.”[56] The Mark III Calculator used
twenty-five such drums, rotating at 6,900 rpm; each was capable of storing 240
binary digits.

In addition to the fast “internal storage” drums, the floor plan included
eight slow “external storage” tape-reader mechanisms. Tape was slower than
drums but cheaper. It easily extended to multiple reels, thus approaching the
architecture of an ideal Turing machine, which, if you recall, called for tape
of “infinite length.”  In practice, tape was in limited supply merely long
enough to answer the needs of military computation. Unlike stationary drums,
tape was portable. Operators could prepare tape in advance, in a different
room, at the allotted instructional tape preparation table. The information on
tape would then be synced with and transferred to a slow drum. In the next
stage the slow drum accelerated to match the higher rotating speeds of the
more rapid internal storage drums, and the information was transferred again
for computation. The Mark III Calculator was further equipped with five
printers “for presenting computed results in a form suitable for publication.”
The printers were capable of determining the “number of digits to be printed,
the intercolumnar and interlinear spacing, and other items related to the
typography of the printed page.”[57] In reflecting on these early
“supercomputers,” one imagines the pathway of a single character as it crosses
surfaces, through doorways and interfaces, gaining new shapes and
temporalities with each transition.

Electromagnetic signals were transcoded into binary numerical notation. To
transfer characters onto tape, operators sat at the numerical tape preparation
table, yet another  separate piece of furniture. Data were stored along two
channels, running along the tape’s length. Operators entered each number
twice, first into channel A and then into channel B. This was done to prevent
errors, because the operators worked blindly, unable to see whether the
intended mark registered properly upon first entry. An error bell would sound
when the first quantity did not match the second, in which case the operator
would reenter the mismatched digits. To “ensure completely reliable results,”
one of the five attached Underwood electric teletypes could further be used to
print all channels and confirm input visually.[58]

Before screes, the potential for incongruence between recondite data formats
and their apparent representation posed a significant problem. In a 1954
patent, filed on behalf of Burroughs Corporation, Herman Epstein and Frank
Innes described an “electrographic printer” involving an “electrical method
and apparatus for making electrostatic images on a dielectric surface by
electrical means which may be rendered permanently visible” (Fig 4.4).[59] The
electrographic printer anticipated the modern photocopier in that it proposed
to use dusting inks to reveal the static charge. Rather than encoding its data
into another representation, such as the Baudot code, the printer traced
human-legible letter shapes directly onto tape. A small printing head would
convert binary input into a five-by-seven grid of electromagnetic charges
rendering the English alphabet. Such magnetic shapes  were then made apparent
by combining them with a “recording medium” that had the “correct physical
properties to adhere to the electrostatic latent images.”[60] A light dusting
of powder ink would reveal the otherwise imperceptible magnetic inscription.
Tape and paper configurations could thus achieve a measure of literal analogy.

[fig4.4]

Advances in magnetic storage found their way into small businesses and home
offices a decade later. In 1964 IBM combined magnetic tape (MT) storage with
its Selectric line of electric typewriters (ST). Selectric typewriters were
popular because they were ubiquitous, relatively inexpensive, and could be
used to reliably transform a keyboard’s mechanical action into binary electric
signal. Consequently, they became a common input interface  in a number of
early computing platforms.[61] In  that it combined electromagnetic tape
storage with keyboard input, the MT/ST machine could be considered one of the
first personal “word processing” systems. Built on a simpler architecture than
its supercomputer cousins, the machine used a single tape reading and writing
mechanism. An advertisement in the American Bar Association Journal, circa
1966, called it the $10,000 typewriter, “worth every penny.” Where typists
previously had to stop and erase every mistake, the IBM MT/ST setup allowed
them to “backspace, retype, and keep going.” Mistakes could be corrected in
place, on magnetic tape, “where all typing is recorded and played back
correctly at incredible speed.”[62]

Despite its advantages, MT/ST architecture inherited the problem of legibility
from its predecessors: Information stored on tape was still invisible to the
typist. In addition to being encoded, electric alphabets were written in
magnetic domains and polarities, which lay beyond human sense.[63] One
therefore had to verify input against stored quantities to ensure
correspondence. But the stored quantity could be checked only by transforming
it into yet another inscription. Like Wittgenstein’s broken hermeneutic
circuit, the magnetic tape was insufficient to close the loop. To verify what
was stored the operator was forced to redouble the original inscription, in a
process that was   prone to error, because storage media could not be accessed
directly without specialized instruments.[64]

Users of the Mark III Calculator  were asked to input quantities several times
over. Another class of solutions involved making the magnetic mark more
apparent. For example, Youngquist and Hanes described their 1962 magnetic
reader as a

@ext:device for visual observation of magnetic symbols recorded on a magnetic
recording medium in tape or sheet form. Magnetic recording tape is often
criticized because the recorded signals are invisible, and the criticism has
been strong enough to deny it certain important markets. For example, this has
been a major factor in hampering sales efforts at substituting magnetic
recording tape and card equipment for punched tape and card equipment which
presently is dominant in automatic digital data-handling systems. Although
magnetic recording devices are faster and more troublefree, potential
customers have often balked at losing the ability to check recorded
information visually. It has been suggested that the information be printed in
ink alongside the magnetic signals, but this vitiates major competitive
advantages of magnetic recording sheet material, e.g., ease in correction,
economy in reuse, simplicity of equipment, compactness of recorded data,
etc.[65]

@tae:The magnetic reader consisted of two hinged plates (Fig 4.5). Youngquist
and Hanes proposed to fill its covers with a transparent liquid that would
host “visible, weakly ferromagnetic crystals.” When sandwiched between the
plates, a piece of magnetic tape incited the crystal medium, which would in
turn reveal the signal’s “visibl[e] outline.”[66]

[fig4.5]

@tx:  I have yet to find an account of a magnetic reader in use. The problem
they were designed to solve remained: Tape and paper were  fundamentally
incompatible media. Data plowed into rows on the wide plains of a broad sheet
had to be replanted along the length of a narrow plastic groove. To aid in
that transformation, the next crop of IBM Magnetic Selectric typewriters added
a composer control unit, designed to preserve some of the formatting lost in
transition between paper and plastic. . It could change margin size or justify
text in memory. The original IBM Composer unit justified text (its chief
innovation over the typewriter) by asking the operator to type each line
twice: “one rough typing to determine what a line would contain, and a second
justified typing.”[67] After the first typing, an indicator mechanism
calculated the variable spacing needed to achieve proper paragraph
justification. The formatting and content of each line thus required separate
input passes to achieve the desired result in print.

[tab4.1]

IBM’s next-generation Magnetic Tape Selectric Composer (MT/SC) build on the
success of its predecessors (see Table 4.1 for the evolution of the MT/ST
line). It combined a Selectric keyboard, magnetic tape storage, and  a
“composer” format control unit. Rather than having the operator type each line
twice, the MT/SC system printed the entered text twice: once on the input
station printout, which showed both content and control code in red ink, and a
second time as the final Composer output printout, which collapsed the layers
into the final typeset copy. Output operators still manually intervened to
load paper, change font, and include hyphens. The monolithic page unit was
thereby further systematically deconstructed into distinct strata of content
and formatting.

Like other devices of its time, the IBM MT/SC suffered from the problem of
indiscernible storage. Error checking of input using multiple printouts was
aided by a control panel consisting of eleven display lights. The machine’s
manual suggested that the configuration of lights be used to peek at the
underlying data structure for verification.[68]

In an attempt to achieve ever greater congruence between visible outputs and
data archived on a magnetic medium, IBM briefly explored the idea of storing
information on magnetic cards instead of tape. On tape, information had to be
arranged serially, into one long column of codes. Relative arrangement of
elements could be preserved, it was thought, on a rectangular magnetic card,
which  resembled paper in its proportions. The 1968 patent “Data Reading,
Recording, and Positioning System” describes a method for arranging
information on a storage medium “which accurately positions each character
recorded relative to each previous character recorded.”[69] In 1969 IBM
released a magnetic card<->based version of its MT/ST line, dubbed the MC/ST.
Fredrick May, whose name often appears on word-processing-related patents from
this period, would later reflect that a “major reason for the choice of a
magnetic card for the recording of medium was the simple relationship that
could be maintained between a typed page and a recorded card.” The card
approximated a miniature page, making it a suitable “unit of record of storage
for a typed page” (Fig 4.6).[70] Although it offered a measure of topographic
analogy between tape and paper, the “mag card” was short-lived partly because
of its limited storage capacity, capricious feeding mechanism, and its
persistent inscrutability .[71]  [fig4.6]

The structure of textual artifacts--from a simple leaflet to a novel in
multiple volumes--has remained remarkably stable since the invention of
movable type. One rarely finds a sentence that spans several paragraphs, for
example. Nor would a contemporary reader expect to find pages of different
sizes in the same tome. Long-standing historical conventions guide the
production of printed text. Likewise, semantic and decorative units on a page
exist within a strict hierarchy. No book of serious nonfiction, for example,
would be typeset in a cursive font. Unless something out of the ordinary
attracts their attention, readers tend to gloss the inconsequential details of
formatting in favor of content. The material contexts of a well-designed book
fade from view during reading.

For a few decades after the advent of magnetic storage media but before the
arrival of screen technology, the sign’s outward shape disappeared altogether.
It is difficult to fathom now, but at that time--after the introduction of
magnetic tape in the 1960s but before the widespread advent of CRT displays in
the 1980s--typewriter operators and computer programmers manipulated text
blindly. Attributes such as indent size and justification were decided before
ink was committed to paper.

In the 1980s an engineer thus reflected on the 1964 MT/ST’s novelty: “It could
be emphasized for the first time that the typist could type at ‘rough draft’
speed, ‘backspace and strike over’ errors, and not worry about the pressure of
mistakes made at the end of the page.” The MT/SC further added a programmable
control unit to separate inputs from outputs. Final printing was then
accomplished by

@ext:mounting the original tape and the correction tape, if any, on the
two-station reader output unit, setting the pitch, leading, impression control
and dead key space of the Composer unit to the desired values, and entering
set-up instructions on the console control panel (e.g., one-station or <grt>
two-station tape read, depending on whether a correction tape is present; line
count instructions for format control and space to be left for pictures, etc.;
special format instructions; and any required control codes known to have been
omitted from the input tape). During printing the operator changes type
elements when necessary, loads paper as required, and makes and enters
hyphenation decisions if justified copy is being printed.[72]

@tae:The tape and control units thus intervened between keyboard and printed
page. The “final printing” combined “prepared copy,” “control and reference
codes,” and “printer output.”[73] Historical documents often mention three
distinct human operators for each stage of production: one entering copy, one
specifying control code, and one handling paper output. These three could
hypothetically work in isolation from one another. The typist would see copy;
the typesetter would enter formatting and control codes; and the printer would
output the interpolated results.

@tx:Researchers working on these early IBM machines considered the separation
of print into distinct strata a major contribution to the long history of
writing. One IBM consultant went so far as to place the MT/SC at the
culmination of a grand “evolution of composition,” which began with
handwriting and continued to wood engraving, movable type, and letterpress:
“The IBM Selectric Composer provides a new approach to the printing process in
this evolution.” He concluded by heralding the “IBM Composer era,” in which
people would once again write books “without the assistance of
specialists.”[74] Inflationary marketing language aside, the separation of the
sign from its immediate material contexts and its new composite constitution
must  be considered a major milestone in the history of writing and
textuality.

The move from paper to magnetic storage had tremendous social and political
consequences for the republic of letters. Magnetic media reduced the costs of
copying and dissemination of the word, freeing it, in a sense, from its more
durable material confines. The affordances of magnetic media—its very speed
and impermanence—created the illusion of light ephemerality. Yet the material
properties of magnetic tape itself continued to prevent direct access to the
site of inscription. Magnetic media created the conditions for a new kind of
illiteracy, which divided those who could read and write at the site of
storage from those who could only observe its aftereffects passively, at the
shimmering surface of archival projection.

The discussed schematics embody textual fissure in practice. The path of a
signal through the machine leads to multiplicity of inscription sites. These
are not metaphoric but literal localities that stretch the sign across
manifold surfaces. Whereas pens, typewriters, and hole punches transfer  {ED:
this continues today, why not present tense?} inscription to paper directly,
electromagnetic devices compound them obliquely into a laminated aggregate.
The propagation of electric signal across space required and continues to
require numerous phase transitions between media: from one channel of tape to
another, from tape to drum, from a slow drum to a fast one, and from drum and
tape to paper. On paper the inscription remains visible in circulation; it
disappears from view on tape, soon after key press. Submerged beneath a facade
of opaque oxide, inscriptions thicken and stratify into laminates.

## 3. Mimesis

@to:The contemporary textual condition took its present form in the late
1960s. Computers subsequently changed in terms of size, speed, and ubiquity.
However, they retained {ED: not sure about the suggested past tense here: I am
trying to say that computers remain essentially the same today, in the
present.}the same essential architecture as they have today: {ED: I feel like
there needs to be a verb here, the transition to the list is otherwise abrupt}
programmable media, electromagnetic storage, and screens.

@tx: The addition of a screen to the floorplan could finally address the
problem of electromagnetic legibility. In the first stage of its digital
development, language became “programmable:” fused with machine instruction it
could be used to automate devices remotely. Language itself became automated.
Coupled with electromagnetic storage in the second stage, programmable media
was freed, to an extent,  from its immutable contexts. It was “lighter,”
faster, more portable and therefore more iterant and malleable than print or
punch. Ferric oxide became the preferred medium for digital storage: memory.
However, that  new memory layer, lay also beyond the reach of human senses. It
was difficult to access and manipulate mentally.

Screens added a much needed window onto the abstraction. On-screen,  machine
memory could be mapped and represented visually, obviating the need for double
entry or frequent printouts. Screens interjected to mediate between input and
output. They flattened stratified complexity to facilitate use. Textual
laminates were still invisible in part. Screen simulacra restored the
appearance of a single surface. However, it also obscured the dynamics of
mediation.

On December 9, 1968, Douglas Engelbart, then the  primary investigator at the
NASA- and ARPA-funded Augmentation Research Center at the Stanford Research
Institute, gave what later became known as the “mother of all demos” to an
audience of roughly 1,000 or so computer professionals attending the Joint
Computer Conference in San Francisco.[75] The flier advertising the event read
as follows:

@ext:This session is entirely devoted to a presentation by Dr. Engelbart on a
computer-based, interactive, multiconsole display system which is being
developed at Stanford Research Institute under the sponsorship of ARPA, NASA
and RADC. The system is being used as an experimental laboratory for
investigating principles by which interactive computer aids can augment
intellectual capability. The techniques which are being described will,
themselves, be used to augment the presentation. The session will use an
on-line, closed circuit television hook-up to the SRI computing system in
Menlo Park. Following the presentation remote terminals to the system, in
operation, may be viewed during the remainder of the conference in a special
room set aside for that purpose.[76]

@tae:The demo announced the arrival of almost every technology prophesied by
Vannevar Bush in his influential 1945 Atlantic essay, “As We May Think.”
During his short lecture, Engelbart presented functional prototypes of the
following: graphical user interfaces, video conferencing, remote camera
monitoring, links and hypertext, version control, text search, image
manipulation, windows-based user interfaces, digital slides, networked
machines, mouse, stylus, and joystick inputs, and “what you see is what you
get” (WYSIWYG) word processing.

@tx:In his report to NASA, Engelbart described his colleagues as a group of
scientists “developing an experimental laboratory around an interactive,
multiconsole computer-display system” and “working to learn the principles by
which interactive computer aids can augment the intellectual capability of the
subjects.”[77] CRT displays were central to this research mission. In one of
many patents that came out of his “intellect augmentation” laboratory,
Engelbart pictured his “display system” as a workstation that combines a
typewriter, a CRT screen, and a mouse. The schematics show the workstation in
action, with the words “<sc>now is the time fob</sc>” prominently displayed
on-screen. The user was evidently in the process of editing a sentence, likely
to correct the nonsensical “fob” into “for” (Fig. 4.7).[78]

[fig4.7]

Reflecting on the use of visual display systems for human-computer
interaction, Engelbart wrote, “One of the potentially most promising means for
delivering and receiving information to and from digital computers involves
the display of computer outputs as visual representations on a cathode ray
tube and the alteration of the display by human operator in order to deliver
instructions to the computer.”[79] The first subjects to read and write
on-screen reported feeling freedom and liberation from paper. An anonymous
account included in Engelbart’s report offered the following self-assessment:

@cext:1B2B1 To accommodate and preserve a thought or

piece of information that isn’t related to the work

of the moment, one can very quickly and easily

insert a note within the structure of a file at such

a place that it will neither get in the way nor get

lost.

1B2B2 Later, working in another part of the file,

he can almost instantly (e.g. within two seconds)

return to the place where he temporarily is storing

such notes, to modify or add to any of them.

 

1B2B3 As any such miscellaneous thought develops,

it is easy (and delightful) to reshape the structure

and content of its discussion material.[80]

 

@tae:Writing, which this typist previously perceived as an ordered and continuous activity, subsequently was performed in a more disjointed way. The typist could delight in shaping paragraphs that more closely matched her mental activity. Screens thus restored some of the fluidity of writing that typewriters denied. Writers could, for example, pursue two thoughts at the same time, documenting both at different parts of the file as one would in a notebook. Not constrained by the rigidity of a linear mechanism, they moved around the document at will.

            @tx:Engelbart recorded what must count as some of the most evocative passages to appear in a NASA technical report. His “Results and Discussion” section contains the following contemplation by an anonymous typist:

 

@cext:1B4 I find that I can express myself better, if I can

make all the little changes and experiments with wording

and structure as they occur to me. [Here the user

experiments a little with using structural decomposition

of a complex sentence.][81]

 

@tae:A decomposition follows indeed. The author deviates dramatically from technical writing conventions. Numbered passages along with unexpected enjambment heighten the staccato quality of prose, which attains an almost lyrical quality:

 

@cext:1B4A I find that I write faster and more freely,

 

1B4A1 pouring thoughts and trial words onto the

screen with much less inhibition,

 

1B4A2 finding it easy to repair mistakes or wrong

choices

 

1B4A2A so while capturing a thought I don’t

have to inhibit the outpouring of thought and

action to do it with particular correctness,

 

1B4A3 finding that several trials at the right

wording can be done very quickly

 

1B4A3A so I can experiment, easily take a look

and see how a new version strikes me--and often

the first unworried attempt at a way to express

something turns out to be satisfactory, or at

least to require only minor touch up.

 

1B4A4 Finding that where I might otherwise

hesitate in search of the right word, I now pour out

a succession of potentially appropriate words,

leaving them all there while the rest of the

statement takes shape. Then I select from among

them, or replace them all, or else merely change the

list a bit and wait for a later movement of the

spirit.[82]

 

@tae:When input and output coincide in time, as they do on paper, mistakes are costly. Once inscribed, the sign gains permanence; it is difficult to emend. An eraser can help remove a layer of physical material. Alternatively, writers use white ink to restore the writing surface. Engelbart’s anonymous typist reports the feeling of freedom from such physical commitment. She can simply backspace and start over. Words come easily because there are no penalties for being wrong. Virtual space seems limitless and endlessly pliable.

            @tx:The feeling of material transcendence--the ephemeral quality of digital text--is tied directly to the underlying physical affordances  of  electromagnetic storage. Screens expose the pliability of the medium, where erasure is effortless. Content can be addressed in memory and copied at the stroke of a key. The numbered paragraphs suggest a novel system for recollection. Data storage units become, in a sense, mental units. I am struck by the distinctly phenomenological quality of technical description: The editor does not merely resemble a page; it is, for the writer, a newly discovered way of thought that changes the writer’s relation not only to text but also to her own thoughts. The highly hierarchical and blocky paragraph structure, along with its repetitive refrain (“finding” and “I find that”), gives the prose a hypnotic drive forward. The cadence matches the reported experience of discovery.

            The writer continues:

 

@cext: 1B4B I find that

 

1B4B1 being much more aware of

 

1B4B1A the relationships among the phrases of a

sentence,

 

1B4B1B among the statements of a list,

 

1B4B1C and among the various level and members

of a branch,

 

1B4B2 being able

 

1B4B2A to view them in different ways,

 

1B4B2B to rearrange them easily,

 

1B4B2C to experiment with certain special

portrayals,

 

1B4B2C1 not available easily in unstructured

data

 

1B4B2C2 or usable without the CRT display,

 

1B4B3 and being aware that

 

1B4B3A I can (and am seeking to) develop still

further special conventions and computer aids

 

1B4B3B to make even more of this available and

easy,

 

1B4B4 all tend to increase

 

1B4B4A my interest and experimentation

 

1B4B4B and my conviction that this is but a

peek at what is to come soon.[83]

 

@tae:The passages appear too contrived to be spontaneous. Despite its experimental structure, these phenomenological reflections advance key elements of Engelbart’s research program, which aimed to develop new data structures in combination with new ways of displaying them. Yet I cannot help but be moved by the fluency of the prose and by the sheer audacity of the project.

            @tx:Engelbart’s research into intellect augmentation created tools that augment research. In an image that evokes Baron Münchhausen pulling himself out of a swamp by his own bootstraps, Engelbart called his group’s methodology “bootstrapping,” which involved the recursive strategy of “developing tools and techniques” to develop better tools and techniques.[84] The “tangible product” of such an activity was a “constantly improving augmentation system for use in developing and studying augmentation systems.”[85]

            It was an appealing vision, but only so long as it remained recursive. Engelbart’s group benefited from creating their own tools and methods. Engelbart also hoped that his system could be “transferred--as a whole or by pieces of concept, principle and technique--to help others develop augmentation systems for many other disciplines and activities.”[86] Undoubtedly, Engelbart’s ideas about intellect augmentation have had a broad effect on knowledge work across disciplines. However, his vision loses the property of self-determination when transferred outside the narrow confines of a laboratory actively engaged in the transformation of material contexts of their own knowledge production. Word processing today rarely involves communities pulling themselves up by their own bootstraps: using their tools and techniques of their own design. Augmentation enforced from without often advances values and principles no longer comprehensible to the entity being augmented.

            To bring his system into being, Engelbart convened a community that through recursive self-improvement could lift itself up toward a smarter, more efficient, more human way of doing research. The group crafted novel instruments for reading and writing. They engineered new programming languages, compilers to interpret them, and debuggers to troubleshoot them. The system shows care and love for the craft of writing. But there is also complexity. “This complexity has grown more than expected,” Engelbart wrote in conclusion.[87] The feeling of transcendence that the anonymous typist describes in using the system engages a sophisticated mechanism. The mechanism was not, however, the primary instrument of augmentation. Rather, it was the process of designing, making, and experimenting with tools that enhanced the intellect.. Engelbart wrote, “The development of the Bootstrap Community must be coordinated with the capacity of our consoles, computer service, and file storage to support Community needs, and with our ability to integrate and coordinate people and activities.”[88] In other words, the development of the community must form a feedback loop with software development. It involves training, practice, critical self-reflection, and thoughtful deliberation.

            Modern word processors enable us to drag and drop passages with unprecedented facility. We live in Engelbart’s world, to the extent that we use his lab’s complex systems daily and in a smaller configuration: screens, keyboards, storage. Today’s computer users rarely form a self-determined bootstrapping community, however. The contemporary writer is bootstrapped passively to the prevailing vision of intellect augmentation. The very metaphor of bootstrapping suggests the impossibility of using one’s bootstraps to pull others out of the Platonic cave. Engelbart’s liberatory research program therefore left another less lofty imprint on the everyday practice of modern intellectual life. Text, which before the advent of the CRT was readily apparent on the page in all its fullness, finally entered a complex system of executable code and inscrutable control instruction. The material lightness of textual being came at the price of legibility.

            Short-lived screenless word processors of the early 1960s (e.g., the MT/ST) were difficult to operate, because typists had no means to visualize complex data structures on tape. Screens helped by representing document topography visually, restoring a sense of apparent space to otherwise opaque media. The contemporary digital document may resemble a page on-screen, but beneath it, it is a jumble of bits, split into the various regions of internal memory. Screens simulate document unity by presenting holistic images of paragraphs, pages, and books. The simulation seems to follow the physics of paper and ink: One can turn pages, write in margins, and insert bookmarks. But the underlying inscription remains in fracture. Simulated text does not transcend matter. Screens merely conceal its material properties while recreating others, more seemingly transcendent ones. The act of continual dissemblage, one medium imitating the other, manufactures an ephemeral illusion by which pages fade in and out of sight, paper folds in improbable ways, and words glide effortlessly between registers of copy and paste.

            In the rift between  input and output, programmable media inject arbitrary intervals of time and space. Forces of capital and control occupy the void as the sign acquires new dimensions and capabilities for automation. Code and codex subsequently sink beneath the matte surface of a synthetic storage medium. Screens purport to restore a sense of lost immediacy, of the kind felt on contact between pen nib and paper as the capillary action of cellulose conveys ink into its shallow conduit.

            Screens are meant to open a window onto the unfamiliar physicalities of electromagnetic inscription. For example, they obviate the need for multiple typings or printouts. Projected image should, in theory, correspond to its originating keystroke. The gap separating inputs and outputs appears to close. Crucially, the accord between archived inscription and its image cannot be guaranteed. The interval persists in practice and is actively contested. Deep and shallow inscriptions entwine. Laminate text seems weightless and ephemeral at some layers of the composite, allowing for rapid remediation. At other layers its affordances are determined by its physics; at still other layers they are carefully constructed to resist movement or interpretation. Alienated from the base particulates of the word, we lose some of our basic interpretive capacities to interrogate embedded power structures.

[^1]: Chun, "Enduring Ephemeral," 148.

[^2]: See Levine, *Forms*, 6-11. Levine explains: "Affordance is a term used
to describe the potential uses or actions latent in materials and designs.
Glass affords transparency and brittleness. Steel affords strength,
smoothness, hardness and durability. Cotton affords fluffiness, but also
breathable cloth when it is spun into yarn and thread" (6).

[^3]: The situation is more complicated for pre-modern texts, where the
materiality of print cannot be taken for granted.

[^4]: See also Gadamer, *Truth and Method*, 110: "In both legal and
theological hermeneutic there is an essential tension between the fixed
text---the law or the gospel---on the one hand and, on the other, the sense
arrived at by applying it at the concrete moment of interpretation."

[^5]: Ricoeur, *Interpretation Theory*, 28.

[^6]: See, for example, Huhtamo and Parikka's "Introduction" in *Media
Archaeology*, 3: "Media Archaeology should not be confused with archaeology as
a discipline. When media archaeologists claim they are 'excavating'
media-cultural phenomena, the word should be understood in a specific way."
The use of quotes signals the metaphoric nature of the borrowing. See also
Grant Wythoff, "Artifactual Interpretation"  Wythoff writes: "[H]ow do we
close the metaphorical divide between the 'excavations' performed in
archaeology and media archaeology?" (27). On the use of stratigraphy related to
hard drive forensics see Perry and Morgan, "Materializing Media Archaeologies:
the MAD-P Hard Drive Excavation."

[^7]: See Schenck, "Applied Paleontology"; Simonetti, "Between the Vertical
and the Horizontal: Time and Space in Archaeology"; and Geikie, "The Rise of
Stratigraphical Geology in England" in *The Founders of Geology*, 337-364.

[^8]: For periodization of computer systems see Denning, "Third Generation
Computer Systems."

[^9]: Programmable media have multiple origins, worthy of their own extended
history. The French textile worker Basile Bouchon used "drill paper" to
automate industrial drawlooms. The invention of the loom could also be
attributed to the Banu Musa brothers, ninth-century automata inventors from
Baghdad; to Jacques de Vaucanson, who delighted the public with his lifelike
mechanisms in the mid-eighteenth century; or to Joseph Charles Marie Jacquard,
who improved on and popularized Bouchon’s looms on an industrial scale around
the same time. See Koetsier, "Prehistory of Programmable Machines," 593-95;
Randell et al., "History of Digital Computers"; and Riskin, "Defecating Duck."

[^10]: Adler and Albertman, “Knitting Machine”; Casper, “Remote Control
Advertising”; Hough, “Wired Radio Program Apparatus.”
