Laminate Text: Material Contexts of Digital Knowledge Production
Dennis Yi Tenen

Digital text, at the basis of all computer-mediated epistemological activity,
appears to view at once an ephemeral and enduring phenomenon. It is, as Wendy
Hui Kyong Chun wrote, an "enduringly ephemeral" inscription that "create[es]
unforeseen degenerative links between humans and machines."[^1] At the site of
its projection, on "soft" screens, the text shimmers and wanes, suspended in
liquid crystal, in response to the modulation of an electric signal. At the
site of its storage, on "hard" drives, among "floating gates" and
ferromagnetic polarities, the text adheres to recondite surfaces persistently.
There, it multiplies and obstinately spreads, like silicon dust, filling the
available crevices with stored data, far beyond the immediate contexts of
inscription.

This inherent duplicity of electromagnetic inscription, as I argue here,
results in conflicting critical accounts in the scholarly literature.
Furthermore, it engenders a fundamental alienation from the material contexts
of digital knowledge production. The alienation ultimately threatens critical
disempowerment. In this essay, I outline a condition I name *laminate
textuality*, by which an inscription fractures to occupy multiple surfaces
concurrently. A historical archaeology of electromagnetic writing anchors the
more theoretical portions of the essay within the emerging affordances of
computational media, which take place, in part, at quantum scale, beyond the
reach of humans senses. Clandestine forces of capital and control subsequently
contest such newly found microscopic expanses, thus limiting the scope of
possible interpretive activity.

## Literary Composites

Little separates ink from paper in print. Ink permeates paper; the conduit is
firmly embedded into the medium. The amalgam of ink and paper implies specific
physical properties and their related affordances: what the medium *is*
relates to what can be *done* with it.[^2] Print is stable on the scale of
decades and sometimes centuries. At the very least, once can be assured that a
text will remain the same as it passes from one pair of hands into another. As
interpreters of texts, we usually can (with some effort) make certain to find
ourselves, literally, on the same page and to insure that our discussion
concerns roughly the same piece of writing.

Traditional models of modernist hermeneutics often assume the relative
permanence of the medium.[^3] For example, Paul Ricoeur wrote about "the
possibility of transferring orders of long distances without serious
distortions" as related to the "birth of political rule exercised by a distant
state."[^4] Among other effects, Ricoeur attributed to the fixity of print
"the birth of market relationships" and economics, "the birth of justice" in
the fixity of the legal codex, "the constitution of archives" and therefore
the very possibility of history. He concluded to write that "such an immense
range of effects suggests that human discourse is not merely preserved from
destruction by being fixed in writing, but that it is deeply affected in its
communicative function."[^5]

The architecture of contemporary computational media---screens, hard drives,
and keyboards---cannot sustain the assumption of fixity. Electromagnetic
inscription occupies several available surfaces at once. It exists on screen,
itself a composite of glass panes and liquid crystal, and "in memory," as
manifested in the arrangement of silicon and circuitry. The word is in the
wires. When reading digitally, on a personal computer for example, the
distance between these localities may extend a few inches, enough to cover the
space between a screen and a hard drive. In other contexts, when viewing text
on a commercial "electronic reader," that space may span continents as stored
data are transmitted over vast distances, between content "owners" and its
"users," who hold only a temporary right to peruse, limited in scope to
specific timelines and geographies. An electronic book borrowed from the New
York Public Library, for example, may be stored somewhere in the city, on
library servers. It disappears from my devices when the terms of my book
borrowing expire.

In this way, the digital sign is continually stretched, elongated, and
fractured across diverse material strata, subject to distinct local
affordances. A simple act of erasure on screen, as when one backspaces over a
word written in error, simulates a comparable action in print. On disk,
metaphoric "erasure" assumes a different connotation, entailing an entirely
new set of operations, not congruent with the implied action on screen or in
print. The incongruence is often benign. Readers and writers are often
interested in handling surface representation alone, dwelling on the level of
words and ideas. In certain contexts however, our inability to perceive the
mechanics of inscription at depth severely undermines critical acuity. To take
a blunt example, imagine a scholar or a journalist who needs to redact
unpublished materials in order to protect her sources. Surface erasure in this
case is insufficient to guarantee true anonymity. Words erased on screen may
persevere through deeper structures on disk and among remote servers, in a way
susceptible to malicious breach or state-sponsored surveillance. Surface
anonymity is sufficient at times, while other times require deep anonymity.

The affordances of laminate text depend not only on the medium, but also on
the contexts of its reception. The same "source text" may be transformed
according to its geography or the identity of its reader. The composition of
the sign changes as a text changes hands. A digital document may respond to a
reader's location, gender, age, or ethnicity. Think of an online newspaper, to
take a common example, where the very composition of headlines and stories are
often tailed to the reader. Such dynamic inscriptions contain the rules of
their own transformation. Content and control code intertwine to produce an
amalgamated artifact, which adapts to its context. The often covert presence
of control structures extend the reach of governance over the process of
interpretation. A government may censor speech through legislation, for
example, and prosecute those who express opinions contrary to the reigning
ideology. Similarly, mechanisms of censorship may be encoded into the fabric
of the laminate itself, through technologies that simply prevent prohibited
ideological formations from appearing on screen. Such technologies can also
limit the transmission and the reception of illicit material. Unlike those
censored explicitly, readers under the reach of algorithmic governance may not
be immediately aware of control mechanisms structuring their everyday
interpretive experience.

The stratified nature of digital inscription poses obvious challenges in the
political sphere. Our ability to mobilize against censorship or surveillance
is in peril when such mechanisms operate at a microscopic scale, requiring
specialized training and tools for interpretation. Laminate text presents a
challenge to the practice of literary hermeneutics more generally. How does
critical interpretive practice persist in conditions where readers can no
longer rely on the continuing stability of the medium? The theoretical problem
is not one of fixing a text as either an immanent object or a transcendent
idea, enduring or ephemeral. We are confronted instead with texts that do not
converge on a single location and whose multivalence is derived from their
structural diffusion. Composite media force us to reconsider long-standing
critical assumptions about the physics of inscription at the foundation of
hermeneutics.

## Stratigraphy

Having summarized the basic theoretical concerns related to the emerging
material properties of laminate text, I would like to turn my attention now to
the historical archaeology of the medium itself. I borrow the term
"stratigraphy" from the field of archaeology proper, as opposed to media
archaeology where the term is used in its more evocative and metaphoric
sense.[^6] The art of analyzing sedimentary rock layers dates back at least to
the geological observations of the seventeenth century Danish scientist
Nicolas Steno, later extended by John Strachey in his *Observations on the
Different Strata of Earths and Minerals* (1727), and to the geological
cross-sections of William Smith and William Maclure, who drew beautifully
detailed cross-sections of the British and American landscapes [Figure 1].

![Stratigraphic map legend. "Sketch of the Succession of Strata and their
relative Altitudes." William Smith, "DELINEATION of the STRATA of ENGLAND and
WALES with part of SCOTLAND; exhibiting the COLLIERIES and MINES; the MARSHES
and FEN LANDS ORIGINALLY OVERFLOWED BY THE SEA; and the VARIETIES of Soil
according to the Variations in the Sub Strata; ILLUSTRATED by the MOST
DESCRIPTIVE NAMES," Section 16 (1815). Image in the public domain. Reproduced
from the collection of Sedgwick Museum of Earth Sciences, University of
Cambridge.](/image/smith-16.jpg)

The concept of stratigraphy is an apt borrowing when applied to the history of
computer technology, where the various layers of historical development are
often extant in one and the same device. In this manner, the legacy of machine
alphabets such as the Morse and Baudot alphabets is, in some real way, present
on modern devices through the ASCII and UTF-16 conventions. A
"conversational," text-based, model of human computer-interaction developed in
the 1960s, coexists with the later, graphic-based "direct-interaction" modes
of interaction in the innards of every modern mobile phone, game console, or
tablet.[^8] The same can be said about "low-level" assembly languages and
their alternatives like FORTRAN, LISP, and COBOL, continuously in use since
the 1950s alongside their modern descendants.

For the purposes of our conversation, which concerns the paradoxically
conflicting nature of electromagnetic inscription, at once enduring and
ephemeral, we may separate the formation of electromagnetic composites into
three historical periods, each leaving behind a distinctive media sediment.

First, with the advance of telecommunications, we observe the increasing
admixture of human-readable text and machine-readable code. Removable storage
media such as ticker tape and punch cards embodied a machine instruction set
meant to effect a mechanism, which in turn produced human-legible inscription.
Unintelligible (to humans without special training) control codes that
actuated machinery were thereby mixed with plain text, the content of
communication. Inscription split between the sites of storage, where an
expanded machine instruction set was archived, and projection, where the human
legible portion of the inscription was displayed.

Second, whereas ticker tape and punch cards were legible to the naked eye,
magnetic tape, a medium which supplanted paper, made for an inscrutable
substance, inaccessible directly without instrumentation. In the 1950s and
1960s machine operators worked blindly, using complicated workarounds to
verify equivalence between data input, storage, and output. Writing began to
involve multiple typings and printings. Specialized magnetic reading devices
were developed to make inscription more apparent and to establish a
correspondence between input, storage content, and output of entered text. The
physical properties of electromagnetic inscription also allowed for rapid
re-mediation. Tape was more forgiving than paper: it could be written and
re-written at high speeds and volume. However, the opacity of the medium has
also placed it, in practice, beyond human sense.

Finally, the appearance of cathode-ray tube (CRT) displays in the late 1960s
restored a measure of legibility lost to magnetic storage. The sign reemerged
on-screen. Crucially, it framed a *simulacrum* of archived inscription: typing
a word on a keyboard produced one sort of a structure on tape or disk and
another on-screen. The two related contingently, without necessary
equivalence. The lay reader lost the direct means to ensure a correspondence
between visible trace and stored mark. An opaque black box of automated word
processing (rules for transmediation) began to intercede between simultaneous
acts of writing and reading.

I have selected a number of textual machines to illustrate this condensed
history. Three mechanisms mark the journey: the Controller patented by Hyman
Goldberg in 1911; the Magnetic Reader introduced by Robert Youngquist and
Robert Hanes in 1958; and, for a lack of a better name, the Display System,
introduced by Douglas Engelbart in 1968.

Goldberg’s device embodied an alphabet understood by both humans and machines.
Youngquist and Hanes attempted to give human operators a glimpse into the
hidden world of magnetic polarities and electric charges. Engelbart’s Display
System belongs to what Peter Denning, a prominent computer scientist, has
called the "third generation" of computer systems. This third, and still
current stage, constitutes an assemblage of storage, input, and output
technologies.[^8] Together, these devices tell a story of a fissure at the
heart of our contemporary textual predicament.

## 1. Controller

The turn of the twentieth century was a pivotal period in the history of
letters. It saw the languages of people and machines enter the same mixed
communications stream. Artificial fixed-length alphabets, such as the Baudot
code, paved the way for the automation of language. The great variety of human
scripts was reduced to a set of discrete and reproducible characters. So
regularized, type was converted into electric signal, sent over great
distances, and used to program machines remotely. These expanded textual
affordances came at a price of legibility. Initially, a cadre of trained
machine operators were required to translate human language into
machine-transmittable code. Eventually, specialized equipment automated this
process, removing the human from the equation.

The advent of programmable media (punch cards and ticker tape) coupled
human-compatible alphabets with machine control code.[^9] Reduced to a
discrete and reliably reproducible set of characters, natural languages could
be conveyed as electric signals. In such a transitive state language became
more mobile than ever before. It was transmitted efficiently across vast
distances. The mechanization of type also introduced new control characters
into circulation capable of affecting machine state changes at a distance.
Initially, such state changes were simple: “begin transmission,” “sound error
bell,” or “start new line.” With time, they developed into what we now know as
programming languages. Thereby content meant for human consumption was
routinely intermixed with code meant to control machine devices. In other
words, a text stream could "mean" one thing to a human interpreter and another
to a machine one. Such early remote control capabilities were quickly adapted
to automate everything from radio stations to advertising billboards and
knitting machines.[^10]

Hundreds of alphabet systems vying to displace Morse code were devised to
speed up automated communications. These evolved from variable-length
alphabets such as Morse and Hughes codes, to fixed-length alphabets such as
Baudot and Murray codes. The systematicity of the signal---equal length at
regular intervals---minimized the "natural" aspects of human languages, affect
and variation, in favor of "artificial" aspects like consistency and
reproducibility.

The fixed-length property of Bacon’s cipher, later implemented in the 5-bit
Baudot code, signaled the beginning of the modern era of serial
communications.[^11] The Baudot and Murray alphabets were designed with
automation in mind.[^12] Both did away with the “end of character” signal that
separated letters in Morse code. Signal units were to be divided into letters
by count, with every five codes representing a single character. Temporal
synchronization was therefore unnecessary, given the receiver’s ability to
read the message from the beginning. A character was simply a unit of space
divisible by five.

In addition, the Murray code was more compact than Morse code and especially
more economical than Hughes code, which, in the extreme, used up to fifty-four
measures of silence to send a signal representing double quotes.[^13] The
signal for “zero” in Morse code occupied twenty-two measures. By contrast, all
Baudot and Murray characters were a mere five units in length, with the
maximum of ten used to switch the receiving device into “figure” or “capital
letter” states, for the total of ten units.[^14]

Fixed-length signal alphabets drove the wedge further between human and
machine communication. Significantly, the automated printing telegraph
decoupled information encoding from its transmission. Fixed-length encoding of
messages could be done in advance, with more facility and in volume. Prepared
messages could then be fed into a machine without human assistance. In 1905
Donald Murray wrote that the “object of machine telegraphy [is] not only to
increase the saving of telegraph wire [...] but also to reduce the labour cost
of translation and writing by the use of suitable machines.”[^15] Baudot’s and
Murray’s codes were not only shorter but also simpler and less error-prone and
thus resulted in less complicated and more durable devices.

With the introduction of mechanized reading and writing techniques, telegraphy
diverged from telephony to become a means for truly asynchronous
communication. It displaced signal transmission in time as it did in space.
The essence of algorithmic control, amplified by remote communication devices,
lies in its ability to delay execution; a cooking recipe, for example, allows
novice cooks to follow instructions without the presence of a master chef.
Similarly, delayed communication could happen in absentia, according to
predetermined rules and instructions. A message could activate a machine that
prints a log and another that trades stocks and another that replies with a
confirmation.

Just as perforated music could be used to displace the act of performance in
time, programmable media deferred the act of inscription. Writers also became
de facto programmers. One could strike a key in the present, only to feel the
effects of its impact later, at a remote location. Decoupled from its human
sources, inscription could be compressed and optimized for speed and
efficiency. Electromechanical readers could ingest prepared ticker tape and
punch cards at rates far exceeding the possibilities of hand-operated Morse
telegraphy.

Besides encoding language, the Baudot schema left space for several special
control characters. The "character space" of an existing alphabet was
therefore expanded further by switching the receiving mechanism into a special
control mode in which every combination of five bits represented an individual
control character, instead of a letter. In this manner, human content and
machine control intertwined to occupy the same spectrum of communication.

By the 1930s devices variously known as printer telegraphs, teletypewriters,
and teletypes displaced Morse code telegraphy as the dominant mode of
commercial communication. A 1932 U.S. Bureau of Labor Statistics report
estimated a more than 50 percent drop in Morse code operators between 1915 and
1931. Morse operators referred to teletypists on the sending side as
“punchers” and those on the receiving side as “printer men.”[^16] The printer
men responsible for assembling pages from ticker tape were called “pasters”
and sometimes, derisively, as “paperhangers.”[^17] Teletype technology
automated this entire process, rendering punchers, pasters, and paperhangers
obsolete. Operators could enter printed characters directly into the machine,
using a keyboard similar to the typewriter, which, by that time, was widely
available for business use. The teletype would then automatically transcode
the input into transmitted signal and then back from the signal onto paper on
the receiving end.

Machine code thus occupied a gray area between "plain text" and "cipher," both
legal as well as technical terms, governed by international treaties that
guaranteed the passage of plain text communications. According to conventions,
code was considered intelligible only when the schema for its decoding was
available to the transmitter.[^18] Without such keys, a transmitted message
was considered secret and therefore not subject to free passage across
national boundaries. In practice, the proliferation of encodings and machine
instructions had the effect of selective illiteracy, if not outright secrecy.
Telegraph operators used multiple cheat sheets to help decipher encoded
messages.

In this sense, telegraphy reintroduced a pre-Lutheran problem of legibility
into human letters. To speak in telegraph was to learn arcane encodings, not
spoken in the vernacular and requiring specialized training. A number of
failed communication schemas consequently attempted to bridge the rift between
human and machine alphabets.

“You must acknowledge that this is readable without special training,” Hymen
Goldberg wrote in the patent application for his 1911 Controller.[^19] The
device was made “to provide [a] mechanism operable by a control sheet which is
legible to every person having sufficient education to enable him to read.” In
an illustration attached to his patent, Goldberg pictured a “legible control
sheet [...] in which the control characters are in the form of the letters of
the ordinary English alphabet.”[^20] Goldberg’s perforations did the “double
duty” of carrying human-readable content and mechanically manipulating machine
“blocks,” “handles,” “terminal blades,” and “plungers.”[^21] Unlike other
schemas, messages in Goldberg’s alphabet could be “read without special
information,” effectively addressing the problem of code’s apparent
unintelligibility [Figure 3].[^22]

![Goldberg's control cards. Machine and human languages coincide on the same
surface. The perforations that actuate levers can also be read "without
special training," in contrast to other text encodings. Goldberg, Hyman Eli.
"Controller." US1165663 A, issued December 1915, sheet 3.](/image/figure-2.jpg)

The inscription remained visible at the surface of Goldberg’s control sheet,
as a perforated figure punched through the conduit. Whatever challenges punch
cards and ticker tape presented for readers, these were soon complicated by
the advent of  magnetic tape.

## 2. Magnetic Reader

The Goldberg Controller and its contemporary devices reveal a growing concern
with the comprehensibility of machine alphabets. However, one could hardly
call early programmable media ephemeral. Anecdotes circulate about Father
Roberto Busa, an early pioneer of computational philology, who in the 1960s
carted his punch cards around Italy in his truck.[^23] Codified inscription,
before its electromagnetic period, was fragile and unwieldy. Just like writing
with pen and paper, making an error on ticker tape entry required cumbersome
corrections and sometimes wholesale reentry of lines or pages. On the surface
of ticker tape, the inscription still made a strong commitment to the medium.
Once committed to paper, it was nearly immutable. Embossed onto ticker tape or
punched into the card, early software protruded through the medium.

Morse code and similar alphabet conventions left a visible mark on the paper.
They were at least legible if not always intelligible. At the 1967 Symposium
on Electronic Composition in Printing, Jon Haley, staff director of the
Congressional Joint Committee on Printing, still spoke of “compromises with
legibility [that] had been made for the sake of pure speed in composition and
dissemination of the end product.”[^24]

Magnetic tape changed the commitment between inscription and medium. It gave
inscription a temporary shelter, where it could dwell lightly and be
transformed, before finding its more permanent, final shape on paper. A new
breed of magnetic storage devices allowed for the manipulation of words in
“memory,” on a medium that was easily erased and rewritten.

The magnetic charge adhered lightly to the tape surface. This light touch gave
the word its newfound ephemeral quality. But it also made inscription
physically illegible. In applications such as law and banking, where fidelity
between input, storage, and output was crucial, the immediate illegibility of
magnetic storage posed a considerable engineering challenge. After the advent
of teletype but before cathode ray screens, machine makers used a variety of
techniques to restore a measure of congruence between invisible magnetic
inscription and its paper representation. What was entered had to be
continually verified against what was stored.

The principles of magnetic recording were developed by Oberlin Smith (among
others), an American engineer who also filed several patents for inventions
related to weaving looms. In 1888, inspired by Edison’s mechanical phonograph,
Smith made public his experiments with an “electrical method” of sound
recording using a “magnetized cord” (cotton mixed with hardened steel dust) as
a recording medium. These experiments were later put into practice by Valdemar
Poulsen of Denmark, who patented several influential designs for a magnetic
wire recorder.[^25]

Magnetic recording on wire or plastic tape offered several distinct advantages
over mechanical perforation. Tape was more durable than paper; it could fit
more information per square inch; and it was reusable. “One of the important
advantages of magnetic recording,” Marvin Camras, a physicist with the Armour
Research Foundation, wrote in 1948, “is that the record may be erased if
desired, and a new record made in its place.”[^26] Most early developments in
magnetic storage were aimed at sound recording. The use of magnetic medium for
data storage did not take off in earnest until the 1950s.[^27] However, early
developers of electromagnetic storage and recording technology already
imagined their work in dialog with the long history of letters (and not just
sound). In an address to the Franklin Institute on December 16, 1908, Charles
Fankhauser, the inventor of the electromagnetic telegraphone, spoke as
follows:

> To transport human speech over a distance of one thousand miles is a
> wonderful achievement. How much more wonderful, then, is the achievement
> that makes possible [...] its storage at the receiving end, so that the
> exact sentence, the exact intonation of the voice, the exact timbre, may be
> reproduced over and over again, an endless number of times.[^28]

Comparing magnetic recording to the invention of the Gutenberg press,
Fankhauser added:

> It is my belief that what type has been to the spoken word, the
telegraphone will be to the electrically transmitted word<4.>As printing
spread learning and civilization among the peoples of the earth and influenced
knowledge and intercourse among men, so I believe the telegraphone will
influence and spread electrical communication among men.[^29]

In that speech Fankhauser also lamented the evanescence of telegraph and
telephone communications. The telephone, he rued, fails to preserve “an
authentic record of conversation over the wire.”[^30] Fankhauser imagined his
telegraphone being used by

> the sick, the infirm, [and] the aged [...] A book can be read to the
sightless or the invalid by the machine, while the patient lies in bed.
Lectures, concerts, recitations--what one wishes, may be had at will. Skilled
readers or expert elocution teachers could be employed to read into the wires
entire libraries.[^31]

Anticipating the popularity of twenty-first-century audio formats like
podcasts and audiobooks, Fankhauser spoke of “tired and jaded” workers who
would “sooth [themselves] into a state of restfulness” by listening to their
favorite authors.[^32] Fankhauser saw his “electric writing” emerge as “clear”
and “distinct” as “writing by hand,” “an absolutely legal and conclusive
record.”[^33] Whereas written language was lossy and reductive, Fankhauser
hoped that electromagnetic signals would hold high fidelity to the original.

In 1909 Fankhauser thought of magnetic storage as primarily an audio format
that would combine the best of telegraphy and telephony. Magnetic data storage
technology did not mature until the 1950s, when advances in composite plastics
made it possible to manufacture tape that was cheaper and more durable than
its paper or cloth alternatives. The state-of-the-art relay calculator,
commissioned by the Bureau of Ordinance of the Navy Department in 1944 and
built by the Computation Laboratory at Harvard University in 1947, still made
use of standard-issue telegraph “tape readers and punchers” adapted for
computation with the aid of engineers from Western Union Telegraph
Company.[^34] It was equipped with a number of Teletype Model 12A tape readers
and Model 10B perforators, using 11/16-inch-wide paper tape, partitioned into
“five intelligence holes,” where each quantity entered for computation took up
thirteen lines of code.[^35] Readers and punchers were capable of running 600
operations per minute. Four Model 15 Page-Printers were needed to compare
printed characters with the digits stored on the ticker tape print register.
The numerical inscription in this setup was therefore already split between
input and output channels, with input stored on ticker tape and output
displayed in print.

The Mark III Calculator, which followed the Computation Laboratory’s earlier
efforts, was also commissioned by the Navy’s Bureau of Ordinance. It was
completed in 1950. Its organization or “floor plan” (“system architecture,” we
would say today) did away with punch cards and ticker tape, favoring instead
an array of large electromagnetic drums coupled with reel-to-reel tape
recorders. The drums, limited in their storage capacity, revolved at much
faster speeds than tape reels. They were used for fast, temporary internal
storage. A drum’s surface was coated with a “thin film composed of finely
divided magnetic oxides of iron suspended in a plastic lacquer, and applied to
the drums with an artist’s air brush.”[^36] A single Mark III calculator used
twenty-five such drums, rotating at 6,900 rpm, each capable of storing 240
binary digits.

In addition to the fast “internal storage” drums, the Mark III floor plan
included eight slow “external storage” tape-reader mechanisms. Tape was slower
than drums but also cheaper. It easily extended to multiple reels, thus
approaching the architecture of an ideal Turing machine, which called for tape
of “infinite length.”[^37] In practice, tape was in limited supply, long
enough to answer the needs of military computation. Unlike stationary drums,
tape was portable. Operators could prepare tape in advance, in a different
room, at the allotted instructional tape preparation table. The information on
tape would then be synced with and transferred to a slow drum.  In the next
stage the slow drum accelerated to match the higher rotating speeds of the
more rapid internal storage drums, and the information was transferred again
for computation. The Mark III Calculator was further equipped with five
printers “for presenting computed results in a form suitable for publication.”
The printers were capable of determining the “number of digits to be printed,
the intercolumnar and interlinear spacing, and other items related to the
typography of the printed page.”[^38]

In reflecting on these early “supercomputers,” one imagines the pathway of a
single character as it crosses surfaces, through doorways and interfaces,
gaining new shapes and temporalities with each transition.

Electromagnetic signals were transcoded into binary numerical notation. To
transfer characters onto tape, operators sat at the numerical tape preparation
table, yet another  separate piece of furniture. Data were stored along two
channels, running along the tape’s length. Operators entered each number
twice, first into channel A and then into channel B. This was done to prevent
errors, because the operators worked blindly, unable to see whether the
intended mark registered properly upon first entry. An error bell would sound
when the first quantity did not match the second, in which case the operator
would reenter the mismatched digits. To “ensure completely reliable results,”
one of the five attached Underwood electric teletypes could further be used to
print all channels and confirm input visually.[^39]

Before screes, the potential for incongruence between recondite data formats
and their apparent representation posed a significant problem. In a 1954
patent, filed on behalf of Burroughs Corporation, Herman Epstein and Frank
Innes described an “electrographic printer” involving an “electrical method
and apparatus for making electrostatic images on a dielectric surface by
electrical means which may be rendered permanently visible” [Figure 4].[^40]
The electrographic printer anticipated the modern photocopier in that it
proposed to use dusting inks to reveal the static charge. Rather than encoding
its data into another representation, such as the Baudot code, the printer
traced human-legible letter shapes directly onto tape. A small printing head
would convert binary input into a five-by-seven grid of electromagnetic
charges rendering the English alphabet. Such magnetic shapes  were then made
apparent by combining them with a “recording medium” that had the “correct
physical properties to adhere to the electrostatic latent images.”[^41] A
light dusting of powder ink would reveal the otherwise imperceptible magnetic
inscription.  Tape and paper configurations could thus achieve a measure of
literal analogy.

Advances in magnetic storage found their way into small businesses and home
offices a decade later. In 1964 IBM combined magnetic tape (MT) storage with
its Selectric line of electric typewriters (ST). Selectric typewriters were
popular because they were ubiquitous, relatively inexpensive, and could be
used to reliably transform a keyboard’s mechanical action into binary electric
signal. Consequently, they became a common input interface in a number of
early computing platforms.[^42] The MT/ST machine could be considered one of
the first personal “word processing” systems in that it combined
electromagnetic tape storage with keyboard input. Built on a simpler
architecture than its supercomputer cousins, the machine used a single tape
reading and writing mechanism. An advertisement in the American Bar
Association Journal, circa 1966, called it the $10,000 typewriter, “worth
every penny.” Where typists previously had to stop and erase every mistake,
the IBM MT/ST setup allowed them to “backspace, retype, and keep going.”
Mistakes could be corrected in place, on magnetic tape, “where all typing is
recorded and played back correctly at incredible speed.”[^43]

Despite its advantages, MT/ST architecture inherited the problem of legibility
from its predecessors. Information stored on tape was still invisible to the
typist. In addition to being encoded, electric alphabets were written in
magnetic domains and polarities, which lay beyond human sense.[^44] One
therefore had to verify input against stored quantities to ensure
correspondence. But the stored quantity could be checked only by transforming
it into yet another inscription. To verify what was stored the operator was
forced to redouble the original inscription, in a process that was prone to
error, because storage media could not be accessed directly without
specialized instruments.[^45]

Users of the Mark III Calculator were asked to input quantities several times
over. Another class of solutions involved making the magnetic mark more
apparent. For example, Youngquist and Hanes described their 1962 magnetic
reader as a

> device for visual observation of magnetic symbols recorded on a magnetic
recording medium in tape or sheet form. Magnetic recording tape is often
criticized because the recorded signals are invisible, and the criticism has
been strong enough to deny it certain important markets. For example, this has
been a major factor in hampering sales efforts at substituting magnetic
recording tape and card equipment for punched tape and card equipment which
presently is dominant in automatic digital data-handling systems. Although
magnetic recording devices are faster and more troublefree, potential
customers have often balked at losing the ability to check recorded
information visually. It has been suggested that the information be printed in
ink alongside the magnetic signals, but this vitiates major competitive
advantages of magnetic recording sheet material, e.g., ease in correction,
economy in reuse, simplicity of equipment, compactness of recorded data,
etc.[^46]

The magnetic reader consisted of two hinged plates [Figure 5]. Youngquist and
Hanes proposed to fill its covers with a transparent liquid that would host
“visible, weakly ferromagnetic crystals.” When sandwiched between the plates,
a piece of magnetic tape incited the crystal medium, which would in turn
reveal the signal’s “visibl[e] outline.”[^47]

!["Magnetic recording tape is often criticized because the recorded signals
are invisible." Youngquist and Hanes imagined a device that physically reveals
the magnetic inscription. Robert Yongquist and Robert Hanes, "Magnetic
Reader," Patent US3013206, 1961.](/image/figure-5.jpg)

Devices like the Magnetic Reader attempted (and failed) to address the
fundamental incongruence between paper and tape. Data plowed into rows on the
wide plains of a broad sheet had to be replanted along the length of a narrow
plastic groove. To aid in that transformation, the next crop of IBM Magnetic
Selectric typewriters added a composer control unit, designed to preserve some
of the formatting lost in transition between paper and plastic. It could
change margin size or justify text in memory. The original IBM Composer unit
justified text (its chief innovation over the typewriter) by asking the
operator to type each line twice: “one rough typing to determine what a line
would contain, and a second justified typing.”[^48] After the first typing, an
indicator mechanism calculated the variable spacing needed to achieve proper
paragraph justification. The formatting and content of each line thus required
separate input passes to achieve the desired result in print.

IBM’s next-generation Magnetic Tape Selectric Composer (MT/SC) build on the
success of its predecessors. It combined a Selectric keyboard, magnetic tape
storage, and  a “composer” format control unit. Rather than having the
operator type each line twice, the MT/SC system printed the entered text
twice: once on the input station printout, which showed both content and
control code in red ink, and a second time as the final Composer output
printout, which collapsed the layers into the final typeset copy. Output
operators still manually intervened to load paper, change font, and include
hyphens. The monolithic page unit was thereby further systematically
deconstructed into distinct strata of content and formatting.

Like other devices of its time, the IBM MT/SC suffered from the problem of
indiscernible storage. Error checking of input using multiple printouts was
aided by a control panel consisting of eleven display lights. The machine’s
manual suggested that the configuration of lights be used to peek at the
underlying data structure for verification.[^49]

In an attempt to achieve ever greater congruence between visible outputs and
data archived on a magnetic medium, IBM briefly explored the idea of storing
information on magnetic cards instead of tape. On tape, information had to be
arranged serially, into one long column of codes. Relative arrangement of
elements could be preserved, it was thought, on a rectangular magnetic card,
which  resembled paper in its proportions. The 1968 patent “Data Reading,
Recording, and Positioning System” described a method for arranging
information on a storage medium “which accurately positions each character
recorded relative to each previous character recorded.”[^50] In 1969, IBM
released a magnetic card-based version of its MT/ST line, dubbed the MC/ST.
Fredrick May, whose name often appears on word-processing-related patents from
this period, would later reflect that a “major reason for the choice of a
magnetic card for the recording of medium was the simple relationship that
could be maintained between a typed page and a recorded card.” The card
approximated a miniature page, making it a suitable “unit of record of storage
for a typed page.”[^51] Although it offered a measure of topographic analogy
between tape and paper, the “mag card” was short-lived partly because of its
limited storage capacity, capricious feeding mechanism, and its persistent
inscrutability.[^52]

The structure of textual artifacts--from a simple leaflet to a novel in
multiple volumes--has remained remarkably stable since the invention of
movable type. One rarely finds a sentence that spans several paragraphs, for
example. Nor would a contemporary reader expect to find pages of different
sizes in the same tome. Long-standing historical conventions guide the
production of printed text. Likewise, semantic and decorative units on a page
exist within a strict hierarchy. No book of serious nonfiction, for example,
would be typeset in a cursive font. Unless something out of the ordinary
attracts their attention, readers tend to gloss the inconsequential details of
formatting in favor of content. The material contexts of a well-designed book
fade from view during reading.

For a few decades after the advent of magnetic storage media but before the
arrival of screen technology, the sign’s outward shape disappeared altogether.
It is difficult to fathom now, but at that time---after the introduction of
magnetic tape in the 1960s but before the widespread advent of CRT displays in
the 1980s---typewriter operators and computer programmers manipulated text
blindly. Attributes such as indent size and justification were decided before
ink was committed to paper.

In the 1980s an engineer thus reflected on the 1964 MT/ST’s novelty: “It could
be emphasized for the first time that the typist could type at ‘rough draft’
speed, ‘backspace and strike over’ errors, and not worry about the pressure of
mistakes made at the end of the page.” The MT/SC further added a programmable
control unit to separate inputs from outputs. Final printing was then
accomplished by

> mounting the original tape and the correction tape, if any, on the
two-station reader output unit, setting the pitch, leading, impression control
and dead key space of the Composer unit to the desired values, and entering
set-up instructions on the console control panel (e.g., one-station or
two-station tape read, depending on whether a correction tape is present; line
count instructions for format control and space to be left for pictures, etc.;
special format instructions; and any required control codes known to have been
omitted from the input tape). During printing the operator changes type
elements when necessary, loads paper as required, and makes and enters
hyphenation decisions if justified copy is being printed.[^53]

The tape and control units thus intervened between keyboard and printed page.
The “final printing” combined “prepared copy,” “control and reference codes,”
and “printer output.”[^54] Historical documents often mention three distinct
human operators for each stage of production: one entering copy, one
specifying control code, and one handling paper output.

Researchers working on these early IBM machines considered the separation of
print into distinct strata a major contribution to the long history of
writing. One IBM consultant went so far as to place the MT/SC at the
culmination of a grand “evolution of composition,” which began with
handwriting and continued to wood engraving, movable type, and letterpress:
“The IBM Selectric Composer provides a new approach to the printing process in
this evolution.” He concluded by heralding the “IBM Composer era,” in which
people would once again write books “without the assistance of
specialists.”[^55] Inflationary marketing language aside, the separation of
the sign from its immediate material contexts and its new composite
constitution must be considered a major milestone in the history of writing
and textuality.

The move from paper to magnetic storage had tremendous social and political
consequences for the republic of letters. Magnetic media reduced the costs of
copying and dissemination of the word, freeing it, in a sense, from its more
durable material confines. The affordances of magnetic media---its very speed
and impermanence---created the illusion of light ephemerality. Yet the
material properties of magnetic tape itself continued to prevent direct access
to the site of inscription. Magnetic media created the conditions for a new
kind of illiteracy, which divided those who could read and write at the site
of storage from those who could only observe its aftereffects passively, at
the shimmering surface of archival projection.

The schematics I discuss in this section and above embody textual fissure in
practice. The path of a signal through the machine leads to multiplicity of
inscription sites. These are not metaphoric but literal localities that
stretch the sign across manifold surfaces. Whereas pens, typewriters, and hole
punches transfer inscription to paper directly, electromagnetic devices
compound them obliquely into a laminated aggregate. The propagation of
electric signal across space required and continues to require numerous phase
transitions between media: from one channel of tape to another, from tape to
drum, from a slow drum to a fast one, and from drum and tape to paper. On
paper the inscription remains visible in circulation; it disappears from view
on tape, soon after key press. Submerged beneath a facade of opaque oxide,
inscriptions thicken and stratify into laminates.

## 3. Display System

The contemporary textual condition took its present form in the late 1960s.
Computers subsequently changed in terms of size, speed, and ubiquity.
However, they retain the same essential architecture, consisting of
programmable media, electromagnetic storage, and screens.

The addition of a screen finally promised to address the problem of
electromagnetic legibility. In the first stage of its digital development,
language became “programmable.” Coupled with electromagnetic storage in the
second stage, programmable media was freed, to an extent, from its immutable
contexts. It was “lighter,” faster, more portable and therefore more itinerant
and malleable than print or punch. Ferric oxide became the preferred medium
for digital storage. This new memory layer lay also beyond the reach of human
senses. It was difficult to access and manipulate mentally.

Screens added a much needed window onto the memory abstraction. On-screen, the
topography of electromagnetic storage could be represented visually, obviating
the need for double entry or frequent printouts. Screens interjected to
mediate between input and output. They flattened the stratified complexity of
a laminate medium to facilitate use. Digital inscription remained invisible.
But the screen simulation could restore the appearance of a single surface.
As it did so it also obscured the dynamics of mediation.

On December 9, 1968, Douglas Engelbart, then the  primary investigator at the
NASA- and ARPA-funded Augmentation Research Center at the Stanford Research
Institute, gave what later became known as the “mother of all demos” to an
audience of roughly 1,000 or so computer professionals attending the Joint
Computer Conference in San Francisco. The demo announced the arrival of almost
every technology prophesied by Vannevar Bush in his influential 1945 Atlantic
essay, “As We May Think.” During his short lecture, Engelbart presented
functional prototypes of the following: graphical user interfaces, video
conferencing, remote camera monitoring, links and hypertext, version control,
text search, image manipulation, windows-based user interfaces, digital
slides, networked machines, mouse, stylus, and joystick inputs, and “what you
see is what you get” (WYSIWYG) word processing.[^57]

In his report to NASA, Engelbart described his colleagues as a group of
scientists “developing an experimental laboratory around an interactive,
multiconsole computer-display system” and “working to learn the principles by
which interactive computer aids can augment the intellectual capability of the
subjects.”[^58] CRT displays were central to this research mission. In one of
many patents that came out of his “intellect augmentation” laboratory,
Engelbart pictured his “display system” as a workstation that combines a
typewriter, a CRT screen, and a mouse. The schematics show the workstation in
action, with the words “now is the time fob” prominently displayed on-screen.
The user was evidently in the process of editing a sentence, likely to correct
the nonsensical “fob” into “for” [Figure 4].[^59]

![Schematics for Engelbart's "Display System." The arrangement of keyboard,
mouse, and screen will define an epoch of human-computer interaction. Source:
Douglas Engelbart, "X-Y Position Indicator for a Display System," Patent
US3541541, 1970.](/image/fob.jpg)

Reflecting on the use of visual display systems for human-computer
interaction, Engelbart wrote, “One of the potentially most promising means for
delivering and receiving information to and from digital computers involves
the display of computer outputs as visual representations on a cathode ray
tube and the alteration of the display by human operator in order to deliver
instructions to the computer.”[^60] The first subjects to read and write
on-screen reported feeling freedom and liberation from paper. An anonymous
account included in Engelbart’s report offered the following self-assessment:

```
1B2B1 To accommodate and preserve a thought or
piece of information that isn't related to the work
of the moment, one can very quickly and easily
insert a note within the structure of a file at such
a place that it will neither get in the way nor get
lost.

1B2B2 Later, working in another part of the file,
he can almost instantly (e.g. within two seconds)
return to the place where he temporarily is storing
such notes, to modify or add to any of them.

1B2B3 As any such miscellaneous thought develops,
it is easy (and delightful) to reshape the structure
and content of its discussion material.[80]
```

Writing, which this typist previously perceived as an ordered and continuous
activity, subsequently was performed in a more disjointed way. The typist
could delight in shaping paragraphs that more closely matched her mental
activity. Screens thus restored some of the fluidity of writing that
typewriters denied. Writers could, for example, pursue two thoughts at the
same time, documenting both at different parts of the file as one would in a
notebook. Not constrained by the rigidity of a linear mechanism, they moved
around the document at will.

Engelbart recorded what must count as some of the most evocative passages to
appear in a NASA technical report. His “Results and Discussion” section
contains the following contemplation by an anonymous typist:

1B4 I find that I can express myself better, if I can make all the little
changes and experiments with wording and structure as they occur to me. [Here
the user experiments a little with using structural decomposition of a complex
sentence.][81]

A decomposition follows indeed. The author deviates dramatically from
technical writing conventions. Numbered passages along with unexpected
enjambment heighten the staccato quality of prose, which attains an almost
lyrical quality:

1B4A I find that I write faster and more freely,
1B4A1 pouring thoughts and trial words onto the
screen with much less inhibition,

1B4A2 finding it easy to repair mistakes or wrong
choices

1B4A2A so while capturing a thought I don’t
have to inhibit the outpouring of thought and
action to do it with particular correctness,

1B4A3 finding that several trials at the right
wording can be done very quickly

1B4A3A so I can experiment, easily take a look
and see how a new version strikes me--and often
the first unworried attempt at a way to express
something turns out to be satisfactory, or at
least to require only minor touch up.

1B4A4 Finding that where I might otherwise
hesitate in search of the right word, I now pour out
a succession of potentially appropriate words,
leaving them all there while the rest of the
statement takes shape. Then I select from among
them, or replace them all, or else merely change the
list a bit and wait for a later movement of the
spirit.[82]

 
When input and output coincide in time, as they do on paper, mistakes are
costly. Once inscribed, the sign gains permanence; it is difficult to emend.
An eraser can help remove a layer of physical material. Alternatively, writers
use white ink to restore the writing surface. Engelbart’s anonymous typist
reports the feeling of freedom from such physical commitment. She can simply
backspace and start over. Words come easily because there are no penalties for
being wrong. Virtual space seems limitless and endlessly pliable.

The feeling of material transcendence--the ephemeral quality of digital
text--is tied directly to the underlying physical affordances  of
electromagnetic storage. Screens expose the pliability of the medium, where
erasure is effortless. Content can be addressed in memory and copied at the
stroke of a key. The numbered paragraphs suggest a novel system for
recollection. Data storage units become, in a sense, mental units. I am struck
by the distinctly phenomenological quality of technical description: The
editor does not merely resemble a page; it is, for the writer, a newly
discovered way of thought that changes the writer’s relation not only to text
but also to her own thoughts. The highly hierarchical and blocky paragraph
structure, along with its repetitive refrain (“finding” and “I find that”),
gives the prose a hypnotic drive forward. The cadence matches the reported
experience of discovery.

The writer continues:

```
1B4B I find that

1B4B1 being much more aware of

1B4B1A the relationships among the phrases of a
sentence,

1B4B1B among the statements of a list,

1B4B1C and among the various level and members
of a branch,

1B4B2 being able

1B4B2A to view them in different ways,

1B4B2B to rearrange them easily,

1B4B2C to experiment with certain special
portrayals,

1B4B2C1 not available easily in unstructured
data

1B4B2C2 or usable without the CRT display,

1B4B3 and being aware that

1B4B3A I can (and am seeking to) develop still
further special conventions and computer aids

1B4B3B to make even more of this available and
easy,

1B4B4 all tend to increase

1B4B4A my interest and experimentation

1B4B4B and my conviction that this is but a
peek at what is to come soon.[83]
```

The passages appear too contrived to be spontaneous. Despite its experimental
structure, these phenomenological reflections advance key elements of
Engelbart’s research program, which aimed to develop new data structures in
combination with new ways of displaying them. Yet I cannot help but be moved
by the fluency of the prose and by the sheer audacity of the project.

Engelbart’s research into intellect augmentation created tools that
augment research. In an image that evokes Baron Münchhausen pulling himself
out of a swamp by his own bootstraps, Engelbart called his group’s methodology
“bootstrapping,” which involved the recursive strategy of “developing tools
and techniques” to develop better tools and techniques.[84] The “tangible
product” of such an activity was a “constantly improving augmentation system
for use in developing and studying augmentation systems.”[85]

It was an appealing vision, but only so long as it remained recursive.
Engelbart’s group benefited from creating their own tools and methods.
Engelbart also hoped that his system could be “transferred--as a whole or by
pieces of concept, principle and technique--to help others develop
augmentation systems for many other disciplines and activities.”[86]
Undoubtedly, Engelbart’s ideas about intellect augmentation have had a broad
effect on knowledge work across disciplines. However, his vision loses the
property of self-determination when transferred outside the narrow confines of
a laboratory actively engaged in the transformation of material contexts of
their own knowledge production. Word processing today rarely involves
communities pulling themselves up by their own bootstraps: using their tools
and techniques of their own design. Augmentation enforced from without often
advances values and principles no longer comprehensible to the entity being
augmented.

To bring his system into being, Engelbart convened a community that through
recursive self-improvement could lift itself up toward a smarter, more
efficient, more human way of doing research. The group crafted novel
instruments for reading and writing. They engineered new programming
languages, compilers to interpret them, and debuggers to troubleshoot them.
The system shows care and love for the craft of writing. But there is also
complexity. “This complexity has grown more than expected,” Engelbart wrote in
conclusion.[87] The feeling of transcendence that the anonymous typist
describes in using the system engages a sophisticated mechanism. The mechanism
was not, however, the primary instrument of augmentation. Rather, it was the
process of designing, making, and experimenting with tools that enhanced the
intellect.. Engelbart wrote, “The development of the Bootstrap Community must
be coordinated with the capacity of our consoles, computer service, and file
storage to support Community needs, and with our ability to integrate and
coordinate people and activities.”[88] In other words, the development of the
community must form a feedback loop with software development. It involves
training, practice, critical self-reflection, and thoughtful deliberation.

Modern word processors enable us to drag and drop passages with unprecedented
facility. We live in Engelbart’s world, to the extent that we use his lab’s
complex systems daily and in a smaller configuration: screens, keyboards,
storage. Today’s computer users rarely form a self-determined bootstrapping
community, however. The contemporary writer is bootstrapped passively to the
prevailing vision of intellect augmentation. The very metaphor of
bootstrapping suggests the impossibility of using one’s bootstraps to pull
others out of the Platonic cave. Engelbart’s liberatory research program
therefore left another less lofty imprint on the everyday practice of modern
intellectual life. Text, which before the advent of the CRT was readily
apparent on the page in all its fullness, finally entered a complex system of
executable code and inscrutable control instruction. The material lightness of
textual being came at the price of legibility.

Short-lived screenless word processors of the early 1960s (e.g., the MT/ST)
were difficult to operate, because typists had no means to visualize complex
data structures on tape. Screens helped by representing document topography
visually, restoring a sense of apparent space to otherwise opaque media. The
contemporary digital document may resemble a page on-screen, but beneath it,
it is a jumble of bits, split into the various regions of internal memory.
Screens simulate document unity by presenting holistic images of paragraphs,
pages, and books. The simulation seems to follow the physics of paper and ink:
One can turn pages, write in margins, and insert bookmarks. But the underlying
inscription remains in fracture. Simulated text does not transcend matter.
Screens merely conceal its material properties while recreating others, more
seemingly transcendent ones. The act of continual dissemblage, one medium
imitating the other, manufactures an ephemeral illusion by which pages fade in
and out of sight, paper folds in improbable ways, and words glide effortlessly
between registers of copy and paste.

In the rift between  input and output, programmable media inject arbitrary
intervals of time and space. Forces of capital and control occupy the void as
the sign acquires new dimensions and capabilities for automation. Code and
codex subsequently sink beneath the matte surface of a synthetic storage
medium. Screens purport to restore a sense of lost immediacy, of the kind felt
on contact between pen nib and paper as the capillary action of cellulose
conveys ink into its shallow conduit.

Screens are meant to open a window onto the unfamiliar physicalities of
electromagnetic inscription. For example, they obviate the need for multiple
typings or printouts. Projected image should, in theory, correspond to its
originating keystroke. The gap separating inputs and outputs appears to close.
Crucially, the accord between archived inscription and its image cannot be
guaranteed. The interval persists in practice and is actively contested. Deep
and shallow inscriptions entwine. Laminate text seems weightless and ephemeral
at some layers of the composite, allowing for rapid remediation. At other
layers its affordances are determined by its physics; at still other layers
they are carefully constructed to resist movement or interpretation. Alienated
from the base particulates of the word, we lose some of our basic interpretive
capacities to interrogate embedded power structures.

[^1]: Chun, "Enduring Ephemeral," 148.

[^2]: See Levine, *Forms*, 6-11. Levine explains: "Affordance is a term used
to describe the potential uses or actions latent in materials and designs.
Glass affords transparency and brittleness. Steel affords strength,
smoothness, hardness and durability. Cotton affords fluffiness, but also
breathable cloth when it is spun into yarn and thread" (6).

[^3]: The situation is more complicated for pre-modern texts, where the
materiality of print cannot be taken for granted.

[^4]: See also Gadamer, *Truth and Method*, 110: "In both legal and
theological hermeneutic there is an essential tension between the fixed
text---the law or the gospel---on the one hand and, on the other, the sense
arrived at by applying it at the concrete moment of interpretation."

[^5]: Ricoeur, *Interpretation Theory*, 28.

[^6]: See, for example, Huhtamo and Parikka's "Introduction" in *Media
Archaeology*, 3: "Media Archaeology should not be confused with archaeology as
a discipline. When media archaeologists claim they are 'excavating'
media-cultural phenomena, the word should be understood in a specific way."
The use of quotes signals the metaphoric nature of the borrowing. See also
Grant Wythoff, "Artifactual Interpretation"  Wythoff writes: "[H]ow do we
close the metaphorical divide between the 'excavations' performed in
archaeology and media archaeology?" (27). On the use of stratigraphy related to
hard drive forensics see Perry and Morgan, "Materializing Media Archaeologies:
the MAD-P Hard Drive Excavation."

[^7]: See Schenck, "Applied Paleontology"; Simonetti, "Between the Vertical
and the Horizontal: Time and Space in Archaeology"; and Geikie, "The Rise of
Stratigraphical Geology in England" in *The Founders of Geology*, 337-364.

[^8]: For periodization of computer systems see Denning, "Third Generation
Computer Systems."

[^9]: Programmable media have multiple origins, worthy of their own extended
history. The French textile worker Basile Bouchon used "drill paper" to
automate industrial drawlooms. The invention of the loom could also be
attributed to the Banu Musa brothers, ninth-century automata inventors from
Baghdad; to Jacques de Vaucanson, who delighted the public with his lifelike
mechanisms in the mid-eighteenth century; or to Joseph Charles Marie Jacquard,
who improved on and popularized Bouchon’s looms on an industrial scale around
the same time. See Koetsier, "Prehistory of Programmable Machines," 593-95;
Randell et al., "History of Digital Computers"; and Riskin, "Defecating Duck."

[^10]: Adler and Albertman, “Knitting Machine”; Casper, “Remote Control
Advertising”; Hough, “Wired Radio Program Apparatus.”

[^11]: Jennings, “Annotated History.”

[^12]: The Australian Donald Murray improved on the Baudot system to minimize
the amount of holes needing to be punched, allotting fewer perforations to
common English letters. See ^Murray, “Setting Type,” 567.

[^13]: Twenty-eight measures indicate the numerical “figure space” and
twenty-six indicate double quotes, which share the encoding length with the
letter z.

[^14]: Beauchamp, History of Telegraphy, 380-97; Murray, “Setting Type.”

[^15]: Murray, "Setting Type," 557.

[^16]: According to the U.S. Bureau of Labor Statistics, women made up 24
percent of the Morse operators in 1915 (before the widespread advent of
automated telegraphy). By 1931 women made up 64 percent of printer and Morse
manual operators. U.S. Bureau of Labor Statistics, “Displacement of Morse
Operators,” 514.

[^17]: Brackbill, “Some Telegraphers’ Terms,” 290.

[^18]: International Telegraph Union, "Telegraph Regulations," 12-13.

[^19]: Goldberg, “Controller,” , sheet 3.

[^20]: Goldberg, “Controller,” 1.

[^21]: Goldberg, Controller, 1-4.

[^22]: Goldberg, “Controller,” 1.

[^23]: For example, Susan Hockey says, “Father Busa has stories of truckloads
of punched cards being transported from one center to another in Italy”
(^Hockey, “History of Humanities Computing,” n.p.).

[^24]: Lee and Worral, Electronic Composition, 48.

[^25]: Daniel et al., Magnetic Recording; Engel, “1888<->1988”; Poulsen,
“Method of Recording”; O. Smith, “Some Possible Forms”; Thiele, “Magnetic
Sound Recording”; Vasic and Kurtas, Coding and Signal Processing.

[^26]: Camras, “Magnetic Recording Tapes,” 505.

[^27]: Dee, “Magnetic Tape,” 1775.

[^28]: Fankhauser, “Telegraphone,” 37-38.

[^29]: Fankhauser, “Telegraphone,” 40.

[^30]: Fankhauser, “Telegraphone,” 39-40.

[^31]: Fankhauser, “Telegraphone,” 44.

[^32]: Fankhauser, “Telegraphone,” 45.

[^33]: Fankhauser, “Telegraphone,” 41.

[^34]: The staff of the Computation Laboratory of Harvard University wrote:
“Two means are available for preparing the functional tapes required for the
operation of the interpolators. First, when the tabular values of f(x) have
been previously published, they may be copied on the keys of the functional
tape preparation unit<3.>and the tape produced by the punches associated with
this unit, under manual control. Second, as suitable control tape may be coded
directing the calculator to compute the values of f(x) and record them by
means of one of the four output punches, mounted on the right wing of the
machine” (Computation Laboratory, Relay Calculator, 33).

[^35]: Computation Laboratory, Relay Calculator, 30.

[^36]: Computation Laboratory, Magnetic Drum Calculator, 1.

[^37]: Turing, "Computing Machinery and Intelligence," 444.

[^38]: Computation Laboratory, Magnetic Drum Calculator, 34-35.

[^39]: Computation Laboratory, Magnetic Drum Calculator, 35, 143-88.

[^40]: Epstein and Innes, “Electrographic Printer,” 1.

[^41]: Epstein and Innes, “Electrographic Printer,” 2.

[^42]: Eisenberg, “Word Processing.”

[^43]: ABA Journal,“The $10,000 typewriter.”.

[^44]: See ^Ohmori et al., “Memory Element”; and ^Stefanita, Magnetism, 1-69.

[^45]: Recall Wittgenstein’s broken reading machines, which exhibited a
similarly recursive problem of verification. To check whether someone
understood a message, one has to resort to another message, and so on.

[^46]: Youngquist and Hanes, “Magnetic Reader,” 1.

[^47]: Youngquist and Hanes, “Magnetic Reader,” 1.

[^48]: Morgan and Norwood, “IBM Selectric Composer,” 69.

[^49]: Bishop et al., “Development.”

[^50]: Clancy et al., “Data Reading,” 1.

[^51]: May, “IBM Word Processing Developments,” 743.

[^52]: May, “IBM Word Processing Developments,” 743.

[^53]: Bishop et al., “Development,” 382.

[^54]: Bishop et al., “Development,” 382. See also May, “IBM Word Processing
Developments.”

[^55]: Frutiger, “IBM Selectric Composer,” 10.

[^56]: Rogers, “The Demo”; Tweney, “Mother of All Demos.”

[^57]: Engelbart, “Doug Engelbart 1968 Demo.”

[^58]: Engelbart, Human Intellect Augmentation Techniques, 1.

[^59]: The source of the cryptic phrase is likely Charles Edward Weller: “We
were then in the midst of an exciting political campaign, and it was then for
the first time that the well known sentence was inaugurated--‘Now is the time
for all good men to come to the aid of the party’; also the opening sentence
of the Declaration of Independence, [...] which sentences were repeated many
times in order to test the speed of the machine” (Weller, Early History of the
Typewriter, 21, 30).

[^60]: Weller, Early History of the Typewriter, 1.

[^61]: Engelbart, Human Intellect Augmentation Techniques, 48-49. I have
reproduced the text verbatim, preserving the line breaks, because formatting
is an important part of the reported experience.

[^62]: Engelbart, Human Intellect Augmentation Techniques, 50.

[^63]: Engelbart, Human Intellect Augmentation Techniques, 50-51.

[^64]: Engelbart, Human Intellect Augmentation Techniques, 51.

[^65]: Engelbart and English, “Research Center,” 396.

[^66]: Engelbart, Human Intellect Augmentation Techniques, 6.

[^67]: Engelbart, Human Intellect Augmentation Techniques, 6.

[^68]: Engelbart, Human Intellect Augmentation Techniques, 67.

[^69]: Engelbart, Human Intellect Augmentation Techniques, 67.
