Theme and Argument
------------------

This book is about the strange entanglement between humans, texts, and
machines. It examines key literary theoretical ideas alongside the
intellectual history of software engineering that frame contemporary
reading, writing, and interpretation practices. It is a deeply
materialist work, in which I argue that even our most ingrained
intuitions about texts are profoundly alienated from the physical
contexts of intellectual production. A new kind of poetics is therefore
necessary to preserve the free play of ideas implicit in the method of
humanistic inquiry. Often, the work of literary theory defines itself in
terms of specific texts, as a series of readings. My object of study is
instead the nature of textuality itself. I am interested here in how
texts are produced; in the metaphors that guide computation; in the
forms, formulae, and formats that structure human-computer interaction;
in the literary device; and in the strange shape of contemporary
inscription, which, no longer a single mark on paper, stretches between
the site of fleeting projection, the screen, and the site of storage in
its solid, archival state. In quite another sense, this book is about
more than words, screens, and archives: it reasons through the dynamics
of technological settlement and displacement, through the comforts and
the discomforts of inhabiting our numerous extensions of self, our media
homes.

My aim in this book is to dispel a pervasive illusion. The electronic
book draws a compelling figure that belies material realities of reading
and writing, transformed by the advent of computation. The device on my
desk is not a book, but a simulation of a book. And everything
associated with reading this metaphor must in itself be understood under
the sign of simulation.[^1] What kind of a metaphor is it? How did it
come into being and how does it affect practices of literary
interpretation? In *Plain Text* I attempt to come to terms with the
conditions of *simulated textuality*.

"Software's ghostly presence produces and defies apprehension," Wendy
Chun wrote in her *Programmed Visions* [@chun_programmed_2011, 3]. But
what happens when all text is a type of software? Friedrich Kittler ends
his book on a similar note: in his vision, literature has finally been
defeated by technologies of military-grade encryption, secrecy, and
obfuscation [@kittler_gramophone_1999, 263]. Attempts to silence print
through book burning or censorship are viscerally obvious and met with
nearly universal disapproval. Unlike censorship or the burning of books,
computational governance proceeds by clandestine means. The
simulation-producing nature of computed text preserves the outward
appearance of printed text, while concealing the specifics of governance
and control. I mean governance and control in the sense of shaping
affordances: a mode of physical regulation that structures the
production, access, and the distribution of knowledge. The challenge of
*Plain Text* will be in the description of such emerging but often
occluded technological possibilities.

A concern with the material conditions of simulated textuality leads us
to a rich archive of materials from the history of literary theory,
semiotics, telegraphy, and electrical engineering from the middle of the
nineteenth to the end of the twentieth centuries. Reflecting on the
development of Morse Code in 1949 in the *Proceedings of the American
Philosophical Society*, Frank Halstead mentioned the difficulty of
finding a home in either the arts or sciences for what he calls "code
development." He wrote, "it is a matter somewhat related to the general
art of cryptology, yet it is not wholly divorced from electrical
engineering nor from general philology" [@halstead_genesis_1949, 456].
As Halstead anticipated, research in the field of code development and
computational culture has led me to a range of primary materials: from
the proceedings of the Association for Computing Machinery (ACM) to the
US Patent and Trademark Office; from Bell Labs to early Soviet
publishing houses that heralded the advance of formalism; from studies
on animal communication behavior, to Unix manuals, to textbooks on
semiotics, and to foundational texts in the philosophy of aesthetics and
literary theory.

I deploy the archive to argue that theories of interpretation evolved
under the conditions tied to static print media. By contrast, electronic
text changes dynamically to suit its reader, political context, and
geography. Consequently, I argue for the development of what I term
*computational poetics*: a strategy of interpretation capable of
reaching past the surface content to reveal the software platforms and
the hardware infrastructures that contribute to the production of
meaning. Drawing on a range of materials at the intersection of literary
thought and the history of modern computing, *Plain Text* examines a
number of key literary-theoretical constructs, recasting the electronic
book as a computational object.

I appeal to the idea of "plain text" in the title of this book to signal
an affinity with a particular mode of computational meaning-making.
Plain text identifies a file format and a frame of mind. As a file
format, it contains nothing but a "pure sequence of character codes."
Plain text stands in opposition to "fancy text," "text representation
consisting of plain text plus added information"
[@unicode_consortium_unicode_1990]. In the tradition of American textual
criticism, "plain text" alludes to an editorial method of text
transcription which is both "faithful to the source" and is "easier to
read than the original document" [@cook_time-bounded_1972]. Combining
these two traditions, I mean ultimately to build a case for a kind of a
systematic minimalism when it comes to our use of computers---a
minimalism that privileges access to source materials, ensuring
legibility and comprehension. I do so in contrast with other available
modes of human--computer interaction, which instead privilege maximizing
system--centric ideals like efficiency, speed, performance, or security.

My use of plain text implies also a poetics of reading and writing. The
title therefore further identifies an interpretive stance one can assume
in relation to the making and the unmaking of literary artifacts.
Besides visible content, all contemporary documents carry with them a
layer of hidden information. Originally used for typesetting, that layer
affects more than innocuous document attributes like "font size" or
"line spacing." Increasingly, devices that mediate literary activity
encode forms of governance. These tacit structures police intellectual
property laws; they censor and carry out surveillance operations. For
example, the Digital Millennium Copyright act, passed in the United
States in 1996, goes beyond written injunction to require in some cases
the management of digital rights (DRM) at the level of hardware. An
electronic book governed by DRM may subsequently prevent the reader from
copying or sharing stored content, even for the purposes of academic
study.\[\^ln-dmca\] Building on the recent work of scholars like Lev
Manovich, Tung-Hui Hu, and Lisa Gitelman I make the case for an
empowered *computational poetics*, a method of inquiry that aims to
bring implicit control structures once again under the purview of
interpretation and critique
[@manovich_there_2011; @gitelman_paper_2014; @hu_prehistory_2015].

My notion of poetics builds also on the long history of literary theory,
combined here with practices borrowed from software engineering and
computer science. Poetics, in the way I am using it here, concerns
itself simply with the construction of literary media artifacts. These
are not however limited to the canonical, straight-ahead structuralisms
of Roman Jakobson or Jonathan Culler. I am borrowing rather from a more
peripheral tradition represented best for me by third culture thinkers
like Viktor Shklovsky and Vilém Flusser, consummate immigrants both, who
extracted a literary and a media theory respectively out of the fabric
of their emigration. The purpose of this introduction is in part to
recreate for the reader the winding path that anticipated the making of
this book. It passes first through a kind of a productive tension
between estrangement and habituation, which although stemming from a
personal experience of geographic displacement, leads to a dialectics,
that is, a method of making sense out of conflicting and often
contradictory logics of the virtual. In the section that follows, I
outline the some of the methodological considerations that motivate my
approach to the study of media in general and text in particular.
Finally, I would like to draw attention to the plant of the present
work.

Annotated Table of Contents
---------------------------

The tangled pathways of inscription winding its way through the device
exist in relation to distinct communities of computational practice. A
researcher cannot therefore expect to discover a single theoretical
framework that captures the complexity of simulated text in motion. The
inscription goes by one name in one part of the system and by another
elsewhere. What counts for "code" and "poetry" in one domain, like
computer science, may not account for the same in another domain, like
creative writing. An engineer's use of the words "code" and "poetry"
differs from that of a poet's. The changing contexts evoke the
corresponding shift in operational definitions. Consequently, this book
is neither a totalizing history of modern computing nor a survey of
literary theory. Rather, the argument therein progresses from the action
of the alphanumerical keyboard switch, through copper and silicon, to
liquid crystal and the floating gate, and on towards the reader and the
community. It is but one of many possible passes through a cavernous
black box.

The passage from keystroke to pixel gives the book its shape. In the
chapters to follow, our mobile phones and laptops will come fully into
view as metaphor machines engendering ubiquitous simulation. The first
three chapters are thus concerned with the structure of the
computational metaphor. The **first chapter** begins with an
explication. What does it mean to turn a page, I ask, when neither the
page nor the action of turning correspond to their implied analogies?
The analysis of the metaphor helps trace the intellectual history of
human--computer interaction, a field which progressed from
"conversational programming" to the "direct manipulation" paradigm
shaped by cognitive metaphor theory and immersive theater. The logic of
"directness" leads to the rapidly developing field of brain-to-computer
interfaces. The chapter concludes with a moment of speculative
formalism, in which we consider the possibility of affective literature
that eschews language and representation.

At the core of the book's **second chapter** lies the notion of a
modernist literary device, understood both as literary technique and a
thought experiment about intelligent machines, directly connected to the
birth of modern computing. A section on literary technique in the
thought of Percy Lubbock, Walter Benjamin, and Mikhail Bakhtin opens the
discussion. Materialist poetics arise concomitantly with a mechanistic,
rule-based view of language leading to a series of thought experiments
first in the writing of Ludwig Wittgenstein, and then in the seminal
paper of Alan Turing on an imaginary computer capable of reading and
writing. The verbs to read and to write imply a type of cognitive
processing. What does it mean to read and to write for a machine? What
about broken mechanisms of comprehension? At once a device and an
algorithm, the Turing machine blurs the boundaries between software and
hardware, code and content, intelligence and its imitation.

Two rich intellectual histories collide on the pages of the **third
chapter** chapter: one, the material history of formatting as a concept
in computer science and the other, the intellectual history of form in
literary theory. Format emerges as a concept that mediates between form
understood as internal "rules for construction" and form understood as
"external shape." The formatting layer transforms one type of structure,
a series of bits arranged into tracks and sectors, into another, letters
arranged into sentences and paragraphs. I draw a short history of text
formats that commences with several "control characters" limited in
function to actions like "carriage return" or "stop transmission." With
time, the formatting layer encompassed all manner of machine
instruction, including structures of governance like "digital rights
management" and "copy protection." A manufacturer's ability to censor or
to surveil electronic books is contained within the formatting layer.
The concept of formatting developed in this chapter is critical
therefore to our understanding the capabilities of digital texts: from
electronic books that modify themselves to suit the reader's geographic
location to "smart" contracts that contain the rules of their own
execution.

What does it mean to read on page that is no longer flat and no longer a
single surface? The book's second section, containing the next three
chapters, describes digital text as a sign stretched between storage
media and the screen, suspended between surfaces (also the title of the
section).

The **fourth chapter** begins with a discussion of an apparent paradox.
A camp of media theorists and textual scholars in the 1990s conceived of
electronic texts as an ephemeral, almost immaterial, phenomenon. The
text shimmered and glared: it was spoken of in terms of *hypertext*,
light writing, and electricity. A generation of theorists that came
after insisted on the weighty materiality of electronic media. Reading
began to engage the morphology of rare metals, media archeology, hard
drive forensics. Both accounts, I argue, capture an aspect of the same
underlying conditions. The perceived image of an archived inscription
splits from its source. The sign plausibly resides both on the screen
and on the hard drive. It splits, in some real a sense, diverging at the
site its projection from the site of the archive. Erasing an inscription
on the screen, for example, may not elicit the corresponding action on
the disk. Using archival materials from the history of telegraphy in the
late nineteenth and early twentieth centuries, I chart the gradual
fracture and the ultimate illegibility of the computational sign. Early
computers stored human-readable text and machine instruction at the
surface of the same storage media like punch cards and ticker tape.
Although difficult to read, these forms of machine writing were readily
visible and therefore amenable to analysis. The advent of magnetic
storage forced the composite inscription into an opaque medium. Unable
to perceive magnetic polarities without the aid of a machine, readers
often manipulated text blindly. In this way a typist would type several
sentences without seeing the printed output. The chapter identifies a
milestone in the history of human textuality: the moment at which the
inscription passes from view, giving rise to the sometimes conflicting
but nevertheless consistent accounts of digital textuality.

The **fifth chapter** charts the emergence of screen reading. The screen
appears to restore a measure of visibility lost to magnetic inscription,
with one major side-effect. Fidelity between the word visible and the
word archived cannot be guaranteed. What the screen shows and what is
stored on tape or hard drive has only a contingent correlation. Screen
reading further happens on screens that refresh themselves at a rate of
around 60 cycles per second (Hertz). The digital word is technically an
animation; it moves even as it appears to stand still. This property of
the medium attunes the reader to a particular mode of apprehension,
affecting not just the physics but also the aesthetics of digital media.
Works by the philosophers Henri Bergson, Jakob von Uexküll, and John
Goodman help construct a phenomenology of screen-based digital
perception. The digital emerges ultimately not as a property of the
medium, but as structure imposed onto matter from without. In the
extreme, that means that a censored *electronic* text can form a
perfectly *analog* artifact, despite being digital in all other senses
of the word. Conversely, texts in print are already "born digital," in
the sense that literary works like Shakespeare's *Hamlet* are amenable
to "reliable processes of copying and preservation"
[@haugeland_analog_1981, 213-225]. Properties that make media "digital"
or "analog" reveal themselves neither to be neither universal nor
essential to the medium. The medium is not the message. "The reliability
and preservation of textual copies" may mean one thing to a literary
scholar, another to a software engineer or a legal professional, and
something entirely different to a librarian, as I argue in the
conclusion of the chapter. It matters not what the medium is, but what
we can do with the text.

The **sixth and final chapter** looks to the site of storage to find the
media "homes" that house the vast archives of our private media
collections. It begins with a close reading of Beckett's *Krapp's Last
Tape*. Krapp makes yearly audio recordings of himself, only to revisit
them and to enter into a sort of dialog with his own voice from the
past. I posit this archival encounter as Krapp's "media being" and
suggest that such encounters are commonplace. Writers and book
collectors regularly deposit "snapshots" of their consciousness into
files, bookshelves, and folders. Jean-Paul Sartre's idea of an
"appointment with oneself" helps to reveal this external construction of
files, folders, and library furnishings as cognitive extension, in need
of delicate pruning and arrangement. In this light, I show that
documents exist not as completed works, but as "vectors" that mutate and
move through time and space.\[\^ln-wark\] I ask: What is being
externalized, communicated, and preserved? And answer: It is not simply
a message, but the subject itself. A close reading of the "home" folder,
the default location of personal files on many systems, concludes with
the discussion of media homes. Finally, I return to the theme of
displacement, arguing for a mode of inhabitance within media that is
uncanny or un-homed \[*Unheimliche*\], contrary to discourse that speaks
in terms of "digital natives" and those who are "born digital." I build
on Flusser's immigrant poetics to suggest a kind of information
processing that necessitates a purposeful movement between the
polarities of settlement and expatriation.

Field Significance
------------------

The book seeks to redress a weakness in the field of digital humanities,
particularly at the point of its relevance to literary studies.
Scholarship at the intersection of these two fields is sometimes
criticized for being ahistorical or atheoretical, abandoning deep
traditions of literary theory and criticism, even where such traditions
would help bolster the case for the digital humanities. The nominally
related field of new media studies has the opposite problem. Although
theoretically sophisticated, it sometimes produces research far removed
from the actual practice of creating new media (the archetypal example
given by Katherine Hayles is that of a contemporary photography critic
not familiar with the use of the "layers" tool in Photoshop). By
contrast, I situate *Plain Text* at the intersection of theory and
practice: somewhere between "technical literacy for new media studies"
and "philosophical bases for computing in the humanities."

In pursuing the above strategy I make several decisive contributions to
the fields of media studies, literary theory, and the digital
humanities:

First, my approach is manifestly *materialist*. Throughout my argument,
I seek to recover the material contexts (paper, silicon, and magnetic
storage media) that give support to higher-order social and cognitive
phenomena (like literature, text, and discourse).

Second, the book contains a strong undercurrent of *humanism*. In making
explicit the ways in which changes in the material substratum affect
higher-order cultural techniques (of knowledge production and literary
dissemination), I argue for the reinstatement of human agency in a
conversation that has largely turned towards the object, the system, and
the post-human. The book's narrative arc can be imagined as developing
from first-order material bases of textual production, to second-order
phenomena, to the emergence of the subject in the later chapters.

Finally, my work is *experimental* in that it affects history and theory
through practice. Because engineering is an evolutionary practice,
contemporary reading and writing implements contain within them traces
of their early development. This means that lines of code from software
running Unix systems in the 1970s are still in some real sense present
on modern machines (like Apple Macintosh laptops and Android phones,
which run Unix-derived operating systems). This property allows for a
media archeology that can "lay bare" the device, making good on the
implied archeological metaphor: involving excavation, surveying, and
artifact discovery at the machine level.

Existing Literature
-------------------

*Plain Text* makes a theoretical intervention in the cluster of media
studies- and digital humanities-related fields that include science and
technology studies, platform studies, history of data, software and
critical code studies, and media archeology. Recent comparable books in
this space include: *Paper Knowledge*, by Lisa Gitelman (Duke University
Press, 2014); *Coding Freedom: The Ethics and Aesthetics of Hacking*, by
Gabriella Coleman (Princeton University Press, 2012); *Mechanisms: New
Media and the Forensic Imagination*, by Matthew G. Kirschenbaum (MIT
Press, 2012); *Files: Law and Media Technology*, by Cornelia Vismann
(Stanford University Press, 2008); *Programmed Visions: Software and
Memory*, by Wendy Hui Kyong Chun (MIT, 2013); *How We Think: Digital
Media and Contemporary Technogenesis* by Katherine Hayles (Chicago,
2012); *Beautiful Data: A History of Vision and Reason since 1945*, by
Orit Halpern (Duke, 2015); and several titles in the Electronic
Mediations series at Minnesota University Press, which published Lori
Emerson's *Reading Writing Interfaces* in 2014.

My work extends the research program represented in these volumes in
several important directions. While committed to broadly theoretical
concerns---that is, ideas that can guide or challenge the way we study
texts, their production, their meaning, and their impact on the people
who use and produce them---my argument also dwells in the realm of
traditional philosophy and (more narrowly) philosophy of text and
technology. Furthermore, more than a decade of professional experience
in software development grounds my thought in the fields of software and
electrical engineering to an extent greater than one generally finds in
similar manuscripts. Finally, my sources may betray academic training in
comparative literature. The reader should not be surprised to encounter
original translations and texts that undercut the preponderance of North
American material.

Consider, for example, my first chapter, called "We Have Always Been
Digital," which commences with a discussion of "digital representation"
as philosophers Nelson Goodman and John Haugeland define it in the
analytic tradition. I proceed by testing their intuitions on the basis
of something called the "soap opera effect" particular to modern liquid
crystal displays (LCDs) and in the related video-processing technique of
"motion-compensated frame interpolation." The resulting analysis
clarifies the various (and often conflicting) meanings of the word
digital in media studies and in the digital humanities. In the second
chapter of the book, "Literature Down to the Pixel," I visit late
nineteenth- and early twentieth-century U.S. and European patent
archives to argue that universal Turing machines, usually viewed as
computational algorithms, should also be considered in the genealogy of
communications and text processing equipment, as devices that cannot
quite shake the material legacy of paper and pencil. Similarly, in the
third chapter of the book, "Laying Bare the Device," I take a deep dive
into Russian formalist aesthetics and resurface to examine the Hegelian
legacy in the development of the Document Object Model (DOM).

Although I do not mean to engage in the debate on disciplinary
formation, I prefer to describe my work as "computational culture
studies," both in the sense of "the study of computational culture" and
as "computational approaches to the study of culture." It is important
for me to make the case for the reciprocal motion between the
constituent elements of "computation" and "culture." Too often rhetoric
around the digital humanities resembles a one-way street, in which
computational methods are promised to reform the humanities
unilaterally.

Books like Alexander Galloway's *Laruelle: Against the Digital*
(University of Minnesota Press, 2014), Matthew Fuller's *Evil Media*
(MIT Press, 2012), and Johanna Drucker's *What Is?* (Cuneiform Press,
2013) represent the sharp edge of a critical counter-movement to digital
positivism. But this response, too, could be balanced against the
constructive potential of the digital humanities, which extend
humanistic inquiry into new and exciting directions. As was the case
with the "linguistic turn" in the decades prior, almost all fields of
human knowledge are now experiencing a turn towards computational
methods that offer insights at previously unavailable scales of
analysis. Witness the emerging fields of computational biology,
computational chemistry, computational linguistics, computational
geometry, computational archeology, computational architectural design,
computational philosophy, and computational social science, among
others. The impact of computation therefore cannot be lightly dismissed.
In *Plain Text*, I stake out a middle ground between Stephen Ramsey's
laudatory *Reading Machines* (University of Illinois Press, 2011) and
David Golumbia's disparaging *The Cultural Logic of Computation*
(Harvard University Press, 2009). Ultimately, I argue in favor of a
transformative use of technology in the humanities, with reciprocal
effects that promise mutual enrichment.

Audience and Market
-------------------

As is the case for most of my work, *Plain Text* appeals to several key
audiences. The first comprises media scholars interested in the history
of data and computing in the twentieth century. The second audience can
be located in textual studies, among scholars seeking to understand the
impact of technology on literary theory or book history. Finally, the
manuscript targets the broader audience of digital humanities and
information science practitioners (particularly in the field of
human--computer interaction) actively engaged in using and creating
textual interfaces that shape contemporary reading and writing praxis.

As a former software engineer and now a literary scholar, I make sure
that my research bridges the (perceived) gap between the "two cultures"
of science and the humanities. My courses at Columbia University, which
include Code & Poetry (Fall 2014), Computing in Context (Spring 2015),
and Foundations of Computing for Journalists (Summer 2014), attract a
diverse body of students from various disciplines (and particularly from
departments of English, history, and computer science). I lecture widely
in language departments, in schools of engineering, and in front of
publishers, architects, artists, and librarians. As one of the founders
of Columbia's Group for Experimental Methods in the Humanities, I
encourage my students to consider technology reflectively, combining
critical theory with a measure of critical practice.

In this spirit, my group has organized workshops on online security for
activists; we have reached out to an online community of engineers to
help us write media history as a project in citizen humanities; and we
are set to teach critical making at Rikers Island in July 2015. I am
inspired in these endeavors by collaborators from Digital Humanities
labs across the country and by my colleagues in the English Department
and at the Berkman Center for Internet & Society, where I am an active
faculty associate. I keep these manifold audiences in mind as I complete
*Plain Text*. The book exposes intellectual frameworks that bolster my
research and teaching activities. I write to strengthen these projects
and to give back to the community that has supported me so generously. I
hope to rely on the same goodwill and support networks in reaching out
to promote my book.

In my teaching career, I am often asked to create workshops, courses,
and certificate programs for graduate students in the humanities
interested in computational studies. These have included seminars at the
Digital Humanities Summer Institute (U. Victoria), courses for the Lede
Program in the Journalism School (Columbia), and workshops for students
at CUNY Digital Praxis and in the NYU Digital Internship Program. Texts
usually assigned in courses like that are either volumes published by
technical presses for a professional audience or theory-based readings
in new media studies related only loosely to teaching the fundamentals
of computer science in context. The (optional) technical appendix could
serve to supplement the main body of the work with a series of
"experiments" that illustrate theoretical concepts in action, at the
keyboard, making the text applicable to a greater variety of educational
environments (beyond the conventional classroom).

Length and Format
-----------------

I am writing the book as a traditional volume, expounding a sustained
thesis across six chapters (along with a short introduction). At this
point, I am aiming for a manuscript of around 80,000 words (not
including citations), allotting around 10,000 to 15,000 words per
chapter. The chapters tend to have five to seven more granular
subsections that help to clearly demarcate chapter structure.

The book could include an optional appendix, discussed in detail in the
Annotated Table of Contents. The appendix does not require any special
treatment. In addition, the manuscript contains 15--20 figures,
primarily as black-and-white diagrams from technical literature and
images created by the author. I have received a modest subvention to
offset any costs associated with image production, publishing, and
preparation of the manuscript.

Relationship to Dissertation and Other Published Work
-----------------------------------------------------

The book bears a resemblance to my doctoral dissertation in the subtitle
only. Several paragraphs from the embargoed dissertation did make it
into *Plain Text* in an ad-hoc manner, but the book as a whole
represents a completely new framework and a new direction in my thinking
about the subject.

With the approval of the press, I plan to place two of the book's
shorter, more peripheral chapters, in their redacted form, into journals
that would help promote and expand an audience for the book as a whole.
I also plan to present the same chapters (in an even more compact form)
at several upcoming conferences, including CHI, the Conference on Human
Factors in Computing Systems (a significant publication organ in the
field of human-computer interaction).

Schedule to Completion
----------------------

The research for this book was enabled by a year-long fellowship at the
Berkman Center for Internet & Society. Consequent to the research phase,
I taught several classes on the subject, which helped refine my thinking
and provided further notes and primary material. As of today, the
manuscript stands at roughly 60,000 words, with three chapters completed
in their draft form. I am writing actively and plan to have the first
draft of the manuscript ready in the summer of 2015. I am on leave next
academic year, having cleared my schedule, with plans of seeing this
project through to publication.

[^1]: See @manovich_there_2011, 53-106.
