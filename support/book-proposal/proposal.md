---
title: "Plain Text: The Poetics of Human-Computer Interaction"
author: Dennis Tenen
cover-image: "images/steno.png"
csl: csl/mla-no-biblio.csl
bibliography: plain-text.bib
toc: true
mainfont: "fbb"
fontsize: 12pt
---

This book is about the strange entanglement between humans, texts, and
machines. It examines key literary theoretical ideas alongside the
intellectual history of software engineering that frame contemporary reading,
writing, and interpretation practices. It is a deeply materialist work, in
which I argue that even our most ingrained intuitions about texts are
profoundly alienated from the physical contexts of intellectual production. A
new kind of poetics is therefore necessary to preserve the free play of ideas
implicit in the method of humanistic inquiry. Often, the work of literary
theory defines itself in terms of specific texts, as a series of readings. My
object of study is instead the nature of textuality itself. I am interested
here in how texts are produced; in the metaphors that guide computation; in
the forms, formulae, and formats that structure human-computer interaction; in
the literary device; and in the strange shape of contemporary inscription,
which, no longer a single mark on paper, stretches between the site of
fleeting projection, the screen, and the site of storage in its solid,
archival state.

In quite another sense, this book is about more than words, screens, and
archives: it reasons through the dynamics of technological settlement and
displacement, through the comforts and the discomforts of inhabiting our
numerous extensions of self, our media homes.

While I write these introductory remarks, a ceiling-mounted smoke detector in
my kitchen begins to emit a loud noise every three minutes or so. A pleasant
female voice announces also "low battery." This is, as I learn, a precaution
stipulated by US National Fire Alarm Code 72-108 11.6.6 (2013). The clause
requiring a "distinct audible signal before the battery is incapable of
operating" is not only required by law, it is encoded into the device. The
smoke detector embodies that piece of legislation in its circuitry. We thus
obtain a condition where the two meanings of code---as governance and machine
instruction---coincide. Code equals code.

I am at home, but I also receive a notification of the alarm on my mobile
phone. Along with monitoring apps that help make my home smarter, the phone
contains most of my library. I often pick it up to read a book. However, what
I mean by "reading a book" obscures a number of metaphors for a series of odd
actions. The "book" is a small, thin black rectangle: three inches wide, five
inches tall, and barely a few millimeters thick. A slab of polished glass
covers the front of the device, where the tiny eyes of a camera and a light
sensor also protrude. At the back, made of smooth soft plastic, we find
another, larger camera. At the foot of the device a grid of small perforations
indicate breathing room for a speaker and several microphones. To "open" a
book I touch the glass. The machine recognizes my fingerprint almost
instantly. I then tap and poke at the surface until I find a small image that
represents both my library and the book store, where I can "buy books." Buying
books does not actually involve ownership or books. Rather, I agree to a
licence that grants limited access to data, which the software then assembles
into something resembling a book on the screen. I tap again to begin reading.
The screen dims to match room ambiance as words fill the screen. One of the
passages on the first page appears underlined: a number of other readers in my
social circle must have found the passage notable. My finger slides along the
glass surface to turn a "page." The device emits a muffled rustle to reinforce
the pretense of manipulating paper. The image curls ever so slightly in a way
that resembles the printed page as another "page" slides into view. My tiny
library metaphor contains hundreds of such page metaphors.

Despite appearances, the electronic metaphor-making device next to my computer
has more in common with the smoke detector than it does with several paper
volumes scattered across my desk. The electronic book and the smoke alarm
contain printed circuit boards, capacitors, silicone chips, and resistors.
Both draw electric current. Both require firmware updates and both are
governed by codes, political and computational. The smoke alarm and the mobile
phone connect to the internet. They communicate with remote data centers and
with each other. And yet, I continue to read electronic books as if they were
familiar, immutable, and passive objects: just books. I think of them as
intimate artifacts---friends even---wholly known to me, comforting, and warm.
The electronic book is none of those things. Besides prose, it keeps my
memories, pictures, words, sounds, and thoughts. It records my reading,
sleeping, and consumption habits. It tries to sell me things, showing me
advertisements for cars, jewelry, and pills. It comes with a manual and terms
of service. It is my confidant, my dealer, my spy.

## Thesis and Scope

Consider for example a news report that alters its content based on the
reader's location. Imagine also an e-book reader that highlights popular
passages of a novel in real time, shortening the less popular passages down to
their algorithmically distilled summaries.  For a literary analyst, the
instability of the digital medium means analysis cannot be confined to reading
for surface meaning alone.  How can close reading persist when reading devices
reconfigure the text to fit individual tastes, mood, or politics? How would we
even agree on the fact that we are reading the same text? The very possibility
of interpretation comes into question.  Influenced by the tradition of formal
literary exegesis in the work of scholars like Boris Eikhenbaum, Franco
Moretti, and Caroline Levine, my overarching aim in *Plain Text* is therefore
to expose the illusion of verisimilitude between text on paper and the
simulated text on a screen. The words may look the same, but the underlying
affordances of the medium differ in significant detail. The device on my desk
is not a book, but a simulation of a book. And everything associated with
reading this metaphor must in itself be understood under the sign of
simulation. What kind of a metaphor is it? How did it come into being and how
does it affect practices of literary interpretation? In *Plain Text* I come to
terms with the conditions of *simulated textuality*.

The transition between the Gutenberg press and "Project Gutenberg," an online
library containing thousands of texts, has revealed hitherto unexamined
possibilities. Unlike pen and paper which come in direct contact with each
other during writing, the contact between keyboard and screen passes through
another complex chain of mediation. Writing in that sense in itself becomes an
experience both programmed and simulated. We do not write in the conventional
sense of inscribing marks into a static host. Rather, we are shown images of
inscription superimposed onto a simulation of the medium.

Neither the digital word nor digital page exist in the way they appear in the
word processor. At best, such composite tropes attain a measure of similarity
to the physical realities of typing, editing, and archiving paper. Simulated
erasure for example, of the kind that happens when the writer presses the
delete key, does not necessarily entail the corresponding erasure of content
on the disk. The erased word could persist and even multiply across other
storage drives and devices. In the worst case, the connection between
keyboards and screens suffers from intractable "man-in-the-middle" attacks, by
which third parties maliciously alter the content of intended communication
[@needham_using_1978].

In this book, I will argue that some of the higher-level political afflictions
of the contemporary public sphere---mass surveillance and online censorship,
for example---relate to our failure as readers and writers to come to terms
with the changing conditions of digital textuality. A society that cares about
the long-term preservation of complex discursive formations like free speech,
privacy, or online deliberation, would do well to take heed of the textual
building blocks at their foundation. The structure of discursive
formation---documents and narratives---has long been at the center of both
computer science and literary theory. Using primary sources from both
disciplines, *Plain Text* uncovers the shared history of literary machines,
bringing computation closer to its humanistic roots, and the humanities closer
to its computational realities.

"Software's ghostly presence produces and defies apprehension," Wendy Chun
wrote in her *Programmed Visions* [@chun_programmed_2011, 3]. But what happens
when all text is a type of software? Friedrich Kittler ended his book on a
similar note: in his vision, literature has been finally defeated by
technologies of encryption, secrecy, and obfuscation
[@kittler_gramophone_1999, 263]. Attempts to silence print through book
burning or censorship are viscerally obvious and met with nearly universal
disapproval. Unlike censorship or the burning of books, the dominion of
computation proceeds by clandestine means. The simulation-producing nature of
computed text preserves the outward appearance of printed text, while
concealing the specifics of discipline and control. I mean discipline and
control in the sense of shaping affordances: a mode of physical regulation
that structures the production, access, and the distribution of knowledge. The
challenge of *Plain Text* will be in the description of such emerging but
often occluded technological possibilities.

A concern with the material conditions of simulated textuality leads us to a
rich archive of materials from the history of literary theory, semiotics,
telegraphy, and electrical engineering from the middle of the nineteenth to
the end of the twentieth centuries. Reflecting on the development of Morse
Code in 1949 in the *Proceedings of the American Philosophical Society*, Frank
Halstead mentioned the difficulty of finding a home in either the arts or
sciences for what he calls "code development." He wrote, "it is a matter
somewhat related to the general art of cryptology, yet it is not wholly
divorced from electrical engineering nor from general philology"
[@halstead_genesis_1949, 456]. As Halstead anticipated, research in the field
of code development and computational culture has led me to a range of primary
materials: from the proceedings of the Association for Computing Machinery
(ACM) to the US Patent and Trademark Office; from Bell Labs to early Soviet
publishing houses that heralded the advance of formalism; from studies on
animal communication behavior, to Unix manuals, to textbooks on semiotics, and
to foundational texts in the philosophy of aesthetics and literary theory.

I deploy the archive to argue that extant theories of interpretation evolved
under the conditions tied to static print media. By contrast, electronic text
changes dynamically to suit its reader, political context, and geography.
Consequently, I argue for the development of what I term *computational
poetics*: a strategy of interpretation capable of reaching past surface
content to reveal the software platforms and the hardware infrastructures that
contribute to the construction of meaning. Drawing on a range of materials at
the intersection of literary thought and the history of modern computing,
*Plain Text* examines a number of key literary-theoretical constructs,
grounding contemporary practices of interpretation within the material
contexts of digital production.

I appeal to the idea of "plain text" in the title of this book to signal an
affinity with a particular mode of computational meaning-making. Plain text
identifies a file format and a frame of mind. As a file format, it contains
nothing but a "pure sequence of character codes." Plain text stands in
opposition to "fancy text," "text representation consisting of plain text plus
added information" [@unicode_consortium_unicode_1990]. In the tradition of
American textual criticism, "plain text" alludes to an editorial method of
text transcription which is both "faithful to the source" and is "easier to
read than the original document" [@cook_considerations_1988]. Combining these
two traditions, I mean ultimately to build a case for a kind of a systematic
minimalism when it comes to our use of computers---a minimalism that
privileges access to source materials, ensuring legibility and comprehension.
I do so in contrast with other available modes of human-computer interaction,
which instead privilege maximizing system-centric ideals like efficiency,
speed, performance, or security.

My use of plain text implies also a poetics of reading and writing. The title
therefore further identifies an interpretive stance one can assume in relation
to the making and the unmaking of literary artifacts. Besides visible content,
all contemporary documents carry with them a layer of hidden information.
Originally used for typesetting, that layer affects more than innocuous
document attributes like "font size" or "line spacing." Increasingly, devices
that mediate literary activity also embody structures of governance and
control. These tacitly encoded agents police intellectual property laws; they
censor and carry out surveillance operations. For example, the Digital
Millennium Copyright act, passed in the United States in 1996, goes beyond
written injunction to require in some cases the management of digital rights
(DRM) at the level of hardware. An electronic book governed by DRM may
subsequently prevent the reader from copying or sharing stored content, even
for the purposes of academic study.[^ln-dmca] In some situations, it may
report the reader's activity to the authorities. Building on the recent work
of scholars like Wendy Chun, Tung-Hui Hu, Finn Brunton, Helen Nissenbaum, and
Lisa Gitelman I make the case for an empowered *computational poetics*, a
method of inquiry that aims to bring tacit control structures once again under
the purview of interpretation and critique [@chun_programmed_2011;
@gitelman_paper_2014; @hu_prehistory_2015; @brunton_obfuscation:_2015].

Annotated Table of Contents
---------------------------

The passage from keystroke to pixel gives the book its shape. In the chapters
to follow, our mobile phones and laptops come fully into view as metaphor
machines engendering ubiquitous simulation. The first three chapters are thus
concerned with the structure of the computational metaphor. The **first
chapter** begins with an explication. What does it mean to turn a page, I ask,
when neither the page nor the action of turning correspond to their implied
analogies?  The analysis of the metaphor helps trace the intellectual history
of human-computer interaction, a field which progressed from "conversational
programming" to the "direct manipulation" paradigm shaped by cognitive
metaphor theory and immersive theater. The logic of "directness" leads to the
rapidly developing field of brain-to-computer interfaces. The chapter
concludes with a moment of speculative formalism, in which we consider the
possibility of affective literature that eschews language and representation.

At the core of the book's **second chapter** lies the notion of a modernist
literary device, understood both as literary technique and a thought
experiment about intelligent machines, directly connected to the birth of
modern computing. A section on literary technique in the thought of Percy
Lubbock, Walter Benjamin, and Mikhail Bakhtin opens the discussion.
Materialist poetics arise concomitantly with a mechanistic, rule-based view of
language leading to a series of thought experiments first in the writing of
Ludwig Wittgenstein, and then in the seminal paper of Alan Turing on an
imaginary computer capable of reading and writing. The verbs to read and to
write imply a type of cognitive processing. What does it mean to read and to
write for a machine? What about broken mechanisms of comprehension? At once a
device and an algorithm, the Turing machine blurs the boundaries between
software and hardware, code and content, intelligence and its imitation.

Two rich intellectual histories collide on the pages of the **third chapter**:
one, the material history of formatting as a concept in computer science and
the other, the intellectual history of form in literary theory.  Format
emerges as a concept that mediates between form understood as internal "rules
for construction" and form understood as "external shape." The formatting
layer transforms one type of structure, a series of bits arranged into tracks
and sectors, into another, letters arranged into sentences and paragraphs. I
draw a short history of text formats that commences with several "control
characters" limited in function to actions like "carriage return" or "stop
transmission." With time, the formatting layer encompassed all manner of
machine instruction, including structures of governance like "digital rights
management" and "copy protection." A manufacturer's ability to censor or to
surveil electronic books is contained within the formatting layer.

The **fourth chapter** charts the emergence of screen reading. The screen
appears to restore a measure of visibility lost to magnetic inscription, with
one major side-effect. Fidelity between the word visible and the word archived
cannot be guaranteed. What the screen shows and what is stored on tape or hard
drive has only a contingent correlation. Screen reading further happens on
screens that refresh themselves at a rate of around 60 cycles per second
(Hertz). The digital word is technically an animation; it moves even as it
appears to stand still. This property of the medium attunes the reader to a
particular mode of apprehension, affecting not just the physics but also the
aesthetics of digital media. Works by the philosophers Henri Bergson, Jakob
von Uexküll, and Nelson Goodman help construct a phenomenology of screen-based
digital perception.

The **fifth and final chapter** begins with a discussion of an apparent
paradox.  A camp of media theorists and textual scholars in the 1990s
conceived of electronic texts as an ephemeral, almost immaterial, phenomenon.
Text shimmered and glared: it was spoken of in terms of *hypertext*, light
writing, and electricity. A generation of theorists that came after insisted
on the weighty materiality of electronic media. Reading began to engage the
morphology of rare metals, media archeology, and hard drive forensics. Both
accounts, I argue, capture an aspect of the same underlying condition. The
perceived image of an archived inscription splits from its source. The sign
plausibly resides both on the screen and on the hard drive. It splits, in some
real a sense, diverging at the site its projection from the site of the
archive. Erasing an inscription on the screen, for example, may not elicit the
corresponding action on the disk. Using archival materials from the history of
telegraphy in the late nineteenth and early twentieth centuries, I tell the
story of the gradual fracture and the ultimate illegibility of the
computational sign. In conclusion, I argue for the necessity of computational
poetics.

<!--
The **sixth and final chapter** looks to the site of storage to find the media
"homes" that house the vast archives of our private media collections. It
begins with a close reading of Beckett's *Krapp's Last Tape*. Krapp makes
yearly audio recordings of himself, only to revisit them and to enter into a
sort of dialog with his own voice from the past. I posit this archival
encounter as Krapp's "media being" and suggest that such encounters are
commonplace. Writers and book collectors regularly deposit "snapshots" of
their consciousness into files, bookshelves, and folders. Jean-Paul Sartre's
idea of an "appointment with oneself" helps to reveal this external
construction of files, folders, and library furnishings as cognitive
extension, in need of delicate pruning and arrangement. A close reading of the
"home" folder, the default location of personal files on many systems,
concludes with the discussion of media homes. Finally, I return to the theme
of displacement, arguing for a mode of inhabitance within media that is
uncanny or un-homed [*Unheimliche*], contrary to discourse that speaks in
terms of "digital natives" and those who are "born digital." I build on the
immigrant poetics of Vilém Flusser and Viktor Shklovsky to suggest a kind of
information processing that necessitates a purposeful movement between the
polarities of familiarization and estrangement, settlement and expatriation.
-->

Contributions to Literature
---------------------------

The book contributes to the fields of literary theory, media studies, and
digital humanities. Scholarship the digital humanities is sometimes criticized
for being ahistorical or atheoretical, abandoning deep traditions of literary
theory and criticism, even where such traditions would help bolster the case
for the digital humanities. The related field of new media studies has the
opposite problem. Although theoretically sophisticated, it sometimes produces
research far removed from the actual practice of creating new
media.[^ln-hayles] By contrast, I situate *Plain Text* at the intersection of
theory and practice: somewhere between "technical literacy for new media
studies and literary theory" and "philosophical bases for computing in the
humanities."

Recent comparable books in this space include: *Paper Knowledge*, by Lisa
Gitelman (Duke University Press, 2014); *Coding Freedom: The Ethics and
Aesthetics of Hacking*, by Gabriella Coleman (Princeton University Press,
2012); *Mechanisms: New Media and the Forensic Imagination*, by Matthew G.
Kirschenbaum (MIT Press, 2012); *Files: Law and Media Technology*, by Cornelia
Vismann (Stanford University Press, 2008); *Programmed Visions: Software and
Memory*, by Wendy Hui Kyong Chun (MIT, 2013); *How We Think: Digital Media and
Contemporary Technogenesis* by Katherine Hayles (Chicago, 2012); *Beautiful
Data: A History of Vision and Reason since 1945*, by Orit Halpern (Duke,
2015); and several titles in the Electronic Mediations series at Minnesota
University Press, which published Lori Emerson's *Reading Writing Interfaces*
in 2014.

My work extends the research program represented in these volumes in several
important directions. While committed to broadly theoretical concerns---that
is, ideas that can guide or challenge the way we study texts, their
production, their meaning, and their impact on the people who use and produce
them---my argument also dwells in the realm of traditional philosophy and
(more narrowly) philosophy of text and technology. More than a decade of
professional experience in software development grounds my thought in the
fields of software and electrical engineering to an extent greater than one
generally finds in similar manuscripts. Finally, the range of primary
materials used in this book reveals my academic training in comparative
literature. The reader will encounter original translations---texts from
Greek, German, and Russian---which undercut the preponderance of North
American material. Consider, for example, my third chapter, called "Form,
Formula, Format," which commences with a discussion of formalism first in
aesthetic philosophy through the works of Plato, Hegel, and the Russian
formalists, then in the works of textual critics like G. Thomas Tanselle,
Jerome McGann, and Johanna Drucker, and finally in the technical manuals
describing something called the document object model, which tests our
theoretical intuitions based on a specific case study crucial to our
understanding of electronic text.

Although I do not mean to engage in the debate on disciplinary formation, I
prefer to describe my work as "computational culture studies," both in the
sense of "the study of computational culture" and as "computational approaches
to the study of culture." It is important for me to make the case for the
reciprocal motion between the constituent elements of "computation" and
"culture." Too often rhetoric around the digital humanities resembles a
one-way street, in which computational methods are promised to reform the
humanities unilaterally. Books like Alexander Galloway's *Laruelle: Against
the Digital* (University of Minnesota Press, 2014), Matthew Fuller's *Evil
Media* (MIT Press, 2012), and Johanna Drucker's *What Is?* (Cuneiform Press,
2013) represent the sharp edge of a critical counter-movement to digital
positivism. But this response, too, could be balanced against the constructive
potential of the digital humanities, which extend humanistic inquiry into new
and exciting directions. As was the case with the "linguistic turn" in the
decades prior, almost all fields of human knowledge are now experiencing a
turn towards computational methods that offer insights at previously
unavailable scales of analysis. Witness the emerging fields of computational
biology, computational chemistry, computational linguistics, computational
geometry, computational archeology, computational architectural design,
computational philosophy, and computational social science, among others. The
impact of computation therefore cannot be lightly dismissed. In *Plain Text*,
I stake out a middle ground between Stephen Ramsey's laudatory *Reading
Machines* (University of Illinois Press, 2011) and David Golumbia's critical
*The Cultural Logic of Computation* (Harvard University Press, 2009).

The book represents a new direction in my research, based on previously
unpublished work. The manuscript has received extensive revisions, with
comments on individual chapters from scholars like Brian Larkin, Barbara
Herrnstein-Smith, Bernard Harcourt, Susan Zieger, Johanna Drucker, Stefan
Andriopoulos, and Lydia Liu.

Author's Bio
-----------

This book is in part a reflection of my professional biography, which combines
a track record of excellence in the fields of literary scholarship and
software engineering.

I began my professional career at Microsoft, where I made significant
contributions to the development of the Microsoft XP operating system. This
experience translated directly to my co-founding of Columbia's Group for
Experimental Methods in the Humanities, where I direct a number of research
projects. The lab's work has been featured on the pages of *The New York
Times*, *TIME* magazine, *Der Spiegel Online*, *FiveThirtyEight*, and *Le
Monde*.[^ln-press] Since the fall of 2012, after spending a year in residence,
I have also been a faculty affiliate at Harvard's Berkman Center for Internet
and Society. My work in the digital humanities has been subject to numerous
grants and awards, including the Brown Institute Flagship Grant for media
innovation in partnership with Stanford University.

My training as a literary scholar concluded under the guidance of Elaine
Scarry and William Mills Todd, of Harvard University, at the department of
English and Comparative Literature. I am currently a fourth-year tenure-track
assistant professor at Columbia University's Department of English and
Comparative Literature, where I am also an affiliate member of the Data
Science Institute, New Media Center. I teach and lecture widely in the
departments of English, History, Computer Science, and Journalism usually on
topics of literary theory, computational culture studies, new media, and
digital humanities. My work has appeared on the pages of scholarly journals
like *Modernism/modernity*, *Boundary 2*, *Computational Culture* along with
popular press like *LA Review of Books* and *Public Books*.

Audience and Market
-------------------

In writing *Plain Text*, I have kept the above manifold audiences in mind. I
am interested in producing deep but accessible scholarship, which has impact
within and outside of academia. The diverse professional networks I have
developed over the years represent my audience and my community of support. I
plan to rely on them to promote the book, to solicit reviews, and to engage
into a dialog with my work.

*Plain Text* appeals to several key audiences. The first comprises media
scholars interested in the history of media, data, and computing in the
twentieth century. The second audience can be located in literary and textual
studies, among scholars seeking to understand the impact of technology on
literary theory or book history. Finally, the manuscript targets the broader
audience of digital humanities and information science practitioners
(particularly in the field of human-computer interaction) actively engaged in
using and creating textual interfaces that shape contemporary reading and
writing praxis.[^ln-ipam]

There is at the moment a paucity of books that can be taught in classes like
the graduate seminar "Digital Studies | Prototyping Texts," taught at the
University of Victoria by Jentery Sayers; the undergraduate lecture course
called "Making and Knowing Project: What is a Book in the 21st Century?" by
Pamela Smith at Columbia; and "Text Transformations" by Matt Gold, at CUNY.
All of the above courses have requested and assigned advanced copies of draft
chapters from *Plain Text*, which bodes well for the book's wider reception.

Finally, I write for the broader public. My book answers the call put forth in
Bernard Harcourt's recent *Exposed: Desire and Disobedience in the Digital
Age*. Books like Harcourt's bring into focus a significant problem in
contemporary society, by which our digital lives become enmeshed in a system
of production that commodifies human experience. Devices that surround us
collect and trade in our reading habits, social interactions, and intimate
conversations. In *Plain Text* I argue widely for a new mode of reading, not
limited to our ability to code, but one that makes visible the ideals implicit
in our technological decisions: to buy a new phone, to contribute an online
review for a book, or to write using free software. Our challenge today is to
uproot ourselves from the comfort that rapidly descends on the digital
dwellings that house our intellectual life. It is in our broadly human, civic
interest, I argue, to keep "smart" technology at hand, under continual
scrutiny of critical, close, and closest possible reading.

Length and Format
-----------------

The book is a traditional volume, expounding a sustained thesis across six
chapters, along with a chapter-length introduction. The manuscript stands at
roughly 90,000 words (including citations).

Several chapters contain figures and illustrations, primarily as
black-and-white diagrams from technical literature and images created by the
author. The total number of illustrations is 22. I have received subvention
funds to offset any costs associated with image production, publishing, and
preparation of the manuscript.

[^ln-hayles]: Katherine Hayles and Ian Bogost among others have advanced this
argument.

[^ln-ipam]: For a representative sample of this group see the Culture
Analytics program hosted by UCLA, which includes over fifty participants from
the sciences and the humanities.

[^ln-press]: For a complete list of our press appearances consult
http://xpmethod.plaintext.in/impact.html#press.
