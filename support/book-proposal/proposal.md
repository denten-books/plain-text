---
title: "Plain Text: The Poetics of Human--Computer Interaction (Book Proposal)"
author: Dennis Tenen

---
\newpage

## Theme and Argument (Placeholder)
## Annotated Table of Contents
### Chapter 1: We Have Always Been Digital

This chapter introduces the book's central themes and arguments, commencing
some of the historical work necessary to the development of a shared critical
vocabulary in use throughout. I argue that discourse around the digital
humanities needs a robust sense of the digital. Popular intuitions about the
"look and feel" of digital aesthetics suggest that sometimes the adjective
carries the connotation of "discrete," while at other times, it is used to mean
something more fluid and continuous, past the point of human perception. A
discussion of Liquid Crystal Display technology (LCD) flows into a section that
deals with digital representation from the perspective of analytic philosophy
and through the aesthetics of Nelson Goodman. My summary of that tradition
reveals that language and text are already in some sense "born digital," that
is "reproducible" and "differentiated" throughout. Furthermore, digitality
depends on "reliable processes of copying and preservation"---attributes that
can mean something different to a philosopher than to a librarian. From these
insights I take it that "being digital" is not an intrinsic ontological
condition, but rather a structure imposed from without. Case studies from the
history of telegraphy illustrate the concluding discussion on the nature of
binary and plain text formats, in a distinction that supersedes the dichotomy
between analog and digital media.

### Chapter 2: Literature Down to the Pixel

In this chapter, I continue the historical narrative started in Chapter 1,
along with presenting the methodological and theoretical underpinnings of the
book. I think of it as the "laying the grounds" chapter. Having established the
roots of digital textuality in the history of character encoding, I begin the
work of moving from first-order concepts such as "text" and "code" up to
second-order concepts such as "file," "folder," and "document." The chapter
starts by developing a theory of "microanalysis," the closest possible kind of
reading that pays attention to the material contexts of knowledge production. I
argue here that the concern with value in literary criticism detracts from the
machinations of naked circuit control embedded into the contemporary text
apparatus. Unlike scholars in the Foucauldian tradition (who often trace the
machinations of power through discourse, on the level of representation), I
concentrate my analysis on mechanisms of control at the material roots of
literary practice. In constructing a media history through primary sources on
the early development of Turing machines, I show the explicit admixture of
content and code: one meant to communicate messages to humans and the other to
program universal machines. I conclude by arguing that Turing machines were
anticipated not by the Babbage calculator alone, but also through a series of
advances in communications, word processing, and media storage. A notion of
text (as opposed to number) is hence "baked into" the system.

### Chapter 3: Laying Bare the Device

At the heart of the book and central to its argument, Chapter 3 begins by
outlining a recent discussion on surface reading. I ask: What lies beneath the
text, literally? The question leads to the common distinction between form and
content. Here, I find that, going back to the Russian formalist reception of
Hegelian aesthetics, "form" was at times used to indicate concrete shape and at
times to indicate abstract universals, such as technique and formula. A case
study in removable storage---like ticker tape and floppy disks---elucidates the
movement of text: from human-legible inscription on the page and punch card to
magnetic inscription invisible to the naked human eye. The case study unfolds
the distinction between print, in which matter, form, and content lie flat, and
screen, where the three layers occupy physically distinct strata of the
Document Object Model, providing only the illusion of flattened textuality.
The apparent immateriality of digital text brings promise of epistemological
(social) and even phenomenological (personal) transformation. But it also has a
major practical drawback. Inscription on magnetic tape cannot be assumed to
correspond to the composite screen image. Forms of governance like Digital
Rights Management can now be embedded deep within the structure of the "data
object" itself and further hidden from view---precluding, and sometimes making
illegal outright, the possibility of interpretation (of any sort). The
discussion concludes with a stark image illustrating the contrast between
screen surface and the underlying bit structure. To produce the image, I use
reverse-engineering tools to inject malicious code into an Adobe Acrobat file
(`.pdf`). The deformed text threatens to damage the literary device. A thick
description of the literary device, now as gadget or instrument, brings
legibility to the fore of reading ethics.

### Chapter 4: Recursive Encounters with Oneself

This chapter continues the movement from the device to the reader. It begins
with a close reading of Beckett's *Krapp's Last Tape*. The title character
makes yearly audio recordings of himself, only to revisit them and to enter
into a sort of dialog with his own voice from the past. I posit this encounter
with the archive as Krapp's "media being" and suggest that such encounters are
commonplace, through similar practices of depositing "snapshots" of one's
consciousness into files, bookshelves, and folders. Sartre's idea of an
"appointment with oneself" helps us see this external structure of files,
folders, and library furniture as cognitive extension, in need of delicate
pruning and arrangement. Documents, in this light, are shown to exist not as
completed works, but as "vectors" that mutate and move through time and space.
Pushing off the communication model offered by Claude Shannon, I ask: What is
being externalized, communicated, and preserved? And answer: It is not simply a
message, but the subject itself.

### Chapter 5: Bad Links

If documents are vectors, where do they terminate? In this chapter I examine
three answers, given at three distinct moments in recent literary history.
First, I recall the discourse surrounding structuralist "intertextuality"---the
idea that textual meaning is always created in relation to another text.
Second, I review the promises and the failures of "hypertext," an idea which
gained prevalence in literary studies with the advent of the internet. Third, I
reflect on the current moment, in which "network analysis," a technique that
seeks to visualize linkages between texts, is being held up by some as the next
step in the evolution of textual studies. In all three of these methodological
moments, I find a similar premise of emergence: the notion that order appears
spontaneously as an aggregate result of simple interactions at the level of the
system. I take the occasion of examining the hyperlinked essays of Gwern (a
mysterious contemporary "researcher, self-experimenter, and writer") to further
criticize what I call the "systems view" of literature, which elevates networks
to the status of ethical and aesthetic actors.

### Chapter 6: Engineering for Dissent

In this final chapter, I argue for the recovery and the preservation of plain
textuality in the day-to-day practice of modern computing. Returning to the
history of the `.txt` file format, I find that early documents from the
International Telecommunication Union archive display unease with encrypted,
non-human-readable formats of information exchange. A theoretical treatment of
technological skepticism (from Karl Marx and Martin Heidegger to Lewis Mumford)
concludes with a discussion about a subject's role in actively shaping material
conditions of media being. As documents that reflect externalized states of
consciousness aggregate in storage locations far removed from the subject, they
become increasingly susceptible to centralized forces of surveillance and
control. Plain text allows political subjects to decouple externalized mental
states from mechanisms of governance. (In other words, to decide actively when
to opt in and when to opt out.) This affordance is not, however, a
deterministic property of literature, the internet, or any other information
exchange system.  Rather, the design of complex systems must itself become
critical practice which, in complement to critical theory, can actively
engineer for textual mechanisms that make individual dissent possible.


### Tech Appendix (optional)

The book assumes no prior technical expertise. It can be read sequentially as a
conventional piece of scholarship in literary/textual theory or new media
studies. But, because much of the book deals with conditions of textuality
extant and recoverable from modern computing devices, I propose to heed the
call of scholars like Jerome McGann and Wendy Chun for the advancement of
theory through practice. To this end, I envision an optional appendix that can
exist on paper or as a companion website, creating an "augmented reading
environment." The appendix would follow each chapter (sequentially) with a
series of experiments at the "command line," a powerful text-based way of
interacting with the computer.

Inspired by the ethos found in Kenneth Ward Church's classic "Unix for Poets"
and by the form of Roland Barthes's seminal *S/Z*, the appendix gives readers
an opportunity to test theoretical intuitions found in the body of the book
against the reality of contemporary computation. For example, the difference
between binary and plain text formats (discussed in the early chapters) could
be made more apparent in comparing the output of `cat file.txt` and `cat
file.pdf` in the terminal.[^ln-cat] In the later chapters, the conversation on
access could be augmented with an exercise on file permissions. Diagnostic
utilities like `ping` and `traceroute` would be brought to bear on network
effects mentioned in the "Bad Links" chapter. In this way, the appendix can
serve to extend historical and theoretical awareness into practical know-how.
An intuitive understanding of the political issues surrounding digital text, be
they "open access," "freedom of information," or "online censorship," begins to
develop at that instrumental level.

Ready-made tools and graphical interfaces for human-computer interaction often
obscure the underlying complexity of the computational environment. For
example, while writing a relatively complicated piece of code, a journalist in
my digital humanities class once confessed to being confused about the
relationship between files and folders. *Plain Text* is a book *about* files
and folders: it is about textuality as encoded in specific ways on machines
that have a shared material history. The book's technical appendix, although
not required for the comprehension of its main ideas, would help cultivate
theoretical intuitions based not on speculation alone but also on "knowledge at
hand."

[^ln-cat]: Use actual file names if you plan to test this out.

## Field Significance

The book seeks to redress a weakness in the field of digital humanities,
particularly at the point of its relevance to literary studies. Scholarship at
the intersection of these two fields is sometimes criticized for being
ahistorical or atheoretical, abandoning deep traditions of literary theory and
criticism, even where such traditions would help bolster the case for the
digital humanities. The nominally related field of new media studies has the
opposite problem. Although theoretically sophisticated, it sometimes produces
research far removed from the actual practice of creating new media (the
archetypal example given by Katherine Hayles is that of a contemporary
photography critic not familiar with the use of the "layers" tool in
Photoshop). By contrast, I situate *Plain Text* at the intersection of theory
and practice: somewhere between "technical literacy for new media studies" and
"philosophical bases for computing in the humanities."

In pursuing the above strategy I make several decisive contributions to the
fields of media studies, literary theory, and the digital humanities:

First, my approach is manifestly *materialist*. Throughout my argument, I seek
to recover the material contexts (paper, silicon, and magnetic storage media)
that give support to higher-order social and cognitive phenomena (like
literature, text, and discourse).

Second, the book contains a strong undercurrent of *humanism*. In making
explicit the ways in which changes in the material substratum affect
higher-order cultural techniques (of knowledge production and literary
dissemination), I argue for the reinstatement of human agency in a conversation
that has largely turned towards the object, the system, and the post-human. The
book's narrative arc can be imagined as developing from first-order material
bases of textual production, to second-order phenomena, to the emergence of the
subject in the later chapters.

Finally, my work is *experimental* in that it affects history and theory
through practice. Because engineering is an evolutionary practice, contemporary
reading and writing implements contain within them traces of their early
development. This means that lines of code from software running Unix systems
in the 1970s are still in some real sense present on modern machines (like
Apple Macintosh laptops and Android phones, which run Unix-derived operating
systems). This property allows for a media archeology that can "lay bare" the
device, making good on the implied archeological metaphor: involving
excavation, surveying, and artifact discovery at the machine level.

## Existing Literature

*Plain Text* makes a theoretical intervention in the cluster of media studies-
and digital humanities-related fields that include science and technology
studies, platform studies, history of data, software and critical code studies,
and media archeology. Recent comparable books in this space include: *Paper
Knowledge*, by Lisa Gitelman (Duke University Press, 2014); *Coding Freedom:
The Ethics and Aesthetics of Hacking*, by Gabriella Coleman (Princeton
University Press, 2012); *Mechanisms: New Media and the Forensic Imagination*,
by Matthew G. Kirschenbaum (MIT Press, 2012); *Files: Law and Media
Technology*, by Cornelia Vismann (Stanford University Press, 2008); *Programmed
Visions: Software and Memory*, by Wendy Hui Kyong Chun (MIT, 2013); *How We
Think: Digital Media and Contemporary Technogenesis* by Katherine Hayles
(Chicago, 2012); *Beautiful Data: A History of Vision and Reason since 1945*,
by Orit Halpern (Duke, 2015); and several titles in the Electronic Mediations
series at Minnesota University Press, which published Lori Emerson's *Reading
Writing Interfaces* in 2014.

My work extends the research program represented in these volumes in several
important directions. While committed to broadly theoretical concerns---that
is, ideas that can guide or challenge the way we study texts, their production,
their meaning, and their impact on the people who use and produce them---my
argument also dwells in the realm of traditional philosophy and (more narrowly)
philosophy of text and technology. Furthermore, more than a decade of
professional experience in software development grounds my thought in the
fields of software and electrical engineering to an extent greater than one
generally finds in similar manuscripts. Finally, my sources may
betray academic training in comparative literature. The reader should not be
surprised to encounter original translations and texts that undercut the
preponderance of North American material.

Consider, for example, my first chapter, called "We Have Always Been Digital,"
which commences with a discussion of "digital representation" as philosophers
Nelson Goodman and John Haugeland define it in the analytic tradition. I
proceed by testing their intuitions on the basis of something called the "soap
opera effect" particular to modern liquid crystal displays (LCDs) and in the
related video-processing technique of "motion-compensated frame interpolation."
The resulting analysis clarifies the various (and often conflicting) meanings
of the word digital in media studies and in the digital humanities. In the
second chapter of the book, "Literature Down to the Pixel," I visit late
nineteenth- and early twentieth-century U.S. and European patent archives to
argue that universal Turing machines, usually viewed as computational
algorithms, should also be considered in the genealogy of communications and
text processing equipment, as devices that cannot quite shake the material
legacy of paper and pencil. Similarly, in the third chapter of the book,
"Laying Bare the Device," I take a deep dive into Russian formalist aesthetics
and resurface to examine the Hegelian legacy in the development of the
Document Object Model (DOM).

Although I do not mean to engage in the debate on disciplinary formation, I
prefer to describe my work as "computational culture studies," both in the
sense of "the study of computational culture" and as "computational approaches
to the study of culture." It is important for me to make the case for the
reciprocal motion between the constituent elements of "computation" and
"culture." Too often rhetoric around the digital humanities resembles a one-way
street, in which computational methods are promised to reform the humanities
unilaterally.

Books like Alexander Galloway's *Laruelle: Against the Digital* (University of
Minnesota Press, 2014), Matthew Fuller's *Evil Media* (MIT Press, 2012), and
Johanna Drucker's *What Is?* (Cuneiform Press, 2013) represent the sharp edge
of a critical counter-movement to digital positivism. But this response, too,
could be balanced against the constructive potential of the digital humanities,
which extend humanistic inquiry into new and exciting directions. As was the
case with the "linguistic turn" in the decades prior, almost all fields of
human knowledge are now experiencing a turn towards computational methods that
offer insights at previously unavailable scales of analysis. Witness the
emerging fields of computational biology, computational chemistry,
computational linguistics, computational geometry, computational archeology,
computational architectural design, computational philosophy, and computational
social science, among others. The impact of computation therefore cannot be
lightly dismissed. In *Plain Text*, I stake out a middle ground between Stephen
Ramsey's laudatory *Reading Machines* (University of Illinois Press, 2011) and
David Golumbia's disparaging *The Cultural Logic of Computation* (Harvard
University Press, 2009). Ultimately, I argue in favor of a transformative use
of technology in the humanities, with reciprocal effects that promise mutual
enrichment.

## Audience and Market

As is the case for most of my work, *Plain Text* appeals to several key
audiences. The first comprises media scholars interested in the history of
data and computing in the twentieth century. The second audience can be
located in textual studies, among scholars seeking to understand the impact of
technology on literary theory or book history. Finally, the manuscript
targets the broader audience of digital humanities and information science
practitioners (particularly in the field of human--computer interaction)
actively engaged in using and creating textual interfaces that shape
contemporary reading and writing praxis.

As a former software engineer and now a literary scholar, I make sure that my
research bridges the (perceived) gap between the "two cultures" of science and
the humanities. My courses at Columbia University, which include Code & Poetry
(Fall 2014), Computing in Context (Spring 2015), and Foundations of Computing
for Journalists (Summer 2014), attract a diverse body of students from various
disciplines (and particularly from departments of English, history, and
computer science). I lecture widely in language departments, in schools of
engineering, and in front of publishers, architects, artists, and librarians.
As one of the founders of Columbia's Group for Experimental Methods in the
Humanities, I encourage my students to consider technology reflectively,
combining critical theory with a measure of critical practice.

In this spirit, my group has organized workshops on online security for
activists; we have reached out to an online community of engineers to help us
write media history as a project in citizen humanities; and we are set to teach
critical making at Rikers Island in July 2015. I am inspired in these endeavors
by collaborators from Digital Humanities labs across the country and by my
colleagues in the English Department and at the Berkman Center for Internet &
Society, where I am an active faculty associate. I keep these manifold
audiences in mind as I complete *Plain Text*. The book exposes intellectual
frameworks that bolster my research and teaching activities. I write to
strengthen these projects and to give back to the community that has supported
me so generously. I hope to rely on the same goodwill and support networks in
reaching out to promote my book.

In my teaching career, I am often asked to create workshops, courses, and
certificate programs for graduate students in the humanities interested in
computational studies. These have included seminars at the Digital Humanities
Summer Institute (U. Victoria), courses for the Lede Program in the Journalism
School (Columbia), and workshops for students at CUNY Digital Praxis and in the
NYU Digital Internship Program. Texts usually assigned in courses like that are
either volumes published by technical presses for a professional audience or
theory-based readings in new media studies related only loosely to teaching the
fundamentals of computer science in context. The (optional) technical appendix
could serve to supplement the main body of the work with a series of
"experiments" that illustrate theoretical concepts in action, at the keyboard,
making the text applicable to a greater variety of educational environments
(beyond the conventional classroom).

[^ln-char]: There are many caveats here, to be explored later. Follow along
with exercises related to the discussion in the Technical Appendix.

[^ln-human]: Recent theory challenges the conceptual boundaries between humans
and machines in a concerted way. Perhaps, such boundaries were never that
clearly articulated in the first place. It is also likely that other modalities
of being are possible on the spectrum between human and machine, or human and
complex system. We will have a chance to explore these possibilities in second
half of the book. For now, I ask that the reader simply rely on the colloquial,
pre-theoretical understanding of both person and instrument.  However
intertwined the hand and the hammer can become, there is an intuitive way in
which a child can separate one from the other. A deep-rooted instinct at work
in that distinction, one that cannot and should not be dismissed as mere
naivet√©. The concept of a human is in itself a powerful theoretical construct,
and, as I will argue throughout, one necessary, not only for the understanding
of key concepts in literary theory and computer science, but also in
articulating an ethics of critical computation.

[^ln-meaning]: I write "meaning" in quotation marks, because the question of
whether it makes sense to talk about meaning for artificial agents is a
question that will remain unresolved, at least until the later chapters, when
we have the chance to discuss notions of data and information as
meaning-carrying units.

[^ln-uni]: The Unicode Consortium. *The Unicode Standard: Worldwide Character
Encoding*, Version 1.0, Volume 1. Reading, Mass.: Addison-Wesley, 1990.

[^ln2-derrida]: This is a bit of a post-structuralist caricature, but it is not
difficult to find direct sources expressing the idea. For example, see John
Caputo quoting Jacques Derrida in his *Deconstruction in a Nutshell: A
Conversation with Jacques Derrida*, "I often describe deconstruction as
something which happens. It's not purely linguistic, involving text or books.
You can deconstruct gestures, choreography. That's why I enlarged the concept
of text. Everything is a text" [@caputo_deconstruction_1996].

[^ln2-iarkho]: I borrow the term "microanalysis" from the largely forgotten
Russian literary scholar and member of the Moscow Linguistic Circle, Boris
Iarkho. In his *Methodologies of Exact Literary Study* (circa 1935-6) he
writes: "I understand 'atomism' as a sort of an ideal aspiration, as an
orientation toward the liminally small. But under no circumstances do I
advocate working with hypothetical quantities, like molecules, atoms,
positrons, and so on, which are located beyond the limits of perception. That
this applied mythology gave us such splendid results in chemistry, should not
conceal its true nature. Tomorrow, all such explanations of visible through the
invisible could give way to other hypotheses, as was the case with their no
less fertile predecessors (elemental spirits, phlogiston, and light ether). But
the cell, the nucleus, and the chromosome endure as lasting accomplishments of
microanalysis. I suggest to move as far as a microscope can reach, and no
further" [@iarkho_metodologia_2006, 363-364]. For Iarkho, the most
quantitatively inclined of the Russian Formalists, microanalysis involved
systematic application of statistical methods to the study of literature.

[^ln2-varela]: See for example @varela_autopoiesis_1974; @barthes_rustle_1989,
5; @nuttall_new_2007, 6-25.

[^ln2-survey]: I can only give anecdotal evidence here, as I often put this
question before my graduate students at the beginning of the semester, with the
reported results.

[^ln2-close]: See [@lentricchia_close_2003] and [@fish_how_2011]. 


[^ln2-internet]: The NEA study has this to say on the topic of What is
responsible for the decline of literary reading?: "If the 2002 data represent a
declining trend, it is tempting to suggest that fewer people are reading
literature and now prefer visual and audio entertainment. Again, the
data---both from SPPA and other sources---do not readily quantify this
explanation [...] the Internet, however, could have played a role. During the
time period when the literature participation rates declined, home Internet use
soared" [@nea_reading_2004, 30].


## Length and Format

I am writing the book as a traditional volume, expounding a sustained thesis
across six chapters (along with a short introduction). At this point, I am
aiming for a manuscript of around 80,000 words (not including citations),
allotting around 10,000 to 15,000 words per chapter. The chapters tend to have
five to seven more granular subsections that help to clearly demarcate chapter
structure.

The book could include an optional appendix, discussed in detail in the
Annotated Table of Contents. The appendix does not require any special
treatment. In addition, the manuscript contains 15--20 figures, primarily as
black-and-white diagrams from technical literature and images created by the
author. I have received a modest subvention to offset any costs associated with
image production, publishing, and preparation of the manuscript.

## Relationship to Dissertation and Other Published Work

The book bears a resemblance to my doctoral dissertation in the subtitle only. Several
paragraphs from the embargoed dissertation did make it into *Plain Text* in an
ad-hoc manner, but the book as a whole represents a completely new framework
and a new direction in my thinking about the subject.

With the approval of the press, I plan to place two of the book's shorter, more
peripheral chapters, in their redacted form, into journals that would help
promote and expand an audience for the book as a whole. I also plan to present
the same chapters (in an even more compact form) at several upcoming
conferences, including CHI, the Conference on Human Factors in Computing
Systems (a significant publication organ in the field of human-computer
interaction).

## Schedule to Completion

The research for this book was enabled by a year-long fellowship at the Berkman
Center for Internet & Society. Consequent to the research phase, I taught
several classes on the subject, which helped refine my thinking and provided
further notes and primary material. As of today, the manuscript stands at
roughly 60,000 words, with three chapters completed in their draft form. I am
writing actively and plan to have the first draft of the manuscript ready in the
summer of 2015. I am on leave next academic year, having cleared my schedule,
with plans of seeing this project through to publication.
