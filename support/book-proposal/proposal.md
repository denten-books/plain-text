---
title: "Plain Text: The Poetics of Human--Computer Interaction (Book Proposal)"
author: Dennis Tenen

---

## Theme and Argument (Placeholder)

This book is about the strange entanglement between humans, texts, and
machines. It examines key literary theoretical ideas alongside the
intellectual history of software engineering that frame contemporary reading,
writing, and interpretation practices. It is a deeply materialist work, in
which I argue that even our most ingrained intuitions about texts are
profoundly alienated from the physical contexts of intellectual production. A
new kind of poetics is therefore necessary to preserve the free play of ideas
implicit in the method of humanistic inquiry. Often, the work of literary
theory defines itself in terms of specific texts, as a series of readings. My
object of study is instead the nature of textuality itself. I am interested
here in how texts are produced; in the metaphors that guide computation; in
the forms, formulae, and formats that structure human-computer interaction; in
the literary device; and in the strange shape of contemporary inscription,
which, no longer a single mark on paper, stretches between the site of
fleeting projection, the screen, and the site of storage in its solid,
archival state. In quite another sense, this book is about more than words,
screens, and archives: it reasons through the dynamics of technological
settlement and displacement, through the comforts and the discomforts of
inhabiting our numerous extensions of self, our media homes.


## Annotated Table of Contents
## Field Significance

The book seeks to redress a weakness in the field of digital humanities,
particularly at the point of its relevance to literary studies. Scholarship at
the intersection of these two fields is sometimes criticized for being
ahistorical or atheoretical, abandoning deep traditions of literary theory and
criticism, even where such traditions would help bolster the case for the
digital humanities. The nominally related field of new media studies has the
opposite problem. Although theoretically sophisticated, it sometimes produces
research far removed from the actual practice of creating new media (the
archetypal example given by Katherine Hayles is that of a contemporary
photography critic not familiar with the use of the "layers" tool in
Photoshop). By contrast, I situate *Plain Text* at the intersection of theory
and practice: somewhere between "technical literacy for new media studies" and
"philosophical bases for computing in the humanities."

In pursuing the above strategy I make several decisive contributions to the
fields of media studies, literary theory, and the digital humanities:

First, my approach is manifestly *materialist*. Throughout my argument, I seek
to recover the material contexts (paper, silicon, and magnetic storage media)
that give support to higher-order social and cognitive phenomena (like
literature, text, and discourse).

Second, the book contains a strong undercurrent of *humanism*. In making
explicit the ways in which changes in the material substratum affect
higher-order cultural techniques (of knowledge production and literary
dissemination), I argue for the reinstatement of human agency in a conversation
that has largely turned towards the object, the system, and the post-human. The
book's narrative arc can be imagined as developing from first-order material
bases of textual production, to second-order phenomena, to the emergence of the
subject in the later chapters.

Finally, my work is *experimental* in that it affects history and theory
through practice. Because engineering is an evolutionary practice, contemporary
reading and writing implements contain within them traces of their early
development. This means that lines of code from software running Unix systems
in the 1970s are still in some real sense present on modern machines (like
Apple Macintosh laptops and Android phones, which run Unix-derived operating
systems). This property allows for a media archeology that can "lay bare" the
device, making good on the implied archeological metaphor: involving
excavation, surveying, and artifact discovery at the machine level.

## Existing Literature

*Plain Text* makes a theoretical intervention in the cluster of media studies-
and digital humanities-related fields that include science and technology
studies, platform studies, history of data, software and critical code studies,
and media archeology. Recent comparable books in this space include: *Paper
Knowledge*, by Lisa Gitelman (Duke University Press, 2014); *Coding Freedom:
The Ethics and Aesthetics of Hacking*, by Gabriella Coleman (Princeton
University Press, 2012); *Mechanisms: New Media and the Forensic Imagination*,
by Matthew G. Kirschenbaum (MIT Press, 2012); *Files: Law and Media
Technology*, by Cornelia Vismann (Stanford University Press, 2008); *Programmed
Visions: Software and Memory*, by Wendy Hui Kyong Chun (MIT, 2013); *How We
Think: Digital Media and Contemporary Technogenesis* by Katherine Hayles
(Chicago, 2012); *Beautiful Data: A History of Vision and Reason since 1945*,
by Orit Halpern (Duke, 2015); and several titles in the Electronic Mediations
series at Minnesota University Press, which published Lori Emerson's *Reading
Writing Interfaces* in 2014.

My work extends the research program represented in these volumes in several
important directions. While committed to broadly theoretical concerns---that
is, ideas that can guide or challenge the way we study texts, their production,
their meaning, and their impact on the people who use and produce them---my
argument also dwells in the realm of traditional philosophy and (more narrowly)
philosophy of text and technology. Furthermore, more than a decade of
professional experience in software development grounds my thought in the
fields of software and electrical engineering to an extent greater than one
generally finds in similar manuscripts. Finally, my sources may
betray academic training in comparative literature. The reader should not be
surprised to encounter original translations and texts that undercut the
preponderance of North American material.

Consider, for example, my first chapter, called "We Have Always Been Digital,"
which commences with a discussion of "digital representation" as philosophers
Nelson Goodman and John Haugeland define it in the analytic tradition. I
proceed by testing their intuitions on the basis of something called the "soap
opera effect" particular to modern liquid crystal displays (LCDs) and in the
related video-processing technique of "motion-compensated frame interpolation."
The resulting analysis clarifies the various (and often conflicting) meanings
of the word digital in media studies and in the digital humanities. In the
second chapter of the book, "Literature Down to the Pixel," I visit late
nineteenth- and early twentieth-century U.S. and European patent archives to
argue that universal Turing machines, usually viewed as computational
algorithms, should also be considered in the genealogy of communications and
text processing equipment, as devices that cannot quite shake the material
legacy of paper and pencil. Similarly, in the third chapter of the book,
"Laying Bare the Device," I take a deep dive into Russian formalist aesthetics
and resurface to examine the Hegelian legacy in the development of the
Document Object Model (DOM).

Although I do not mean to engage in the debate on disciplinary formation, I
prefer to describe my work as "computational culture studies," both in the
sense of "the study of computational culture" and as "computational approaches
to the study of culture." It is important for me to make the case for the
reciprocal motion between the constituent elements of "computation" and
"culture." Too often rhetoric around the digital humanities resembles a one-way
street, in which computational methods are promised to reform the humanities
unilaterally.

Books like Alexander Galloway's *Laruelle: Against the Digital* (University of
Minnesota Press, 2014), Matthew Fuller's *Evil Media* (MIT Press, 2012), and
Johanna Drucker's *What Is?* (Cuneiform Press, 2013) represent the sharp edge
of a critical counter-movement to digital positivism. But this response, too,
could be balanced against the constructive potential of the digital humanities,
which extend humanistic inquiry into new and exciting directions. As was the
case with the "linguistic turn" in the decades prior, almost all fields of
human knowledge are now experiencing a turn towards computational methods that
offer insights at previously unavailable scales of analysis. Witness the
emerging fields of computational biology, computational chemistry,
computational linguistics, computational geometry, computational archeology,
computational architectural design, computational philosophy, and computational
social science, among others. The impact of computation therefore cannot be
lightly dismissed. In *Plain Text*, I stake out a middle ground between Stephen
Ramsey's laudatory *Reading Machines* (University of Illinois Press, 2011) and
David Golumbia's disparaging *The Cultural Logic of Computation* (Harvard
University Press, 2009). Ultimately, I argue in favor of a transformative use
of technology in the humanities, with reciprocal effects that promise mutual
enrichment.

## Audience and Market

As is the case for most of my work, *Plain Text* appeals to several key
audiences. The first comprises media scholars interested in the history of
data and computing in the twentieth century. The second audience can be
located in textual studies, among scholars seeking to understand the impact of
technology on literary theory or book history. Finally, the manuscript
targets the broader audience of digital humanities and information science
practitioners (particularly in the field of human--computer interaction)
actively engaged in using and creating textual interfaces that shape
contemporary reading and writing praxis.

As a former software engineer and now a literary scholar, I make sure that my
research bridges the (perceived) gap between the "two cultures" of science and
the humanities. My courses at Columbia University, which include Code & Poetry
(Fall 2014), Computing in Context (Spring 2015), and Foundations of Computing
for Journalists (Summer 2014), attract a diverse body of students from various
disciplines (and particularly from departments of English, history, and
computer science). I lecture widely in language departments, in schools of
engineering, and in front of publishers, architects, artists, and librarians.
As one of the founders of Columbia's Group for Experimental Methods in the
Humanities, I encourage my students to consider technology reflectively,
combining critical theory with a measure of critical practice.

In this spirit, my group has organized workshops on online security for
activists; we have reached out to an online community of engineers to help us
write media history as a project in citizen humanities; and we are set to teach
critical making at Rikers Island in July 2015. I am inspired in these endeavors
by collaborators from Digital Humanities labs across the country and by my
colleagues in the English Department and at the Berkman Center for Internet &
Society, where I am an active faculty associate. I keep these manifold
audiences in mind as I complete *Plain Text*. The book exposes intellectual
frameworks that bolster my research and teaching activities. I write to
strengthen these projects and to give back to the community that has supported
me so generously. I hope to rely on the same goodwill and support networks in
reaching out to promote my book.

In my teaching career, I am often asked to create workshops, courses, and
certificate programs for graduate students in the humanities interested in
computational studies. These have included seminars at the Digital Humanities
Summer Institute (U. Victoria), courses for the Lede Program in the Journalism
School (Columbia), and workshops for students at CUNY Digital Praxis and in the
NYU Digital Internship Program. Texts usually assigned in courses like that are
either volumes published by technical presses for a professional audience or
theory-based readings in new media studies related only loosely to teaching the
fundamentals of computer science in context. The (optional) technical appendix
could serve to supplement the main body of the work with a series of
"experiments" that illustrate theoretical concepts in action, at the keyboard,
making the text applicable to a greater variety of educational environments
(beyond the conventional classroom).

[^ln-char]: There are many caveats here, to be explored later. Follow along
with exercises related to the discussion in the Technical Appendix.

[^ln-human]: Recent theory challenges the conceptual boundaries between humans
and machines in a concerted way. Perhaps, such boundaries were never that
clearly articulated in the first place. It is also likely that other modalities
of being are possible on the spectrum between human and machine, or human and
complex system. We will have a chance to explore these possibilities in second
half of the book. For now, I ask that the reader simply rely on the colloquial,
pre-theoretical understanding of both person and instrument.  However
intertwined the hand and the hammer can become, there is an intuitive way in
which a child can separate one from the other. A deep-rooted instinct at work
in that distinction, one that cannot and should not be dismissed as mere
naiveté. The concept of a human is in itself a powerful theoretical construct,
and, as I will argue throughout, one necessary, not only for the understanding
of key concepts in literary theory and computer science, but also in
articulating an ethics of critical computation.

[^ln-meaning]: I write "meaning" in quotation marks, because the question of
whether it makes sense to talk about meaning for artificial agents is a
question that will remain unresolved, at least until the later chapters, when
we have the chance to discuss notions of data and information as
meaning-carrying units.

[^ln2-derrida]: This is a bit of a post-structuralist caricature, but it is not
difficult to find direct sources expressing the idea. For example, see John
Caputo quoting Jacques Derrida in his *Deconstruction in a Nutshell: A
Conversation with Jacques Derrida*, "I often describe deconstruction as
something which happens. It's not purely linguistic, involving text or books.
You can deconstruct gestures, choreography. That's why I enlarged the concept
of text. Everything is a text" [@caputo_deconstruction_1996].

[^ln2-varela]: See for example @varela_autopoiesis_1974; @barthes_rustle_1989,
5; @nuttall_new_2007, 6-25.

[^ln2-survey]: I can only give anecdotal evidence here, as I often put this
question before my graduate students at the beginning of the semester, with the
reported results.

[^ln2-close]: See [@lentricchia_close_2003] and [@fish_how_2011]. 


[^ln2-internet]: The NEA study has this to say on the topic of What is
responsible for the decline of literary reading?: "If the 2002 data represent a
declining trend, it is tempting to suggest that fewer people are reading
literature and now prefer visual and audio entertainment. Again, the
data---both from SPPA and other sources---do not readily quantify this
explanation [...] the Internet, however, could have played a role. During the
time period when the literature participation rates declined, home Internet use
soared" [@nea_reading_2004, 30].


## Length and Format

I am writing the book as a traditional volume, expounding a sustained thesis
across six chapters (along with a short introduction). At this point, I am
aiming for a manuscript of around 80,000 words (not including citations),
allotting around 10,000 to 15,000 words per chapter. The chapters tend to have
five to seven more granular subsections that help to clearly demarcate chapter
structure.

The book could include an optional appendix, discussed in detail in the
Annotated Table of Contents. The appendix does not require any special
treatment. In addition, the manuscript contains 15--20 figures, primarily as
black-and-white diagrams from technical literature and images created by the
author. I have received a modest subvention to offset any costs associated with
image production, publishing, and preparation of the manuscript.

## Relationship to Dissertation and Other Published Work

The book bears a resemblance to my doctoral dissertation in the subtitle only. Several
paragraphs from the embargoed dissertation did make it into *Plain Text* in an
ad-hoc manner, but the book as a whole represents a completely new framework
and a new direction in my thinking about the subject.

With the approval of the press, I plan to place two of the book's shorter, more
peripheral chapters, in their redacted form, into journals that would help
promote and expand an audience for the book as a whole. I also plan to present
the same chapters (in an even more compact form) at several upcoming
conferences, including CHI, the Conference on Human Factors in Computing
Systems (a significant publication organ in the field of human-computer
interaction).

## Schedule to Completion

The research for this book was enabled by a year-long fellowship at the Berkman
Center for Internet & Society. Consequent to the research phase, I taught
several classes on the subject, which helped refine my thinking and provided
further notes and primary material. As of today, the manuscript stands at
roughly 60,000 words, with three chapters completed in their draft form. I am
writing actively and plan to have the first draft of the manuscript ready in the
summer of 2015. I am on leave next academic year, having cleared my schedule,
with plans of seeing this project through to publication.
