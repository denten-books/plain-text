---
title: "Plain Text: The Poetics of Human-Computer Interaction (Sample Chapters)"
subtitle: "Chapter 2: Literature Down to the Pixel"
author: "Dennis Tenen"
style: csl/chicago-note.csl
bibliography: plain-text.bib
toc: true
documentclass: article
cover-image: images/steno.png
header-includes:
- \usepackage{ftnxtra}
- \usepackage{titlesec}
- \usepackage{fancyhdr}
- \pagestyle{fancy}
- \newcommand{\sectionbreak}{\clearpage}
- \rhead{DRAFT (please do not distribute)}
- \lhead{}

---

## Chapter 1: Literature Down to the Pixel
Keywords: `literature, value, control, microanalysis`


## Method

I take two things from Latour.

### Everything is lit (connect to computational as lit)

1. Methodologically, materialism of the sort described above has naturally led me
to sociological approaches to the study of knowledge formation. In a study on
the construction of scientific knowledge, Bruno Latour and Steve Woolgar
perform what they call an ethnography of a science lab by exposing the passage
of ideas from applied experiment to literary production. For Woolgar and Latour
a laboratory functions as a factory of sorts, ingesting matter and artifact to
produce fact and inscription, beyond simplistic notions of knowledge-making and
discovery [@latour_laboratory_1986]. A cite of production. 

Why am I readin Ltour. Becaue latour gives us a way to see much lit.

Although much cited in the study of culture, science, and technology,
*Laboratory Life* has had less of an impact on the study of literature, even
though Latour and Woolgar borrowed much the other way around. In the 1986
postscript to the American edition, they remark on the broad trend in literary
theory toward treating texts as objects of interpretation, disavowing the kind
of criticism aimed at the "real meaning of texts," at "what the text says,"
"what really happened," or "what the authors intended"
[@latour_laboratory_1986, 273]. Similarly, the authors wanted to aim research
in the social study of science at the "contingent character of objectification
practices." Science, in their view, is itself a type of literary system,
"generating texts whose fate (status, value, utility, facticity) depends
subsequently on interpretation" [@latour_laboratory_1986, 273]. Throughout the
book, Woolgar and Latour use the term literature to mean something "inscribed,
printed, or published," a combination of verbs that coincides with "literature"
consistently throughout [@latour_laboratory_1986, 47-53]. Viewed as a work of
literary theory, *Laboratory Life* reifies the research program began by the
post-structuralists. Where Jacques Derrida could remark offhandedly that
"everything is a text,"[^ln2-derrida] Woolgar and Latour make concrete the
literal transformation of matter into text. No longer was literature an
illustration of something that happens in the laboratory: Latour and Woolgar
showed the laboratory itself to function as an inscription-making machine.

The conventional way to understand Woolgar and Latour brings the interpreter
into the laboratory, breaking the illusion of scientific objectivity: instead
of "discovering" facts, scientists construct them (therefore, *social
constructivism*). Paradoxically, as Latour and Woolgar worked to undermine the
empirical unity of science (turning it into a kind of hermeneutics), they
performed literary analysis as an empirical method. To "read" the laboratory,
researchers observed the space in person, embedded into the fabric of its
everyday activity. As ethnographers, Woolgar and Latour were interested in
discovering models of objectivity (and science) that emanate from the practice
doing science localized to the particular laboratory under observation. A
theory of knowledge formation therefore becomes visible through ethnographic
description. In the *Laboratory Life* the reader is asked to examines a range
of evidentiary material along with published work. Floor plans, research
samples, workflow diagrams, dialog transcriptions, time sheets, and logbooks
all become a part of the epistemology.

To view Woolgar and Latour as scholars of literature (and not just of
laboratories) is to open up the floodgates of textuality. The laboratory, the
courthouse, the hospital---these institutions cannot be reduced to text, but
the lesson of *Laboratory Life* is that they are, in part, literary systems.
Beneath every scientific discovery, every popular song, every film, radio, or
television serial, every art or architectural project, every political office,
every legal judgment, every restaurant meal, every doctor's prescription, every
website, and every machine manual---beneath that giant and perplexing pile of
human activity lies a complex network of circulating textual documentation.
Woolgar and Latour show that literary scholarship can involve more than just
reading books, but also making visible that textual undercurrent: following it
and mapping its movements across rooms and offices, all the while exposing the
mechanisms of its mutation and locomotion. It is an exciting prospect: to view
the hospital as a library, where cadres of physicians and administrators
convert human stories into diagnoses, into controlled vocabularies, into files,
into billable codes, into inscriptions, and into archives. Or to see the legal
system as a process of literary discovery, where legions of attorneys and
paralegals pore over terabytes of textual material, transforming inscription
into evidence. That is not all these institutions are, but they are also that;
and there is much work to be done there for a scholar of literature.

On an average day in 2008, at home, an average American read around 100,500
words a day. At 250 words per page, that is around 402 printed pages.  Between
the years of 1980 and 2008, the consumption of information in bytes---a measure
that would obviously privilege storage-heavy content like sound and
video---grew at a modest 5.4 percent per year. Reading, in decline until the
advent of the internet, has tripled in the same period.  Reading in print
accounted for 26 percent of verbal information consumed in
1960. That number fell to 9 percent in 2008, but the consumption of words
      digitally increased to 27 percent of total consumption, which means that
reading has increased its share of the overall household attention span
[@bohn_how_2009; @hilbert_worlds_2011; @hilbert_info_2012]. The first decade of
the twenty-first century saw a 20 percent increase in library visitation
[@u.s._institute_of_museum_and_library_services_public_2010]. According to
UNESCO data, literacy rates continue to rise globally, as the world calibrates
imbalances of access along the lines of gender and geography
[@huebler_adult_2013]. By a conservative estimate, the number of scientific
publications grows at about 4.7 percent per year, which means that the amount
of published research roughly doubles every 15 years or so, and the figures are
much higher in some fields [@archambault_welcome_2005; @crespi_empirical_2008;
@larsen_rate_2010]. The number of books published in the United States almost
tripled from 2005 to 2009 [@bowker_u.s._2009].

All measures point to a drastic expansion of the textual field. At the same
time, we are often told that reading of short stories, novels, poetry, and
plays appears to be at grave risk [stephens_rise_1998; @merrin_television_1999;
@nea_reading_2004; @golumbia_cultural_2009; @fish_digital_2012;
@marche_literature_2012].[^ln2-internet] Something does not add up. As a
society we are reading, writing, and publishing more each year. The
reconciliation of vocabularies locates the source of the perceived metaphysical
threat in the culture's profound alienation from the material contexts of
literary production. The language of literary theory is supremely tuned to deal
with symbols. Only after long reflection, on the order of centuries, do we
begin to note the sheer strangeness of the textual interface: the medieval
palimpsest, the stylus, and the goat parchment. Yet we cannot afford to care
less about the keyboard and the screen today that we do about ink and paper.  A
battle is quietly brewing for the soul of computing and hence for the control
of our basic reading and writing apparatus.

Where literacy thrives, the very nature of what constitutes a literary text (of
any kind) has changed significantly with the advent of ubiquitous computing and
its related techniques involving the internet, integrated circuits, magnetic
storage, miniaturization, and search and suggest algorithms. By many accounts,
the second half of the twentieth century saw a major technological shift
affecting the production, access, and distribution of knowledge, in a shift
comparable in its consequences to those accompanying the spread of the
Gutenberg printing press in the fifteenth century [@johns_piracy:_2009;
@kovarik_revolutions_2011]. Matters always central to the field of literary
studies remain vitally important to the functioning of modern society. Literary
studies, when conceived broadly, provide fertile grounds for collaboration
between diverse disciplines interested in the changing dynamics of narrative,
interpretation, language, form, prosody, composition, dialog, discourse, story,
genre, authorship, influence, and text. It is in this core conceptual cluster
of operational terms that I want to locate a notion of poetics capable of
addressing not only the machinations of poetry and prose, but also of pixel,
code, string, script, inscription, list, document, file, record, project, and
archive.

### Macro and micro (connect to plain and human)

Such textual proliferation comes at a cost. Until recently, the bread and
butter of literary scholarship has been close reading. Close reading, like
critical thinking, is an idea more easily performed than explained, because the
details become contentious as soon as they are formalized.[^ln2-close] I will
tread carefully, therefore, by committing myself to a broad definition of close
reading that involves some notion of deliberate and reflective practice aimed
at deep comprehension. The Partnership for Assessment of Readiness for College
and Careers (PARCC), the organization responsible for the controversial
Common Core standards being implemented in schools across the United States
today, promotes close reading as "thorough," "methodical," and "analytical"
practice that "enables students to reflect on the meanings of individual words
and sentences; the order in which sentences unfold; and the development of
ideas over the course of the text, which ultimately leads students to arrive at
an understanding of the text as a whole" [@parcc_parcc_2012, 7]. The general
movement here is from "text" on the page to "work" (text as a whole, in the
language of the report). The model of textuality implicit in the institutional
project of close reading assumes an environment of received literary canons,
naturally accessible to the human intellect. For the duration of the "Gutenberg
galaxy," the age of print, a well-educated person might have been expected to
internalize some several hundred or perhaps thousands of major texts
constituting the canon. Close reading was honed to operate on that naturalized
scale.

The expansion of the textual field has radically increased the cognitive
demands of literary engagement. The pipeline between text and work has
lengthened considerably. On the one side, the matter of canon formation can no
longer be relegated to stable, long-term systems of social filtering. Seen from
the perspective of a literary interface, the database, the social stream, and
the search engine are tools for dynamic, "on the fly," generative
canon-formation. Consider the task of finding an unknown (to me) factoid
online, about philosophy in the times of Andalusian Spain, for example. Where
in the past I might have started with a subject catalog compiled by librarians,
today I construct a search query, using resources that I believe will return a
reasonably authoritative list of texts on the subject. The search engine, in
effect, replaces (or rather complements) centuries-long processes of
canon-formation. A near-instantaneous list of results now becomes my ephemeral,
but nevertheless authoritative, collection of relevant literature.

Each text in the returned list still requires the instrumentation of close,
analytical interpretation. However, the same discipline of critical and
reflective deliberation exercised on the level of an individual text needs to
also be exercised on the level of procedurally generated search engine results:
Where to search? Using what engine? How to construct the query? What are the
implicit biases of the system?  The academic question of canon-formation
transforms into a (not yet critical) practice of rapid, iterative, generative
canon-making. Whatever ideals motivate close reading between "text" and "work"
surely must drive the process on the level of dynamic corpus composition.

Examples of macroanalysis.  The various practices of distant reading arise from
the condition in which canons are no longer accessible, in their entirety, to
the unaided (natural) human intellect. These include distant reading and
macroanalysis in literary studies [@jockers_macroanalysis_2013;
@moretti_distant_2013], culturomics in economy [@aiden_uncharted:_2014],
e-discovery in law [@scheindlin_electronic_2004; @scheindlin_electronic_2009],
automatic essay evaluation in education [@shermis_handbook_2013], and medical
informatics in medicine [@shortliffe_biomedical_2013], among others. At the
foundations of these nascent disciplines is a shared toolkit of statistical
natural language processing [@manning_foundations_1999; @jurafsky_speech_2008],
automatic summarization [@radev_centroid-based_2004; @nenkova_pyramid_2007],
machine learning [@rasmussen_gaussian_2006; @flach_machine_2012], network
analysis [@opsahl_node_2010; @szell_measuring_2010; @takhteyev_geography_2012],
and topic modeling [@wallach_topic_2006; @blei_probabilistic_2012].

*Plain Text* compliments distant reading on yet another scale of analysis. The
book is structured along a trajectory that begins at the material foundations
of modern textual technology, moving from the stratum of circuit and silicon to
reach up, through layers of abstraction (files, folders, and operating
systems), towards the reader. In following that path, I aim to make visible
material conditions of knowledge production that give rise higher order
phenomena like pixels, letters, words, books, and literature. In developing
such an approach to the study of textual phenomena, my book introduces a method
of literary microanalysis. Where distant reading perceives patterns across
large-scale corpora, microanalysis breaks literary systems down to their minute
constituent components.

![Micro, macro, and close reading.](images/micro.png)

Where distant reading and macroanalysis are concerned with text aggregates,
microanalysis, of the type suggested (but not carried out to its logical
conclusion) by Kittler and Latour and Woolgar, occupies the other side of the
text--work equation. Note that contrary to what one may intuit, distant reading
often happens prior to close reading and not the other way around. In most
cases, the reader must find the text before analyzing it. Canons, corpora,
collections, libraries, and archives are all aggregates that mediate the
process of discovery. Microanalysis, too, constitutes a study of mediation, but
on the level of physical minutiae otherwise not readily observed in cursory
exploration. The instruments of microanalysis may coincide with computational
tools, designed to find hidden patterns lurking above or beneath a given
document.[^ln2-iarkho] The micro-instrumentation might also include a
screwdriver, a binding needle, or a soldering iron: sharp tools that help to
pry open and to scrutinize otherwise magical textual black boxes.

The pre-modern model of literary transmission assumes relatively short physical
distances separating authors, texts, and readers. Were I to write these words
in front of you, on paper, I would simply pass the page into your hands. In
receiving this text, you could be fairly certain that no third party meddled in
the process of passing the note. When communicating in this way, it makes sense
to talk about notions of strong "authorial intent" and "fidelity to the
original," because authors and their works are thought to occupy the same
contiguous space and time. The advent of cheap mechanical reproduction of print
(for the sake of brevity let's say the printing press) has introduced a range
of devices that mediate between the author and the reader. Distance, time, and
mediation subsequently weaken facile notions of authorial fidelity or intent.
At the very least, we know that editorial practices, publishing markets, and
communication technologies can introduce an unintended element of noise into
the channel. At the maximal break down of the communication act, long-distance,
asynchronous communication is susceptible to "man-in-the-middle" attacks, by
which a third party maliciously alters the content of communication
[@needham_using_1978].

Changing material conditions of textual transmission push against familiar
literary critical ideas. For example, as the mechanical reproduction of print
weakens the material basis for authorship attribution, the notion of authorship
itself undergoes change. That is not say that the author is dead, as Barthes
would have it. Authors continue to live and to collect royalties from the sale
of their works. The weakening of the authorship function merely makes certain
ways of talking about things like "authorial intent" and "fidelity to the
original" difficult to sustain. Massively collaborative writing projects like
Wikipedia and procedural narrative generation (machine writing) further erode
ideas of authorial production based on individual human agency. Yet, it would
be a mistake to believe that the myth of autopoiesis (the literature that
"writes itself," writing that writes, discourse that speaks, etc.)[^ln2-varela]
can displace the myth of the author. A discipline of close attention to the
atomic particulars of encoding, transmission, storage, and the decoding of text
at the site if its application to the human condition ultimately aims to
reclaim subjective agency, in motion. This may seem strange at first: to
recover the subject in the physical minutiae of the literary--technological
encounter. Yet the point of contact is crucial, for it is here that the subject
seems to disappear, in a compressed moment of time that needs to be unpacked
with some precision.

Come back to vocab. Not the difficulty of obscurity unfamiliar. Ima guide you
through it. I care.

[^ln-sweatshop]: See @freeman_high_2000.

[^ln-witt]: For more on the connection between Wittgenstein and James
see @goodman_james_2004.

[^ln-pragma-truth]: For a more thorough discussion on the topic see
@seigfried_william_1990, @pihlstrom_structuring_1996, and @putnam_jamess_1997.

[^ln-pragmatism]: The intellectual legacy of pragmatism is wide-ranging and diffuse. It is
perhaps most pronounced in the teacher colleges, where James and Dewey are
still read widely, which could explain the ascendancy of such pedagogical terms
as "situated cognition"[@brown_situated_1988, @lave_situated_1991] and
"experiential learning"[@kolb_hegel_1981]: both terms denoting some sense of
necessary synthesis between of knowing and doing. In the field of linguistics,
philosophy of language, and communication studies, pragmatics are
well-encapsulated by the "language-as-action tradition," which harkens back to
the Oxford language philosophers like J.L. Austin, Paul Grice, and John Searle
[@trueswell_approaches_2005]. Austin's *How to Do Things with Words,* is
perhaps the paradigmatic formulation of the idea that words don't just mean
things, but that they enact change in the world.

In an ASCII-rendered plain text file, byte count corresponds to character
count.[^ln-char] Let these words soak in for a moment. I will spend the rest of
the book unpacking this idea. We will have to come to terms with what "plain
text," "ASCII," and "bytes" really represent. But for now, a common-sense
understanding of the terms should be enough to form the intuition that texts
and characters are units of meaning that make sense to humans; and that bytes,
by contrast, "make sense" for machines. Remarkably, the two modes of
meaning-making gain a measure of equivalence under the singular condition of
plain text (and even then, not always, and with many caveats).[^ln-human],
[^ln-meaning]


### 5.2 Copy Text (textual criticism)

The division of text into form and content makes a difference in more
utilitarian ways as well. For example, imagine the challenge of compiling all
known commentaries on Shakespeare's *Hamlet*, by hand or programmatically. The
problem of what edition to use as the "base" for such a project presents itself
immediately. In the words of Barbara Mowat and Paul Werstine, the editors of
*Folger Digital Texts*, "readers assume that there is a single text for the
plays: what Shakespeare wrote. But Shakespeare's plays were not published the
way modern novels or plays are today: as a single, authoritative text. In some
cases, the plays have come down to us in multiple published versions,
represented by various Quartos (Qq) and by the great collection put together by
his colleagues in 1623, called the First Folio (F). There are, for example,
three very different versions of *Hamlet*, two of *King Lear*, *Henry V*,
*Romeo and Juliet*, and others. Editors choose which version to use as their
base text, and then amend that text with words, lines or speech prefixes from
the other versions that, in their judgment, make for a better or more accurate
text" [@mowat_textual_2012]. In other words, where the readers expect to
encounter *Hamlet* as a single, unified idea, they instead encounter a
multiplicity of slightly diverging textual "witnesses." Errors in transcription
and editorial interventions accumulate. The proliferation of unlicensed copies,
translations, and imitations further adds to the complexity of the problem.

Editors of classical literature will often solve the problem of diverging
copies by designating one version as canonical, preserving the formal
characteristics of the given version down to individual line breaks, as is
often done with Platonic dialogs. It then becomes possible to collate
commentaries, connecting each to individual words and passages at their
canonical location. The problem remains in the abstract: which version to count
as canonical? The act of deciding on what constitutes an authoritative edition
can quickly become a contentious issue, particularly when the text in question
carries religious or political significance.

A whole field of textual criticism exists in the service of negotiating
"critical" editions which in some way preserve the struggle to reconstruct a
measure of authority. But what should count for being authoritative? A number
of logical possibilities find as many ardent adherents. For some, the act of
deriving the authoritative text lies in reconstructing authorial intent, using
the author's notes, for example, or from first-hand witness testimony. Others
place a premium on reconstructing and analyzing the writing process itself, in
what has been dubbed as "genetic criticism" [@deppman_genetic_2004, 1-36]. Yet
another approach will compare multiple editions of the same text to derive a
possible meta-text, preserving all extant variations. Yet others will simply
rely on their judgment to produce what they believe is a "true" text based on a
number of personal and eclectic considerations [@bowers_rationale_1950,
@bowers_shakespeares_1954, @mcgann_radiant_2004]. Such approaches to textual
criticism have at least this one thing in common: behind the fuzzy profusion of
textual material they perceive the outlines of a single, unified work. As G.
Thomas Tanselle (a prominent textual scholar) puts it, "one must be able to
distinguish the work itself from attempts to reproduce it."[^ln11-tanselle] For
a textual critic of his kind, all real-word variants and reproductions are
suspect because possibly corrupt in some way. Despite its careful attention to
the materiality of the text as artifact, textual criticism in this vein remains
a classically idealistic pursuit. The work exists as an ideal form in the realm
of the ideal.

[ln11-tanselle]:  "Whatever concept of authorship one subscribes to, the act of
reading or listening to receive a message from the past entails the effort to
discover, through the text (or texts) one is presented with, the work that lies
behind" [@tanselle_rationale_2010, 13-18.].

Restoration and Anti-restoration. Eugene-Emmanuel Viollet-le-Duc (for) and
William Morris and John Ruskin (against). Also Teoria del restaruo by Brandi.

"To restore an edifice means neither to maintain it, nor to repair it, not to
rebuild it; it means to reestablish it in a finished state, which may in fact
never have actually existed at any given time." [@price_historical_1996, 314;
orig from @viollet-le-duc_foundations_1990, 195]

"It is for all these buildings, threfore, of all times and styles, that we
plead, and call upon those who hve to deal with them to put Protection in the
place of Restoration, to stave off decay by daily care, to prop a perilous wall
or mend a leaky roof by such means as are obvioiusly mant for support or
convering, and show no pretence of other art, and othrwise to resist all
tampering with either the favric or ornament of the buildings as it stands; if
it has become 

But whose who make the change wrout in our day under the name of Restoration,
while professing to bring back a building to the best time of its hisory, have
no guide but each his own individual whim to point out to them what i s
admirable and what contemptible; while the very nature o their task compels
them to destroy something and to supply the gap by imagining what the earlier
builders should or might have done." Data munging. Irreversible changes.
@price_historical_1996, 320; orig in "William Morris, "The Principles of the
Society as Set Forth Upon its Foundation," Builder 35 August 1877.

Ruskin lamp of memory.

"It is impossible, as impossible as to raise the dead, to restore anything that
has every been great or beautiful in architecture [...] That spirit which is
given only by the hand and eye of the workman, never can be recalled. Another
spirit may be given by another time, an it is then a new building; but the
spirit of the dead workman cannot be summoned up, and commanded to direct other
hands, and other thoughts. And as for direct and simple copying, it is palpably
impossible. What copying can there be of surfaces that have been worn half an
inch down [@staniforth_historical_2013, 2]?"

"But it is said there may come a necessity for restoration! Granted. Look the
necessity full in the face and understand it on its own terms. It is a
necessity for destruction. Accept it as such, pull the building down, throw its
stones into neglected corners, make ballast of them, or mortar, if you will'
but do it honestly, and do not set up a Lie in its
place"[@staniforth_historical_2013, 3].

On the Restoration of The Circumcision by Signorelli. "At Volterra, over the
alter of a Company in the Church of S. Francesco, he painted in fresco the
Circumcision of Our Lord, which in considered beautiful to a marvel, although
the Infant, having been injured by damp, was restored by Sodoma and made much
less beautiful than before. And, in truth, it would be sometimes better to
leave works half spoilt, when they have been made by men of excellence, rather
than to have them retouched by inferior masters." 1550 Tim's Vermeer.

picture cleaning as removing as removal of "varnishes and other encrustations,
by which a paininting may be obscured, but by which it is usulaly also in some
measur protected from injury." [@bomford_issues_2004, 60] Original in the
Report from the Select Committee on the National Gallery, London 1853, vi-xi.

Colliveau set about [restoring]this picture with uncommon solenity, because it was his
own. He began with the most delicate pumice stone, and very soon got into the
walls of th building, but was astonished to find the apearance of bricks and
mortar still, although not guite soneat; he soon cleared away the wall on
discovering a room beyond, and pictures, furniture, &c. most beautiful.
Gracious powers! how was he agitated; forgetting totally the mischief he was
commiting, his elbow went like a fiddler's; every thing vanished.---However,
finding a bed, the curtains of which (a dark green) were drawn close, he went
to work upon it ver tenderly; presently a pot de chambre came  in sight!---His
whole souldwas absorbed; his face all one ghastly grin; his legs (like German
flutes) tottered under him. Not that great philosopher Guffin Ragba, was more
agitated while cutting the leather from off his bellow, to disover how wind was
make, than was Monsieru Collifeau while scroring off the bed cloaths; at last
he found a lady asleep, very beautiful, because very fat, and a Dutch woman.
Now this part I did not see, and I believe but very few besides himself; but
alas! hew was soon to be at the end of his discoveries. Having got a glimpse of
some gilding, he redoubled his efforts, when he found it was an inscription; it
was in Dutch, and nearly thus, as translated by Mr. Peter La Cave:---"Now
caitiff, meditate on th havoc though hast made throughout thy lie, and go thang
thyself. In this picture, wretch! though have destroyed, what to the end of the
world, can never be replaced." [@bomford_issues_2004, 44-45]. Originally from
Julio Caesar Ibbetson, An Accidence, or Gamut, of Painting in Oil and water
Colous . London: Darton and Harvey, 1803, 5-14.

"The sculptor wastes away th rudo block til he has accomplished the desired
form. not so the painter. he builds up his forms from a blank surface, and
hides, as he rogresses all the preliminary layers upon which the etxternal
colours depend for durability and lustre. And hence, not on the meaning and
spririt of the work must be understood; the restorer must also be familiar with
he naure of the materials and the manner of their employment.
[@bomford_issues_2004, 70]. Original from @merritt_dirt_1854, 67-72


The wound
that heals.

Just giving you a taste of restoration vs. preservation. What would


[^ln11-counter]: A documentary on counterfeit goods produced by BBC4
interviews Christophe Zimmerman, Senior Technical Officer at World Customs
Organization, who deplores the industry's lack of cooperation with law
enforcement [@bbc4_counterfeiting_2013].



We have now traveled from the pixel down to magnetic storage media and back
from the storage media to the screen. The passage opens up space between
visible content, media, and the imposed forms that govern any and all higher
notions of literary activity "floating" above this nominally "digital" layer.
Technology does not determine the literary space: it has only the potential to
hide implicit mechanisms of machine control, or, to offer possibilities for
transformation not otherwise available to other forms of textuality. Loosely
coupled to its material contexts, text can continue its relentless drive from
matter to idea and into other matters as long as its passage is not hampered by
regimes that prohibit further sharing, remixing, and transmediation. I say nothing
yet of the potential necessity of such regulation. Under certain conditions, in
the name of privacy, security, or property rights, it may become necessary to
flatten out and to treat text as more of an analog, media-bound modality of
communication, limited in its ability to move across minds and cultures. But,
it is also in our broadly human, civic interest to keep such mechanisms of
constraint visible to view, under continual scrutiny of critical, close, and
closest possible reading.

### 5.3 Authenticity (problem of copies)

It is quite odd, if you think about it, to believe that objects somehow retain
an aura or a trace of their history: "a handkerchief used by Princess Diana,"
or "the cigar box owned by President Kennedy." In some real sense, the
historical patina covering these objects is only imagined. President Kennedy's
fingerprints and the tears of Princess Diana have long disappeared from their
physical medium. Their trace cannot be reduced to "mere" physical
characteristics: that is, to actual scratches, stains, or molecules,
recoverable from the object. Rather, the object had to "be there" in some way,
acquiring a patina of authenticity. "Having been there" is what gives the
object value, sentimental or otherwise, in a condition that refuses formal
reduction to a set of merely material properties. The high price contemporary
society places on authentic artifacts is therefore a type of magical thinking.
Walter Benjamin's famous essay on the work of art in the age of mechanical
reproduction appears to both celebrate and lament the demise of such magic.
"The technique of reproduction detaches the reproduced object from the domain
of tradition," wrote Benjamin. "By making many reproductions it substitutes a
plurality of copies for a unique existence. And in permitting the reproduction
to meet the beholder or listener in his own particular situation, it
reactivates the object reproduced. These two processes lead to a tremendous
shattering of tradition which is the obverse of the contemporary crisis and
renewal of mankind" [@benjamin_work_1969, 217-253]. And yet, for Benjamin, the
easy reproducibility of modern art forms like film and photography also
"emancipates the work of art from its parasitical dependence on ritual"
[@benjamin_work_1969, 217-253]. One could read Benjamin's famous essay as a
piece of technological nostalgia, or as an expose on the magic and superstition
latent in modernist art. Crisis or emancipation? Benjamin feels both, and in
doing so he captures the metaphysical anxiety of the modern age. We desire for
our Vermeers to stay special but also for a Vermeer to hang in every home,
peering back at us from a multitude of anonymous reproductions.

The modern world faces the challenge of authenticity, which we have only began
to assimilate: politically, economically, and philosophically. The ubiquitous
holographic certificate of authenticity on a luxury handbag or on a "Digital
Versatile Disk" (how antiquated the DVD sounds already!) is a cheap band-aid
hiding the immaterial nature of these artifacts. Fashion and software companies
are reluctant to talk publicly about piracy,[^ln11-counter] because the so
called "knock-offs" are essentially exact duplicates, often "leaked" from the
very same factories producing the original.

Note that it makes no sense to talk about "what was meant" by the designer of
the handbag, the way textual critic may talk about authorial intention. Nor
would it make sense to "correct" some perceived flaw in the painting by
Vermeer. The practice parallel to "textual criticism" in visual arts is
restoration. Although similar to textual criticism in its aims to recover some
notion of the original, crucially, the practice of art restoration takes place
without reproduction. Unlike a critical volume, which essentially reprints the
work in its reconstructed entirety, restoration happens at the site. All
"unauthorized" copies constitute a forgery that competes for attention with the
original. One could say that for painting and handbags "matter is synonymous
with meaning," but the sentiment strains at the limits of critical vocabulary
in the tradition that gives us only the binaries of form and content.

The context-dependence of an artifact is not even medium-specific, as
Goodman would suggest. Texts are more allographic than handbags and paintings,
but Vermeers and Louis Vuitton purses are also in some sense perfectly
reproducible, like texts. They are texts, and I do not mean it in some
post-structuralist notion of "everything is a text."[^ln11-derrida]

Technologies like 3D scanning and printing increasingly reduce physical objects
to their textual representation, as is the case with the stereo lithographic
document (STL) format that describes objects for 3D printing.  Here, for
example, I reproduce code that describes "a surface" of a functional gun (the
code made available online by *Defense Distributed*, a self-described civic
liberty activist group):

```
22=(
BOUNDED_SURFACE()
B_SPLINE_SURFACE(1,2,((#34104,#34105,#34106),(#34107,#34108,#34109)),
 .UNSPECIFIED.,.F.,.F.,.F.)
B_SPLINE_SURFACE_WITH_KNOTS((2,2),(3,3),(15.1641909236141,15.3257778193699),
(2.99329024107099,3.14159265358979),.UNSPECIFIED.)
GEOMETRIC_REPRESENTATION_ITEM()
RATIONAL_B_SPLINE_SURFACE(((1.,0.997252058758362,1.),(1.,0.997252058758362,
1.)))
REPRESENTATION_ITEM('')
SURFACE()
);
```

This piece of code exists in its Platonic ideal *before* the physical artifact.
As costs of reproduction approach zero, so does the line between an object and
its description. Imagine a world where to think "gun" is also to seamlessly
bring one into existence. The erosion of boundaries between object and idea has
a pronounced effect on the governance of goods, felt precisely at the
distinction between the autographic and the allographic. Governments currently
regulate the manufacture and the distribution of guns as artifacts, but how
does one regulate guns as texts and ideas? Any such effort amounts to
censorship. What was first regulated under the regime of property law, now
increasingly falls under the regimes of intellectual property and rights to
speech. Where does that leave the practice of textual criticism? In such a
world there can be no distinction between works and texts. Unlike millions of
reproduced Vermeers (which no doubt occupy this world in the form of postcards,
posters, and photographs), text is text. It lies before us in glorious
flatness, as thick as a sheet of paper.

Text as I have already began to define it Chapter One, is a digital mode of
representation, defined, at least in part, by a relative lack of "stickiness"
to its physical medium. Textual artifacts therefore exhibit the dualism between
text and work to a much greater extent than other forms of art and
communication. A Vermeer is always a work. Not so with *Hamlet*. "Hamlet" may
indicate a category of relate, or, an individual instance of a given work.
Philosophers Nelson Goodman and Catherine Elgin explain this peculiarity by
making the distinction between "works of art whose identity depends on their
history of production" and works whose identity does not. The former they call
"autographic" and the latter "allographic" [@goodman_reconceptions_1988, 65].
These terms will be useful for us throughout the book. Architecture and
painting, in this view, are strongly autographic disciplines. An exact copy of
a Vermeer or of a building by Frank Lloyd Wright could never rise to the status
of the original. The originals have what Walter Benjamin called an "aura or
authenticity." No amount of precision, down to the molecular level, could
substitute for the historical patina of the original artifact.

### 5.4 Problem of Drafts

Text as vector. Barthes: text as "an organism which grows by vital
expansion"[@barthes_work_1978, 161].  "The metaphor of the Text is that of the
*network*" [@barthes_work_1978, 161].

Internet is not rhyzomatic. Mediation is at stake. " But the bottom line is
that dumb-pipe email is unmediated, and therefore it's a business that Google
wants to get out of as soon as it can."

" It's also why I believe Google will kill Gmail as soon as it comes up with a
mediated alternative everyone loves. Of course, Google may offer an antiquated
"Gmail view" as a semi-obscure alternative to the default "Inbox"-like mediated
experience."

Problem of Drafts and Versions Documents as vectors. Not completed things.
Files as cognitive scaffolding for collective memory. Vissman.

Science lecture vs. humanities. Stuff that is known. Stuff that is not known

What does it really mean to "know" something? Or to have "read" a book? I have
had the following conversation countless times. Someone asks, "Have you read
Nabokov's *Pale Fire*," and I respond, "Yes I have." But there is a world of a
difference between reading it yesterday, last week, or ten years ago. The book
as an interface leaves an organic trace in the human mind---an imprint that
begins to fade as soon as it is created.

How much knowledge is there in the world? According to a recent paper published
in Science, 295 exabytes (or billion gigabytes).[@hilbert_info_2012;]

Whatever consciousness is, it is propelled forward on a thin edge of material
substratum. Reality exists only in the now, where the past is a memory and the
future only a possibility. Imagine whispering something to a friend. It does
not have to be very complicated, something like "I love you" or "I miss you."
The whisper dissipates as soon it is uttered. The percussion of the speaker's
breath creates temporary order: giving shape and pushing air molecules into
waves of pattern and form. That order begins dissipating as soon as it is
created. Within milliseconds, molecules return to their natural state of chaos.

To steel ourselves against entropy, we change the substratum from air to stone.
It's molecules are more stable. Etched in stone will be the same message. It
will last longer now. But it will also take longer to create. You will have to
carry around your tablet and chisel. The message etched in stone will outlive
the whisper and indeed will

Repercussion? How we preserve ourselves today affects the future. Appointment
with self.

Leroi-Gourhan, André.

The book as an interface between human and ?. We must see it in the context of
interfaces.

Wikipedia as a Turing complete language. The power of versioning. Text as a
vector. The problem of annotation. What are we annotating? Annotation solutions
force a platform. Is Hamlet a platonic object? A family of related objects?
Standard English editions in Europe.



### Hyper Erudition

Writing on the state of
twenty-first century poetics Craig Dowrkin speaks about poetry's "Malthusian limit."
"Bound by discreptant rates of production and consumption, the readerly economy
of poetry in the twenty-first century cannot avoid a catastrophic calculus: the
rate of consumption quickly hits an arithmetic limit (any one person can only
read so much), but the rate of production is increasing geometrically"
[@dworkin_seja_2008, 9].

Flattening of the knowledge realm.
https://medium.com/the-physics-arxiv-blog/the-extraordinary-growing-impact-of-the-history-of-science-642022a39d67


Impact on the study of textual artifact. If are are to take the hypothesis of
extended cognition seriously, we must treat the tools of cognitive augmentation
as seriously as we would our own mental development, in the traditional sense.

The flattening of the knowledge realm. It is not the absence of topography. It
is our ability to readily perceive topography, and to place ourselves within
that discourse. The knowledge of the mechanic vs. the knowledge of a surgeon.
The surgeon should get more money because of the subjective experience of
cutting people up. That part does not transfer. The technical literature is
actually quite understandable. Incommensurability of subjective experience.
Commensurability of knowledge. The fundamental problem of modern world is the
problem of expertise.

http://word.mvps.org/faqs/general/wordvswordperfect.htm
http://wptoolbox.com/tips/MSWordToWP.html
modal vs. other kinds of processing
### 2.4 Content + Control

With the site more broadly defined, we can begin the work of recovering the
material contexts of literary production. I want to foreshadow now what will
become the punchline of the whole book: namely that the ghost in the machine
cannot be reduced to some vague notions of value---these amount to a
distraction, which lies plainly upon the surface. The conspicuous ghostly
apparition (indicating the absence of a living subject) is agency itself,
which, for now, necrotically attaches itself to dead things like systems,
mediums, and automated discourses. If all this talk of value, ghosts, and
necromancy sounds familiar, the reader will not be surprised when I advance
Karl Marx as a preeminent theorist of the algorithm and someone able to help us
reason through Kittler's computational conundrum. On towards Marx then (who
will appear much later), through the machine that, although intimately familiar
to every writer and consumer of text by touch, remains alien and alienating in
its hidden possibilities, soothing and threatening all at once.

The personal computer governing the production of textuality today emerged from
an amalgam of automated devices, chief among them the telegraph, the
typewriter, and the calculator. In his seminal 1937 paper "On Computable
Numbers," Alan Turing describes an automatic machine (*a*-machine) capable of
transposing the problem of calculability into "effective calculability." Where
all previous calculators are special-purpose mechanisms, engineered to augment
a specific type of computation, Turing thought of his device as a universal
"*a*-machine." In theory (and it was conceived as a thought experiment, at
first), it could imitate any other mechanism for calculation, because its
internal composition could be reduced to a dynamic representational state (as
opposed to a fixed, mechanical one). The device would receive input by means of
a paper ribbon containing discrete symbolic inputs. At its bare minimum, the
device would need only the ribbon, a means of "reading," and the means of
"writing" the symbols onto the tape. Mechanically, its movement could be
restricted to one axis or to the movement of the tape through the mechanism.
What makes such a device a *universal* machine is its ability to internalize
symbol as part of its mechanism. Symbolic input, in this case, can represent
computable data (letters and numbers), but it can also function as control code
("move left," "erase mark") altering the movement of the mechanism.

In fact, the Turing machine does not properly differentiate between content and
control code. Its "internal state" (Turing uses terms of sentient awareness
throughout) "is determined by *m*-configuration *q<sub>n</sub>* and the scanned
symbol," where the *m*-configurations are defined as "a finite number of
conditions" of a machine, which, in Turing's words, may be compared to "a man
in the process of computing a real number" [@turing_computable_1937, 231]. The
initial configuration plus the scanned symbol define the next internal state of
the machine. The material and the symbolic fuse into one: here lies the essence
of the universal Turing machine (UTM). Software is also hardware. In being able
to internalize configuration as symbol, the *universal* Turing machine can in
effect imitate all other special-purpose Turing machines. This transition of
symbols into machine states effectively defines modern programming. A universal
machine, unlike other, definitive, single-purpose and limited-state mechanisms
(a clock for example), contains the ability to take on differing internal
symbolic configurations. It can imitate a clock, an abacus, a scale, and, with
time, Turing thought it could imitate humans as well.

This ambiguity between hardware and software leads to some confusion in the
critical literature, as evidenced by Lev Manovich's playful response to
Kittler's "there is no software" argument. If I understand it correctly,
Kittler's short but often cited essay picks up the thread of Kittler's earlier
work to posit what he calls a "postmodern writing scene." "We do not write
anymore," writes Kittler: "human-made writing passes through microscopically
written inscriptions which, in contrast to all historical writing tools, are
able to read and write by themselves" [@kittler_there_1995]. According to this
schema, Kittler sees the paper-bound design blueprints of the first integrated
microprocessor as the last "real" piece of writing. Everything written after
that point is hardware (because software is hardware at that "microscopic"
level).

Manovich inverts Kittler's argument into "there is only software," by which he
means that in a pragmatic sense, the affordances of a given medium are
determined by software. A printed page begins to differ from a screen only when
the readers are able to effect something on the screen that they could not on
paper. To this end, Manovich encourages his readers to become active developers
of software, rather than its passive consumers [@manovich_there_2011, 274]. In
that, Manovich reasserts the possibility of writing in the silicon age. Kittler
(who passed in 2011) could perhaps object to that line of reasoning in
maintaining that chip architecture (the last written work) still determines (as
foundation) all higher levels of textuality "floating" above the silicon
bedrock. And no amount of learning to code would give an ordinary subject the
resources required to write in silicon---a process so advanced and expensive as
to be limited to a handful of international chip manufacturers. In opening a
successive nested series of black boxes, the post-silicon writer hits the
impenetrable bedrock of chip architecture. In such conditions, is it even
worthwhile to follow Manovich's call for new literacies? Is writing still
possible? If "there is no software," then no. Else if everything is software,
then one can answer to the affirmative.

This ontological complication has its roots deep in Cartesian dualism. The
question of where do brains end and minds begin remains unresolved in cognitive
science, for example. Similarly, at some imperceptible point software
disappears into hardware. But before we ourselves get lost in that liminal
space between matter and idea, let us recover a measure of oddity found in the
now ubiquitous operation of Turing machines. First, note that Turing's original
formulation happens at the level of a thought experiment. (Turing does not
begin to build actual machines until his move to Princeton in 1936.) A
universal Turing machine comes to life initially as an idea that can take on
the structure of other ideas expressed symbolically. Second, note that though
Turing describes his machine in the language of mathematics (where his most
significant contribution lies), his description also contains the bare minimum
of a mechanical device. No matter how symbolic a Turing machine aspires to be,
no matter how ascendant to the realm of the ideal, it still needs a bare
minimum of physical matter to function. And Turing's paper does contain the
canonical description of that bare physical minimum.

With the above two observations in mind, we can view abstracted universal
Turing machines, as implemented in the Wireworld universe (a cellular automaton
simulation), for example, or in Minecraft (a procedurally generated sand-box
world-exploration game), as recursive, second-order ideational constructs,
built on top of first-order physical mechanisms (a personal computer, in the
case of Wireworld and Minecraft). We know this, because all *n+* order Turing
machines are limited in computational power by the physical capabilities of
that bottom-most device (the physical machine writing the simulation). The
simulated UTM cannot outperform (in terms of cycles per second, instructions
per cycle, or its capacity to hold a number of instructions) the machine doing
the simulation. If we disregard the dizzying levels of recursion (a Turing
machine, simulating a Turing machine, simulating a Turing machine and so on),
we can begin to examine the turtle at the bottom, which has its head in the
symbolic and its feet firmly in the material world.

Literature in computer science tends to confront universal Turing machines as
algorithms: in other words, as virtual, second-order symbolic representations.
As consummate thinkers on the level of the symbol, computer scientists and
literary scholars (unlike, say, electrical engineers or book binders) rarely
need to pay heed to that strange bottom-most turtle.[^ln2-bottom] Yet it is
impossible to entirely disassociate the implementation from the idea. In his
review of Turing's "On Computable Numbers" paper, Alonzo Church, the American
mathematician whose work anticipated Turing's (independently) in several
important aspects, wrote that "a human calculator, provided with pencil and
paper and explicit instructions can be regarded as a kind of a Turing machine"
[@church_computable_1937, 42-3; also cited in @petzold_annotated_2008, 63].
Disregarding the broader, metaphysical implications of that statement, note for
now the persistence of two essential implements required for the minimally
viable operation of the Church--Turing human and machine calculators. Pen and
paper persevere and assert themselves through the
abstraction.[^ln2-abstraction]

Forced to confront the universal Turing machine *as a mechanism*, the historian
must acknowledge that it borrows from a number of extant designs, which,
together and incrementally, give the UTM its physical form. A media history of
the Turing machine as device differs from its intellectual history as symbolic,
mathematical abstraction in interesting and instructive ways.[^ln2-turing]

![Universal Turing machine as an idea. "Nick Gardner's Wireworld multiplier,
via a Turing machine."](images/turing-idea.png)

Although Turing's original article did not include any drawings, he describes
his "*a*-machine" in terms of paper tape medium, scanning, erasing, "writing
down notes," "external operators," and "moves" of the mechanism. From the
engineering standpoint, Turing's machine can be reduced (1) to the mechanism
for "reading" and "writing" symbols onto and from paper storage media, and (2)
to the paper tape storage medium itself (as seen in Mike Davey's
reconstruction). In the United States, these elements would find prior art in
mechanisms such as the "Numeral adding or subtracting attachment for
type-writing machines" [@daugherty_numeral_1894], "Combined
Type-writing and Computing Machine" [@degener_combined_1911], "Computing
Attachment for Typewriters" [@wright_computing_1914], "Computing Mechanism"
[@wright_computing_1915], and "Combined Type-writing and Adding Machine"
[@ellis_combined_1914] among others.

![Universal Turing machine as a device. Reconstructed by Mike Davey, circa
2012. Displayed in the Collection of Historical Scientific Instruments,
Harvard University. Go Ask A.L.I.C.E. Exhibition, 9/11-12/20/212. Photograph by
Rocky Acosta under CC Attribution 3.0 license.](images/half-turing.jpg)

By the end of the nineteenth century a number of lesser mechanisms anticipate
the functional elements of Turing's machine. And by 1936, when Turing
publishes his paper on computable numbers, these inventions not only anticipate
the modern computer, but are brought to mass market in the widespread
manufacture of computing scales, dial recorders, electric tabulating machines,
and computing typewriters made by companies like Underwood Computing Machine,
Electromatic, and International Business Machines (IBM). Rather than a single
eureka moment, the invention of the universal machine should be viewed as a
gradual historical process that culminates in Turing's universal (and minimally
viable) specifications.

When viewed in the context of literary, book, and textual history (instead of
the history of mathematics, for example), the universal Turing machine signifies a
culmination of several broad, long-term trends that begin with the invention of
writing itself. Since much has been written on the first two of these periods,
I will outline them briefly with the intention of elucidating the third in
greater detail:

1. The mechanization of type and the accompanying normalization of language
begins with the diffusion of movable type in China and Europe, circa 1040
(China) and 1450 (Germany) [@mcluhan_gutenberg_1962; @he_diffusion_1994;
@needham_shorter_1994, 1-34; @febvre_coming_2010]. With the invention of
movable type and the typewriter, the variability of hand-written script can be
normalized to a limited set of discrete characters. The process of
normalization continues today as contemporary technologies like natural
language processing and optical character recognition struggle to bring
non-regularized writing systems under the computational umbrella.

2. The development of distant writing (telegraphy) and remote communications,
which, although extant in many early societies (as smoke and mirror signals,
for example) accelerates dramatically at the beginning of the nineteenth
century [@shaffner_telegraph_1859; @beauchamp_history_2001;
@standage_victorian_2014]. When combined with the mechanization of type,
telegraphy amplifies the range and the speed of geographical displacement
enabled by the circulation of printed matter. The telegraph further weakens the
authorial function in transposing writing from its immediate physical
environment across vast geographical distances. The advent of telecommunication
lengthens the chain of technological mediation between author and reader. In
the absence of the identifying "hand," the telegraph clerk's and the censor's
mark cannot be distinguished from the author's (a common literary plot device
in the fiction of the period). The notion of a telegram's "fidelity" therefore
becomes an attribute of the communication channel as a whole (instead of an
attribute attached to authorial intent).

3. Finally, the rise of Turing machines in the late nineteenth and early
twentieth centuries elevates print from an intelligence-conveying medium to one
of communication and control. "Content" intended for humans is now routinely
mixed with "control codes" intended to alter the operation of the receiving
device.

It is this last and still current moment of historical development that is
least theorized. A dominant tradition in the history of the human sciences
follows Louis Althusser and Michel Foucault (and, to a lesser extent, Jacques
Derrida)[^ln2-derr] in analyzing mechanisms of structure and control in terms
of ideology, discourse, and representation. Where for Althusser "every science
is a science of ideology,"[^ln2-alt] Foucault posits his archeology of the
human sciences on the level of "positive unconscious," revealing unspoken
"rules" that govern the formation of "theories, concepts, and objects of study"
on the level of "scientific representation" [@foucault_order_1994, xi].

There are a number of theoretical niceties here (that the reader may skip, if
not interested in the history of Marxist thought). For example, the concept of
ideology, in the orthodox Marxist schema of things, exists as a "political
superstructure [...] to which correspond definite forms of social
consciousness." In the Marxist model, ideology floats *above* the material
base---the dominant mode of production---that "determines the general character
of the social, political and spiritual processes of life"
[@marx_contribution_1904, 11]. Althusser's influential contribution to this
world view was to insist that ideology, although secondary to the mode of
production, nevertheless assures its continual reproduction as a means of
continuing subjugation and political control. Simply put, for Althusser, the
ideological apparatus of the modern capitalist state ends up being "installed
in the dominant [social] position" [@althusser_lenin_2001, 85-127]. In a
similar (although not equivalent) rhetorical move, Foucault envisions that
beneath the "epistemological level" of human activity there exists an
"archaeological layer," where representation "governs the mode of being of
language, individuals, nature, and need itself" [@foucault_order_1994, 209].
Ideology and representation, once determined, now determine. The rhetorical
jujitsu inverts Hegel yet again, to put Marx back on his head.[^ln2-flip] My
short history of the universal Turing machine runs counter to this top-heavy
tradition of reading Marx in the inverse. As a device, the Turing machine fuses
control and communication. Control is meant here bluntly, as physical circuit
control, at the level of the mechanism and not at the attenuated level of
ideology or representation.

A number of inventions at the end of the nineteenth century pertain
specifically to "circuit-controlling devices controlled by a traveling
perforated strip or tape" [@cuttriss_telegraphy_1893]. Prior to perforated
tape, the transmission of messages by telegraph required the presence of a
skilled operator, able to transcribe messages from text to Morse code, and into
the physical motion of a lever-operated circuit. In this system, the human
operator acted as a mute interface between text and machine. The transcription
of text into signal, and back onto paper, required the real-time presence of
human encoders and decoders. The perforated tape decoupled the human from the
machine. In US1187035 (1916) on "Telegraphy" Albert and Ralph Bumstead explain,
"the object of our invention is to provide a system of telegraphy which does
not require skilled operators for the transmission and reception of messages"
[@bumstead_telegraphy_1916]. Instead, the message was transcribed into
perforation via mechanical means and then fed into the device.  The tape
mechanics of the typewriter could then be coupled with the electrics of the
telegraph, with perforated tape acting as a mediator between the two "worlds"
of mechanics and electricity. A number of devices emerged at the time with the
aim of transforming the mechanical action of the typewriter into perforation,
and, consequently, perforation into script, completing the circuit between
automated "encoding" and "decoding." As one device converts human input into
mechanical states, and into signal, another device converts the signal into
mechanical states and into human-legible messages.

What began as a trickle at the end of the nineteenth century ended in a flood
at the beginning of the twentieth. A multitude of devices capitalized on the
control capabilities of removable storage media. These included machines for
tape-controlled telegraphic transmission [@wheatstone_improvement_1874;
@murray_tape-controlled_1905; @bumstead_telegraphy_1916], tape-controlled
printing [@creed_printing_1911], printing telegraphs
[@hallden_printing-telegraph_1929], and remote broadcast programming devices
for radio and television content [@vriendt_program_1934; @brown_automatic_1936;
@brown_selective_1936]. With the invention of punch cards and perforated tape
(also used in textile looms, as early as 1725), a message meant for another
human became also a physical medium---bumps and holes---used to animate the
mechanical movement of the transmission apparatus.

For example, of the 33 asserted claims in the Bumstead brothers' patent, the
first 13 relate to the "transmission of intelligence [...] adapted to initiate
a succession of electrical impulses all of which have a character representing
significance, a receiver adapted to detect variations in time intervals
elapsing between successive impulses, a plurality of interpreting relays
selectively actuated by said receiver, and a printed mechanism responsive for
the combined action" [@bumstead_telegraphy_1916, 12-13]. Up to this point, the
patent describes a device for transmission information, from type to print.
Starting with clause 14, the language changes to describe a more general
"telegraph system," involving "an impulse transmitter having means to vary the
time intervals between successive transmitted impulses, each time interval
having a character *representing significance*, a receiver responsive to said
devices, and signal, *distributing*, *interpreting*, and recording devices
responsive to the contiguous significant time intervals defined by the impulses
[emphasis mine]" [@bumstead_telegraphy_1916, 14]. For the automated telegraph
the control code and the message are one: that is, the arrangement of
perforation on the ticker tape affects the internal mechanical configuration of
the machine. Another way to put this would be to say that the state of the
tape-driven telegraph machine at any given time is also, in part, a message
meant for the human recipient.  The printing telegraph of Brothers Bumstead,
along with their subsequents and antecedents, contain all the necessary parts
for a Turing machine: discrete symbolic language, removable storage medium, and
a device that can alter its internal state based on the reading and writing
passing symbols.

By 1905, Murray (of the Murray telegraph and alphabet) can write that "if we
disregard the small class of telegrams that merely express emotions, *the
essence of telegraphy is control* [emphasis mine]." He goes on to write that
"telegraph systems, therefore, belong not to the class of producing or
distributing, but to the class of controlling mechanisms"
[@murray_setting_1905, 556].

With this history in mind, I am interested in reformulating the metaphysical
anxiety about literature and value formation in terms of text and control. If
you are reading these words on a screen, my message has reached you through a
long chain of transformations: from the mechanical action of my keyboard, to
the arrangement of electrons on magnetic storage media, to the modulation of
fiber-optic signal, to the shimmer of the flowing liquid crystal. Many, many
third parties were involved in that act of communication. And some part of that
message was used to control the electrical circuits of the device in your lap,
in your hand, near to your eye, embedded, or embodied. Close reading limited to
the parsing of content and representation risks missing the machinations of
naked circuit control. It looks like you are reading a book, but this book
changes its internals as it receives instructions to inform and to control. For
now I ask only, Who controls the device? But were a book also a pill or fused
with the neural circuitry of the brain, would you know what and whom you were
reading?

### 2.5 Fleas

Plain text + fleas from the Unix book should go here.

[^ln2-gurevich]: Kittler mistakingly attributes "Algorithms in the World of
Bounded Resources" to Brosl Hasslacher. The author is rather Yuri Gurevich,
Principle Researcher at Microsoft Research and then a professor at the
University of Michigan. Hasslacher's essay entitled "Beyond the Turing Machine"
appeared in the same volume of collected essays, @herken_universal_1988.

[^ln2-bottom]: For example, in the Open Systems Interconnection (OSI) model of
communication, the top-most layer of protocols and interface method is called
the "application layer" and the bottom-most layer the "physical layer"
[@peterson_computer_2007, 26-28]. Timothy Colburn and Gary Shute describe it as
being "responsible for encoding bits onto a transmission medium, whether wires,
fiber optics, or radio broadcast, in ways that maximize the transmission rate
and minimize sensitivity to noise [@colburn_abstraction_2007, 181].

[^ln2-abstraction]: This is a topic of some contention in the literature. In
his influential paper on the topic, James Moor includes the immateriality of
software as one of the "three myths" of computer science. "As a practical
matter, what we regard as computer instructions, and consequently what we
regard as computer programs, is determined by computers available," he writes
[@moor_three_1978, 215]. Nurbay Irmark argues that software is instead a purely
abstract artifact, akin to a musical work [@irmak_software_2012]. See also
@turner_programming_2013; @colburn_software_1999.

[^ln2-turing]: The intellectual history of the Turing machine is well
established, in multiple works on the subject. It follows the Greek Diophantus,
René Descartes, Georg Cantor, David Hilbert, Gottlob Frege, Bertrand Russell,
Kurt Gödel, Ludwig Wittgenstein [@petzold_annotated_2008;
@herken_universal_1988; @grattan-guinness_development_1981].

[^ln2-alt]: "We have to think (in a completely novel way) the relation between
a science and the ideology [...] the fact that such an investigation confronts
us with the observation that every science, in the relationship it has with
ideology it emerged from, can only be thought as a 'science of ideology, would
disconcert us, were we not forewarned of the name of the *object* of knowledge,
which can only exist in the form of ideology" [@althusser_reproduction_2014,
46].

[^ln2-derr]: See @derrida_writing_1978. I am alluding particularly to
statements like "ethnology-like any science-comes about within the element of
discourse," and "this moment was that in which language invaded the universal
problematic; that in which, in the absence of a center or origin, everything
became discourse-provided we can agree on this word-that is to say, when
everything became a system where the central signified, the original or
transcendental signified, is never absolutely present outside a system of
differences. The absence of the transcendental signified extends the domain and
the interplay of signification ad infinitum" (278-294).

[^ln2-flip]: There is a long-standing joke in Marxist literature that involves
flipping Hegel, who prioritized the transcendent spiritual over the physical
and material forms of life, over "back to his feet." See for example
@marx_marx-engels_1978: "The form of wood, for instance, is altered, by making
a table out of it. Yet, for all that, the table continues to be that common,
every-day thing, wood. But, so soon as it steps forth as a commodity, it is
changes into something transcendent. It not only stands with its feed on the
ground, but, in relation to all other commodities, it stands on its head, and
evolves out of its wooden brain grotesque ideas, far more wonderful than
'table-turning' ever was" (320). See also @engels_ludwig_1941: "Thereby the
dialectic of the concept itself became merely the conscious reflex of the
dialectical motion of the real world and the dialectic of Hegel was placed upon
its head; or rather, turned off its head, on which it was standing before, and
placed on its feet again" (44).



### 3.7 Spooky Action at a Distance

Book as cultural conserve (@moreno_who_1953, 238). "Many of the robots have also the
attribute in common of being able to affect human beings or other targets 'at a
distance,' a book a radio or a television sender an entertain or teach at a
distance, like a gun, a rocket and an atomic bomb can kill people and destroy
at a distance. The book is a robot par excellence."

IBM Format

By 1932 the ITU documents list two new standards, International Telegraphic
Alphabet No. 1 (ITA-1) and International Telegraphic Alphabet No. 2 (ITA-2).
The ITA formats, like the Baudot, represented latin characthers in a system of
fixed-lenght "bit" codes, For example, in ITA-1 the letter "A" and the number
"1" are both represented by `-++++`; "X" and "," by `+-++-`, and the "error"
code by `+++--` [@itu_telegraph_1932, 34]. Because the devices are not
synchronized, the pause between each transmitted character can vary in length.
For this reason ITA-2 includes two extra bits for indicating "start" and stop"
elements. The letter "A" and a hyphen in ITA-2 would be encoded as `0110001`,
where ones and zeros represent the absence and the presence of
current[@itu_telegraph_1932, 36].

By the 1960, multiple competing character encodings existed in the United
States and globally.

ASCII.
Plain text in Unix. Human readability. TROFF.

Further separation. On benefits of computer-aided document preparation. "The
style or format of the document can be decoupled from its content; the only
format control information that need be embedded is that describing textual
categories and boundaries, such as titles, section headings, paragraphs, and
the like [@kernighan_unix_1978, 2116]

Plain and fancy Text Unix system. What you see is not what you get. What is
plain text. Unix ideas of plain text. ASCII. From form and content to content,
semantic markup, and typesetting. Semantic markup as part of the
extra-linguistic meaning making.

http://www.unicode.org/reports/tr29/ The concept of Grapheme Clusters.

Semantic markup is interesting because it contains both material and ideal. The
way textuality is encoded mediates between idea and matter. Mediation. Visible
form and hidden form.

The theory of preaching. Lectures on homiletics (1892). Plain text vs. obscure
text.o

John Charles Ryle
Expository Thoughts on the Gospels: For Family and Private Use. With the ...

"One plain text is worth a thousand arguments."


Words: Their Use and Abuse
Say plain things in a plain way.

Time and truth reconciling the moral and religious world to Shakespeare: the
 By B. S. Naylor
"Thus it is, that the plain text, the obvious meaning, of Shakespeare, is
dostorted by Commentators and beclouded by Illustrators" (p64)

"One plain text is worth a thousand"

Signal Book, United States Army: 1916


"Robots have also the attribute in common of being able to affect human beings
or other targets 'at a distance,' a book, a radio or a television sender can
entertain or teach at a distance, like a gun, a rocket and an atomic bomb can
kill people and destroy at a distance. The book is a robot par
excellence"[@moreno_who_1953, 238]. So wrote Jacob Moreno, a contemporary
adversary of Sigmund Freud and one of the pioneers in sociology and group
therapy.

Zoomatics --- the relationship of man to man and the relation of man "to certain
peculiar products of his mind, which when separated from him, can function
independently." "Aggression coming from robots." [@moreno_who_1953, 237] The
first is sociometry. The second is "creative revolution." Robots = cultural
conserves and machines. 238. "A book, a film, an atomic bomb, they do not
perish in the human sense, the same capacity is always there, they can be
reproduced ad infinitum. A book may have to be reprinted, a film copied on
and off, but if anything perishes it is not their essence but some material
entourage. Our human world is increasingly filled with robots." 239

A text is a special type of a gun. It can be reduced to textual description,
transmitted, and manufactured cheaply. As the pendulum of textual scholarship
swings back from treating text as pure idea, the materialists must acknowledge
the aberrant specificity of textual material. Text routinely separates from its
physical medium. The ability to change form is what gives it the ability to act
at a distance.

### 4.5 Short, Swift, and Secret (Steno)

stenography Engelbart's dreams.

Transcription of speech was a major concern for the various stenographic
movements in the English-speaking world, which go back the publication of
Timothy Bright's Characterie: An Arte of Shorte, Swifte Secrete Writing by
Character in 1588. In the introduction to this slim volume, Bright writes that
his work was inspired by the "speedie kind of writing by character" mentioned
first in Plutarch's Life of Cato the Younger, improved upon by Seneca, and then
lost to history.1 

"The uses and diuers: short, that a swifte hande may therewith write orations
or publike actions of speach, uttered as becometh the grauitie of such actions,
verbatim. Secrete, as no kinde of wryting like, and herein (besides other
properties) excelling the wryting by letters, and alphabet, in that, nations of
strange languages, may hereby communicate their meaning together in writing,
through of sundrie tongues, it is reported of the of the people of China, that
they have no other kinde, and so traffike together many Prouinces of that
kingdom, ignorant one of an others speach. Their characters are very long, and
harde to make, that a dousen of mine, may be written and soone as one of
theirs; besides, they wanting an alphabet, fal into an infinite number, which
is a thing that greatlie chargeth memory, and may discourage the learner.2

The task of stenography then, from its early modern inception was first to
bring handwriting closer to the speed of speech, and second, to imagine a
universal alphabet that could be used by speakers of all languages and
dialects. In modern terms, we might say that stenography was concerned first
and foremost with our ability to cope with an increasing amount of information,
and second with the international standardization of character encoding (that
is, with the universal transmission of information). In The Elements of
Tachygraphy, first published in 1869, David Philips Lindsley writes: Though we
depend upon facilities to aid in mental and moral advancement, as much as in
physical, yet we have been slow to apply to the mental and moral elevation of
the race the principles that have enabled us to develop our material resources.
Inventions to aid in the commerce of ideas are no less essential than those
which we have realized in the interchange of coarser commodities. The art of
writing was the original lever by which the race was at first raised above
barbarism. Successive improvements in this art have marked the epochs of the
greatest mental activity.3

In this program we thus see the birth of the intellect augmentation movement of
the mid-twentieth century. The contemporary Unicode universal character
encoding standard, used for the representation of text for computer processing
is the natural development of this program. The unicode.org website mirrors
Bright's Characterie in defining Unicode as "a consistent way of encoding
multilingual plain text," which "brings order to a chaotic state of affairs
that has made it difficult to exchange text files internationally."4

By the late 19th century stenography existed as a full-blown international
movement concerned with universal spelling reform. John Westby-Gibson's The
Bibliography of Shorthand contains no fewer than twenty volumes on the subject
in the first half of the 17th century, thirty-six titles in the second half,
fifty-six titles in the 18th century, and more than four thousand entries for
the 19th century. This publishing activity was supported by the proliferation
of stenographic societies across France, Germany, England, and the United
States—many of these with regional chapters and publication organs.
Westby-Gibson himself was editor of Shorthand; President of The Shorthand
Society from 1886 to 18887; honorary member of the Shorthand Writers'
association, London Branch; honorary member of the Manchester Shorthand
Writers' Association; honorary Secretary and founder of the International
Shorthand Congress, 1887; and finally, Chairman-elect of the Historical and
Literary Section of the International Shorthand Congress.5 The meetings of
these societies were often accompanied by public speeches and poem recitations,
dully taken down by the stenographers in attendance. The 1843 issue of the
Phonotypic Journal, supported by Isaac Pitman's Phonographic Institution
contains the following several poems from the Nottingham and the Birmingham
Phonetic Festivals: From the proceedings of the

The stenographers of the 19th century understood their movement as the kind of
a global phenomenon that approximates our contemporary understanding of the
internet and social networking. Again Lindsley from The Elements of
Tachygraphy: When Tachygraphy shall be generally known and used, an electric
thrill of life will run through our communities, awakening new sympathies, and
forming bonds of union long dissevered. What railroads have done in bringing
friends together that could otherwise never interchange a visit during a
lifetime, brief writing will do, in bringing minds together that would, without
it, communicate with too much difficulty to be able to continue the
acquaintance begun in youth; and aid in extending that more valuable
interchange of thought among those of similar tastes, which tends both to the
perfection of our knowledge of science, and its general diffusion.6

In this project stenography and typewriting technologies compete for the same
vision of augmenting the natural human facilities of information processing and
knowledge dissemination.7 Stenography is understood here as a thoroughly modern
and modernizing technology that does for the mental and moral development what
the steam engine and areal flight did for the material realm.







### 6.2 Cryptography and Steganography

Hobbes on secret knowledge (p43-44!)

Where cryptography is the art of secret communication, steganography is the art
of hiding the secret. "The purpose of steganography is to hide the very
presence of communication by embedding messages into innocuous-looking objects,
such as digital images," Jessica Fridrich and her colleagues write for the
proceedings of the Society of Photographic Instrumentation Engineers,
appropriately shortened to SPIE [@fridrich_searching_2004].

The real danger comes not from a metaphysical source, but from the superficial
similarity of print and pixel. Where the pen or the typewriter inscribe the
image directly into their paper medium, digital type passes through a series of
transformations between the keystroke and its corresponding pulse of the
cathode ray, or in the flow of liquid crystal. Secrecy and surveillance
technologies that rightly worry Kittler exist in the gap between text as medium
and text as content. They threaten literature (the something to be said) only
in so far as the mechanisms of literary production (how it is said) remain
hidden from the critical view. When the veil is lifted, we can begin to reclaim
the passage of textuality. In this process, technology remains a relatively
neutral conduit, enabling liberation and oppression alike. The physical control
of expression cannot and should not be reduced to something as seemingly
self-contingent as "the conditions of high technology." Systems of control
naturally obscure the pathways of agency. Letting go of agency as a critical
concept plays into the hand of those (human agents) that benefit directly from
the illusion of disempowerment. I view "control" in that sense not as an
ideology, but as the ability to physically alter the material conditions of
literary production, dissemination, and consumption. The transition from
mechanical typewriter to electrical word processor precisely couples code as
content and code as control.  ### 7.1 ITU

Related to secrecy.  Plain language and Secret language (telegraph regulations
1932).  "Plain language is that which presents an intelligible meaning in one
or more of the languages authorized for international telegraph correspondence,
each word and each expression having the meaning normally assigned to it in the
language to which it belongs" [@itu_telegraph_1932, 12]. "Code language is
composed either of artificial words, or of real words not used in the meaning
normally assigned to them in the language to which they belong and consequently
not forming intelligeble phrases" [@itu_telegraph_1932, 12].

Encrypted literature. Revealment and dissimulation. Notions of textuality as
embroiled in contemporary ideas of privacy, secrecy, and transparency.
Computation does not necessarily work for the military-industrial apparatus (as
argued by Golumbia, Lennon, and McPherson).  Recovering and preserving
textuality in computing.

Brooks Act 1965.  Man in the middle attacks.

Alice, Bob, Caryl.
http://www.computerworld.com/article/2838775/why-google-wants-to-replace-gmail.html
Communication theory of secrecy systems, shannon

Zimmermann responded by publishing the source code as a book, an act that was
constitutionally protected as free speech. 
### 5.1 Copy Machine

We often think of literary works as completed products. *To the Lighthouse,*
*Hamlet,* *Moby Dick* seem to circulate as complete, fixed entities. Textual
critics, in charge of creating a work out of a multiplicity of texts, remind us
that the work is a carefully-crafted illusion. The text multiplies, always
threatening to undermine the unity of the work. Think of something substantial
you have written recently: a paper or a proposal perhaps.

"Computer assisted tracing of text evolution"

Goodman's "Art and Authenticity." Benjamin. Immanence and transcendence.
Goodman: thinking about the perfect copy. But there is a confusion here: fake
art is not necessarily about a copy, it is about provenance. Autographic vs.
allographic art. Whether forgery is possible. Digital vs. analog art. Analog
art is medium bound. Digital abstracts from the material.

Duplicates in Computer Science. Almost 30% of webpages are "very similar to
other pages" [@fetterly_evolution_2003].

It is a commonplace sentiment now that the printing press brought with it the
Protestant Reformation [@dittmar_information_2011, @febvre_coming_2010,
@mcluhan_gutenberg_1962, @eisenberg_word_1992]. Movable type is responsible for
at least a part of that story. The mechanization of print is what turned
writing from an autographic into a supremely allographic art. The Gutenberg
press helped place a copy of the bible into the hands of every ordinary
believer. That meant also minimizing the role of the priest as a privileged
interpreter. Without books in hand Martin Luther could not have written "all
who are in the church are priests" [@hillerbrand_protestant_2009]. Movable type
lifted text from rarefied media of parchment into that of the reified and
disposable leaflet. But until well into the middle of the 20th century, the
printing press remained in the realm of big technology: expensive, centralized,
unwieldy, and relatively easy to suppress. The liberatory potential of media
independence comes into its full fruition after the age of the typewriter.
After a historically brief flowering in the wild, our daily textual practice is
once again in danger of returning to controlled, media-bound, and
tightly-regulated conditions of existence.

Several important developments in the twentieth century aided in the liberation
of text from its material contexts. The first is the copy machine.  It is an
entirely unglamorous job to ponder the phenomenology of reading from the point
of view of a photocopier. Yet the impact of the humble Ditto and Xerox machines
has only recently began to get the scholarly attention it deserves. In the
United States, the copy machine placed a smaller version of Gutenberg press
into every office. And it allowed for the flourishing of a vibrant samizdat
(self-publishing) scene, usually connected to underground culture movements
without access to conventional publishing: punks, skinheads, computer- and
game- geeks, feminists, fans of science fiction and queer-core music
[@klanten_behind_2011, @piepmeier_girl_2009, @duncombe_notes_2008].

The word "xerox" firmly entered the English lexicon by the
mid-1960s[^ln11-xerox] along with stencil duplicators, mimeograph machines,
microfiche, spirit duplicators,[^ln11-spirit] hectographs, and dot-matrix
printing. A whole industry of photocopying still exists to seamlessly peel the
content away from one material medium and plaster it into another.

From the "point of view" of a photocopier, the text exists only as pure form.
The electro-photographic process at the basis of modern photocopying (invented
in the 1930s, but with patents dating back to the middle of the 19th century)
describes documents in terms of "electric charge patterns," "photoemissive
layers," and "sheet surfaces"  [@carlson_electron_1940,
@carlson_electrophotography_1942, @carlson_graphic_1953]. The photocopier is a
device for total graphesis and extreme surface reading.[^ln11-ooo]. There is no
word or content for the photocopier: only paper and image. The photocopier's
"job", if you will, is to lift a picture from one page and to impress it into
another. It does so by temporarily imaging text (as picture) onto a
photo-sensitive drum. Ink powder sticks to the electrically charged surfaces of
the drum in correspondence to the shapes on the page. The shapes are then
rolled onto a new page, re-inscribing the image into the new medium. The text
is never immaterial. But for a moment it finds shape as an invisible electric
charge. The transformation should not be reduced to some sort of mystical,
metaphysical moment. On the contrary, in instrumental terms we observe the
relatively mundane process of media independence.

Let's linger on that moment of remediation. The process of textual
transcription by hand involves a similar transformation. When a scribe copies a
page from the bible, for example, he also "lifts" content from paper.  Not
unlike a photocopier, the human stores text in the electro-chemical pathways of
the brain, before reproducing it once again into another medium. And like a
photocopier, the human can perform the copy function mechanically, without
comprehension and on a purely formal, visual level. In fact, the historical
record is full of errors that suggest that pre-modern scribes wrote at widely
varying levels of literacy [@goudsmit_illiterate_1974,
@bucking_training_2007].[^ln11-literacy] In the Muslim tradition, the prophet
Muhammad himself is sometimes called "the unlettered prophet," to emphasize
fidelity and lack of mediation in the transcription of the message
[@calder_ummi_1990, @gunther_muhammad_2002, 1-26]. In other words, the Qur'an
claim to authenticity relies on the prophet's ability to copy the message
"verbatim." On the conservative Muslim view, anything but a faithful
transcription of the original text amounts to an interpretation, not
self-identical to the letter of the text (a translation, for example)
[@tibawi_is_1962, @fatani_translation_2005, @wilson_first_2009].[^ln11-quran]

Where the photocopier placed a printing press into every office, the printer
placed one on every desk. Falling cost of print.

Finally, the text exists on the screen and magnetic media only, where the cost
of reproduction approaches zero.

### Works Cited
