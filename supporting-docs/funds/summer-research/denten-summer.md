---
title: "Humanities and Social Sciences Junior Faculty Summer Research Support
Program (Application)"
author: Dennis Tenen

---

To Whom it May Concern:

I hereby request summer support in the amount of $3,000 to further progress on
my first book, entitled *Plain Text: The Poetics of Human-Computer
Interaction.* The funds will be used to defray costs associated with research,
copy editing, and manuscript preparation.

## Theme and Argument

Plain text is a file format containing nothing but the "pure sequence of
character codes." In technical terms, it stands in opposition to *fancy text*:
"text representation consisting of plain text plus added information."[^ln-uni]
The book unfolds a history of and an argument for plain text. In making my
case, I argue that "other information" routinely embedded in all forms of
contemporary textuality includes much more than instructions for "font size"
and "paragraph justification." Increasingly, devices that mediate literary
activity encode specific models of governance and control.

The central concern of the book is to dispel the illusion of verisimilitude
between text on paper and text on the screen. The words may look the same, but
the underlying material affordances of the medium differ in significant
regards. For a literary scholar, that means analysis cannot be confined to
reading for meaning alone. How will close reading persist, when the reading
device reconfigures a text dynamically: to fit individual taste, mood, or
politics?  The formulation of this narrowly literary-theoretical concern opens
the way to the broader question of empowered human--computer interaction.
Textual legibility, as I will argue, is crucial to a critical understanding of
what it means to remain human in a digital world.

[^ln-uni]: The Unicode Consortium. *The Unicode Standard: Worldwide Character
Encoding*, Version 1.0, Volume 1. Reading, Mass.: Addison-Wesley, 1990.

The book is structured along a trajectory that begins at the material
foundations of modern computing, moving from the level of circuit and silicon,
and reaching up, through levels of abstraction (files, folders, and operating
systems), towards the reader. In following that path, I reconstruct the passage
of an electron from the mechanical action of the keyboard, to magnetic storage
medium, and to liquid crystal, that together give rise to pixels, letters, and
words. In this sense, my book introduces a method of literary microanalysis.
Where distant reading perceives patterns across large-scale literary copora,
microanalysis breaks textuality down to its minute constituent components. It
is at this level, that I find that readers and writers are in danger of
becoming fundamentally alienated from the immediate material contexts of
knowledge production.

## Annotated Table of Contents

### Chapter 1: We Have Always Been Digital

Discourse around the digital humanities needs a robust sense of the digital.
My first chapter begins with popular intuitions about the "look and feel" of
digital aesthetics. Here, I find that sometimes, the adjective carries the
connotation of "discrete," while at other times, it is used to mean something
more fluid and continuous, past the point of human perception. A discussion of
Liquid Crystal Display technology (LCD) flows into a section that deals with
digital representation from the perspective of analytic philosophy and through
aesthetics of Nelson Goodman. My summary of that tradition reveals that
language and text are already in some sense "born digital," that is
"reproducible" and "differentiated" throughout. Furthermore, digitality depends
on "reliable processes of copying and preservation"---attributes that can mean
something different to a philosopher than to a librarian. From these insights I
take it that "being digital" is not an intrinsic ontological condition, but
rather structure imposed from without. The chapter ends with a history of
Morse, Hughes, and Baudot character encodings, in which the dichotomy between
digital and analog is superseded by the distinction between binary and plain
text formats.

### Chapter 2: Literature Down to the Pixel

Having established the grounds for digital textuality in the history of
character encoding, I begin the work of moving from "low-level" first-order
concepts such as "text" and "code" up to second-order concepts such as
"literature" and "canon." The chapter starts by developing a theory of
"microanalysis," the closest possible kind of reading that pays attention to
the material contexts of knowledge production. I argue here that the concern
with value in literary criticism detracts from the machinations of naked
circuit control embedded into contemporary reading and writing devices. Unlike
scholars in the Foucauldian tradition (who often trace the machinations of
power through discourse, on the level of representation), I concentrate my
analysis on mechanisms of control at the material roots of literary practice.
In presenting a media history through primary sources on early development of
Turing machines, I show the explicit admixture of content and code: one meant
to communicate messages to humans and the other to program universal machines.
I conclude by arguing that Turing machines were anticipated not by the Babbage
calculator alone, but also through a series of advances in communications, word
processing, and media storage. A notion of text (as opposed to number) is hence
"baked into" the system.

### Chapter 3: Laying Bare the Device

The chapter begins by outlining a recent discussion on surface reading. I ask:
What lies beneath the text? The question leads into a discussion about the
differences between form and content: two theoretical concepts crucial to
understanding the structure of plain text and binary file formats. I find that,
going back to Plato and Hegel, "form" was at times used to indicate physical
structure and at other times to indicate immaterial categories in the ideal
realm. The chapter's middle section takes up the notion of "ephemeral"
textuality. A case study in removable storage (like ticker tape and floppy
disks) elucidates the movement of text from human-legible inscription on page
(and punch card) to an inscription (in the form of electrical charge) embedded
into magnetic media and literally invisible to the naked human eye. The case
study unfolds the distinction between print (in which matter, form, and content
lie flat) and screen (where the three layers occupy physically distinct layers,
providing only the illusion of flattened textuality). The discussion concludes
with a stark image, visually illustrating the difference between plain text and
binary formats. To produce the image, I inject malicious code into an Adobe
Acrobat file (`.pdf`), showing the contrast between surface text and its
underlying magnetically bound bit structure.

### Chapter 4: Media are Not the Message

In this chapter I make two significant interventions in media studies and
literary theory. First, I argue that media should not be confused with
messages. Starting with the (sometimes oversimplified) legacy of Marshall
McLuhan, I examine several models of communication, including ones proposed by
semioticians (Charles Sanders Peirce, Ferdinand de Saussure, Roman Jakobson),
engineers (Harry Nyquist, Claude Shannon), and animal anthropologists (Jakob
von Uexk√ºll, Gregory Bateson, and Thomas Sebok). Corresponding to the mediating
role of "form" in the previous chapters, I find that the "mode" of the written
communication act does something akin to "attuning" the reader (receiver) to
the encoding of the message (and to the corresponding cultural techniques of
apprehension). At the center of the chapter is a case study based on a
real-world text "written" by a troupe of Sulawesi macaques as part of an art
project at Britain's Paignton Zoo. (The text was subsequently published under
the title *Notes towards the Complete Works of Shakespeare*, Vivaria Press,
2002.) The possibility of a randomly generated zoo-text points to the
conclusion of the chapter, containing an argument against a "systems"
definition of information. In what Shannon calls a "strange feature" of this
communication theory, information is defined as the amount of "freedom" or
entropy in the system. By contrast, I want to insist on an agency-based model
of information processing, in which texts mean things *for* free and conscious
beings.

### Chapter 5: Recursive Encounters with Oneself

This chapter begins with a close reading of Beckett's *Krapp's Last Tape*. The
title character makes yearly audio recordings of himself, only to revisit them
and to enter into a sort of a dialog with his own voice from the past. I posit
this encounter with the archive as Krapp's "media being" and suggest that such
encounters are commonplace, through similar practices of depositing "snapshots"
of one's consciousness into files, bookshelves, and folders. Sartre's idea of
an "appointment with oneself" helps to see this external structure of files and
folders as cognitive extension, in need of delicate pruning and arrangement.
Documents, in this light, are shown to exist not as completed works, but as
"vectors" that mutate and move through time and space. Building on the
communication models described in the last chapter, I ask: What is being
externalized, communicated, and preserved?  And answer: It is not simply a
message, but the subject itself.

### Chapter 6: Bad Links

If documents are vectors, where do they terminate? In this chapter I examine
three answers, given at three distinct moments in recent literary history.
First, I recall the discourse surrounding structuralist "intertextuality"---the
idea that textual meaning is always created in relation to another text.
Second, I review the promises and the failures of "hypertext," an idea which
gained prevalence in literary studies with the advent of the internet. Finally,
I reflect on the current moment, in which "network analysis," a technique that
seeks to visualize linkages between texts, is being held up by some as the next
step in the evolution of textual studies. In all three of these methodological
moments, I find a similar premise of emergence: the notion that order appears
spontaneously as aggregate result of simple interactions at the level of the
system. I take the occasion of examining the hyperlinked essays of Gwern (a
mysterious contemporary "researcher, self-experimenter, and writer") to
criticize what I call the "systems view" of literature, which elevates networks
to the status of ethical and aesthetic actors.

### Chapter 7: Engineering for Dissent

In this final chapter, I argue for the recovery and the preservation of plain
textuality in the day-to-day practice of modern computing. Returning to the
history of the `.txt` file format, I find that early documents from the
International Telecommunication Union archive display unease with encrypted,
non-human-readable formats of information exchange. A theoretical treatment of
technological skepticism (from Marx, Martin Heidegger, to Lewis Mumford)
concludes with a discussion about a subject's role in actively shaping material
conditions of media being. As documents reflecting externalized states of
consciousness become increasingly transparent, they are susceptible to
increased surveillance and control. Plain text allows political subjects to
decouple externalized mental states from mechanisms of governance.  (In other
words, to decide actively when to opt in and when to opt out.) This affordance
is not, however, a deterministic property of literature, the internet, or any
other information exchange system. Rather, the design of complex systems must
itself become critical practice, which, in complement to critical theory, can
actively engineer for textual mechanisms that make individual dissent possible.


## Field Significance

The book seeks to redress a weakness in the field of digital humanities,
particularly at the point of its relevance to literary studies. Scholarship at
the intersection of these two fields is sometimes criticized for being
ahistorical or atheoretical, abandoning deep traditions of literary theory and
criticism, even where such traditions would help bolster the case for the
digital humanities. The nominally related field of new media studies has the
opposite problem. Although theoretically sophisticated, it sometimes produces
research far removed from the actual practice of creating new media (the
archetypal example given by Katherine Hayles is that of a contemporary
photography critic not familiar with the use of the "layers" tool in
Photoshop). By contrast, I situate "Plain Text" at the intersection of theory
and practice: somewhere between "technical literacy for new media studies" and
"philosophical bases for computing in the humanities."

## Schedule to Completion and Budget Rationale

As it stands, the book manuscript stands at three completed chapters. I propose
to use the summer funds to advance research and to aid in manuscript
preparation for the next two chapters.

The projected budget is as follows:

1. $1,500 to offset expenses associated with hiring a part-time graduate
student assistant, at the rate of 10hr/week at $25/hr (6 weeks).

2. $1,500 for professional copy editing and manuscript-preparation services.
