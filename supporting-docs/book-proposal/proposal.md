---
title: "Plain Text: The Poetics of Human--computer Interaction (Book Proposal)"
author: Dennis Tenen
---
\newpage

## Theme and Argument

Plain text is a file format and a frame of mind. As a file format, it contains
nothing but a "pure sequence of character codes." In technical terms, it stands
in opposition to *fancy text*: "text representation consisting of plain text
plus added information."[^ln-uni] As a frame of mind, it indicates a preference
for human-readable input and output streams of data, for computational tools
that are simple and elegant, and for open systems that are transparent to the
user. The book unfolds a history of and an argument for plain text. In making
my case, I argue that "other information" routinely embedded in all forms of
contemporary textuality includes much more than instructions for "font size"
and "paragraph justification." Increasingly, devices that mediate literary
activity encode specific models of governance and control.

The central concern of the book is to dispel the illusion of verisimilitude
between text on paper and text on the screen. The words may look the same, but
the underlying material affordances of the medium differ in significant
details. As an obvious example, consider a news report that changes slightly
based on the reader's location; or an e-book reader device that highlights
popular passages. For a literary scholar, such instability of medium means
analysis cannot be confined to reading for meaning alone. How will close
reading persist, when the reading device reconfigures a text dynamically: to
fit individual taste, mood, or politics? The formulation of this narrowly
literary-theoretical concern leads to the broader question of empowered
human--computer interaction. Building on the work of scholars like Jerome
Mcgann, Wendy Hui Kyong Chun, Katherine Hayles, Matthew Kirschenbaum, Lori
Emerson, Lisa Gitelman, and Johanna Drucker, I contend that textual legibility
is becoming increasingly crucial to a critical understanding of what it means
to remain human in a digital world.

Software developers, graphic designers, system administrators, and project
managers routinely architect technologies that have deep cultural significance,
affecting a range of cultural practices: from the ways we relate to our family
and friends to the formation of shared cultural archives. Because such
"cultural techniques" are formative of our culture, supposedly technical
decisions like choosing a text editor, a filing system, or a social networking
platform cannot be adequately addressed in shallow instrumental terms limited
to efficacy, speed, or performance. A secondary aim of this volume is therefore
to convince computer "users" to view their computational environments as a
literary system of sorts. I mean a "literary system" differently to what one
might conventionally mistake for a "binary" or "digital" one, however imprecise
those terms are in everyday use. In clarifying usage, I ask those who may have
considered themselves mere "users" to become close readers, thinkers, and
makers of technology, able to apply the same critical acuity to reading code
and device as they do to the close reading of prose and poetry. Ultimately, the
book makes a case for the recovery of textual roots already latent in the
mechanisms of modern computing.

[^ln-uni]: The Unicode Consortium. *The Unicode Standard: Worldwide Character
Encoding*, Version 1.0, Volume 1. Reading, Mass.: Addison-Wesley, 1990.

The book is structured along a trajectory that begins at the material
foundations of modern textual technology, moving from the stratum of circuit
and silicon, and reaches up, through layers of abstraction (files, folders, and
operating systems), towards the reader. In following that path, I reconstruct
the passage of an electron from the mechanical action of the keyboard, to
magnetic storage medium, and to liquid crystal, which, together, give rise to
pixels, letters, words, books, and literature. In using this approach, my book
introduces a method of textual microanalysis. Where distant reading perceives
patterns across large-scale literary corpora, microanalysis breaks textuality
down to its minute constituent components. It is at this level that I find that
readers and writers are in danger of becoming fundamentally alienated from the
immediate material contexts of their knowledge production.

## Annotated Table of Contents

### Chapter 1: We Have Always Been Digital

Discourse around the digital humanities needs a robust sense of the digital. My
first chapter begins with popular intuitions about the "look and feel" of
digital aesthetics. Here, I find that sometimes the adjective carries the
connotation of "discrete," while at other times, it is used to mean something
more fluid and continuous, past the point of human perception. A discussion of
Liquid Crystal Display technology (LCD) flows into a section that deals with
digital representation from the perspective of analytic philosophy and through
the aesthetics of Nelson Goodman. My summary of that tradition reveals that
language and text are already in some sense "born digital," that is
"reproducible" and "differentiated" throughout. Furthermore, digitality depends
on "reliable processes of copying and preservation"---attributes that can mean
something different to a philosopher than to a librarian. From these insights I
take it that "being digital" is not an intrinsic ontological condition, but
rather a structure imposed from without. Case studies from the history of
telegraphy illustrate the concluding discussion on the nature of binary and
plain text formats, in a distinction that supersedes the dichotomy between
analog and digital media.

### Chapter 2: Literature Down to the Pixel

Having established the grounds for digital textuality in the history of
character encoding, I begin the work of moving from first-order concepts such
as "text" and "code" up to second-order concepts such as "file," "folder," and
"document." The chapter starts by developing a theory of "microanalysis," the
closest possible kind of reading that pays attention to the material contexts
of knowledge production. I argue here that the concern with value in literary
criticism detracts from the machinations of naked circuit control embedded into
the contemporary text apparatus. Unlike scholars in the Foucauldian tradition
(who often trace the machinations of power through discourse, on the level of
representation), I concentrate my analysis on mechanisms of control at the
material roots of literary practice. In presenting a media history through
primary sources on the early development of Turing machines, I show the
explicit admixture of content and code: one meant to communicate messages to
humans and the other to program universal machines. I conclude by arguing that
Turing machines were anticipated not by the Babbage calculator alone, but also
through a series of advances in communications, word processing, and media
storage. A notion of text (as opposed to number) is hence "baked into" the
system.

### Chapter 3: Laying Bare the Device

At the heart of the book and central to its argument, Chapter 3 begins by
outlining a recent discussion on surface reading. I ask: What lies beneath the
text, literally? The question leads to the common distinction between form and
content. Here, I find that, going back to the Russian formalist reception of
Hegelian aesthetics, "form" was at times used to indicate concrete shape and at
times to indicate abstract universals, such as technique and formula. A case study in
removable storage---like ticker tape and floppy disks---elucidates the movement
of text from human-legible inscription on the page and punch card to magnetic
inscription invisible to the naked human eye, complicating the formalist
vocabulary. The case study unfolds the distinction between print, in which
matter, form, and content lie flat, and screen, where the three layers occupy
physically distinct strata of the Document--Object model, providing only the
illusion of flattened textuality. The apparent immateriality of digital text
brings promise of epistemological (social) and even phenomenological (personal)
transformation. But it also comes at a cost. Inscription on magnetic tape
cannot be assumed to correspond to the composite screen image. Forms of
governance like Digital Rights Management can now be embedded deep within the
structure of the "data object" itself and further hidden from view, precluding,
and sometimes making illegal outright, the possibility of interpretation (of
any sort). The discussion concludes with a stark image, illustrating the
contrast between screen surface and the underlying bit structure. To produce
the image, I use reverse-engineering tools to inject malicious code into an
Adobe Acrobat file (`.pdf`). The deformed text threatens to damage the literary
device. A thick description of the literary device, now as gadget or
instrument, brings legibility to the fore of reading ethics.

### Chapter 4: Recursive Encounters with Oneself

This chapter begins with a close reading of Beckett's *Krapp's Last Tape*. The
title character makes yearly audio recordings of himself, only to revisit them
and to enter into a sort of dialog with his own voice from the past. I posit
this encounter with the archive as Krapp's "media being" and suggest that such
encounters are commonplace, through similar practices of depositing "snapshots"
of one's consciousness into files, bookshelves, and folders. Sartre's idea of
an "appointment with oneself" helps us see this external structure of files,
folders, and library furniture as cognitive extension, in need of delicate
pruning and arrangement. Documents, in this light, are shown to exist not as
completed works, but as "vectors" that mutate and move through time and space.
Pushing off the communication model offered by Claude Shannon, I ask: What is
being externalized, communicated, and preserved? And answer: It is not simply a
message, but the subject itself.

### Chapter 5: Bad Links

If documents are vectors, where do they terminate? In this chapter I examine
three answers, given at three distinct moments in recent literary history.
First, I recall the discourse surrounding structuralist "intertextuality"---the
idea that textual meaning is always created in relation to another text.
Second, I review the promises and the failures of "hypertext," an idea which
gained prevalence in literary studies with the advent of the internet. Third, I
reflect on the current moment, in which "network analysis," a technique that
seeks to visualize linkages between texts, is being held up by some as the next
step in the evolution of textual studies. In all three of these methodological
moments, I find a similar premise of emergence: the notion that order appears
spontaneously as an aggregate result of simple interactions at the level of the
system. I take the occasion of examining the hyperlinked essays of Gwern (a
mysterious contemporary "researcher, self-experimenter, and writer") to further
criticize what I call the "systems view" of literature, which elevates networks
to the status of ethical and aesthetic actors.

### Chapter 6: Engineering for Dissent

In this final chapter, I argue for the recovery and the preservation of plain
textuality in the day-to-day practice of modern computing. Returning to the
history of the `.txt` file format, I find that early documents from the
International Telecommunication Union archive display unease with encrypted,
non-human-readable formats of information exchange. A theoretical treatment of
technological skepticism (from Karl Marx and Martin Heidegger to Lewis Mumford)
concludes with a discussion about a subject's role in actively shaping material
conditions of media being. As documents reflecting externalized states of
consciousness become increasingly transparent, they are susceptible to
increased surveillance and control. Plain text allows political subjects to
decouple externalized mental states from mechanisms of governance. (In other
words, to decide actively when to opt in and when to opt out.) This affordance
is not, however, a deterministic property of literature, the internet, or any
other information exchange system. Rather, the design of complex systems must
itself become critical practice, which, in complement to critical theory, can
actively engineer for textual mechanisms that make individual dissent possible.

### Tech Appendix (optional)

The book assumes no prior technical expertise. It can be read sequentially as a
conventional piece of scholarship in literary/textual theory or new media
studies. But, because much of the book deals with conditions of textuality
extant and recoverable from modern computing devices, I propose to heed the
call of scholars like Jerome McGann and Wendy Chun for the advancement of
theory through practice. To this end, I envision an optional appendix that can
exist on paper or as a companion website, creating an "augmented reading
environment." The appendix would follow each chapter (sequentially) with a
series of experiments at the "command line," a powerful text-based way of
interacting with the computer.

Inspired by the ethos found in Kenneth Ward Church's classic "Unix for Poets"
and by the form of Roland Barthes's seminal *S/Z*, the appendix gives readers
an opportunity to test theoretical intuitions found in the body of the book
against the reality of contemporary computation. For example, the difference
between binary and plain text formats (discussed in the early chapters) could
be made more apparent in comparing the output of `cat file.txt` and `cat
file.pdf` in the terminal.[^ln-cat] In the later chapters, the conversation on
access could be augmented with an exercise on file permissions, where utilities
like `ping` and `traceroute` would be brought to bear on network effects
mentioned in the "Bad Links" chapter. In this way, the appendix can serve to
extend historical and theoretical awareness into practical know-how. An
intuitive understanding of the political issues surrounding digital text, be
they "open access," "freedom of information," or "online censorship," begins to
develop at that instrumental level.

Ready-made tools and graphical interfaces for human-computer interaction often
obscure the underlying complexity of the computational environment. For
example, while writing a relatively complex piece of code, a journalist in my
digital humanities class once confessed to being confused about the
relationship between files and folders. *Plain Text* is a book *about* files
and folders: it is about textuality as encoded in specific ways on machines
that have a shared material history. The book's technical appendix, although
not required for the comprehension of its main ideas, would help cultivate
theoretical intuitions based not on speculation alone but also on "knowledge at
hand."

[^ln-cat]: Use actual file names if you plan to test this out.

## Field Significance

The book seeks to redress a weakness in the field of digital humanities,
particularly at the point of its relevance to literary studies. Scholarship at
the intersection of these two fields is sometimes criticized for being
ahistorical or atheoretical, abandoning deep traditions of literary theory and
criticism, even where such traditions would help bolster the case for the
digital humanities. The nominally related field of new media studies has the
opposite problem. Although theoretically sophisticated, it sometimes produces
research far removed from the actual practice of creating new media (the
archetypal example given by Katherine Hayles is that of a contemporary
photography critic not familiar with the use of the "layers" tool in
Photoshop). By contrast, I situate *Plain Text* at the intersection of theory
and practice: somewhere between "technical literacy for new media studies" and
"philosophical bases for computing in the humanities."

In pursuing the above strategy I make several decisive contributions to the
fields of media studies, literary theory, and the digital humanities:

First, my approach is manifestly *materialist*. Throughout my argument, I seek
to recover the material contexts (paper, silicon, and magnetic storage media)
that give support to higher-order social and cognitive phenomena (like
literature, text, and discourse).

Second, the book contains a strong undercurrent of *humanism*. In making
explicit the ways in which changes in the material substratum affect
higher-order cultural techniques (of knowledge production and literary
dissemination), I argue for the reinstatement of human agency in a conversation
that has largely turned towards the object, the system, and the post-human. The
book's narrative arc can be imagined as developing from first-order material
bases of textual production, to second-order phenomena, to the emergence of the
subject in the latter chapters.

Finally, my work is *experimental* in that it affects history and theory
through practice. Because engineering is an evolutionary practice, contemporary
reading and writing implements contain within them traces of their early
development. This means that lines of code from software running Unix systems
in the 1970s are still in some real sense present on modern machines (like
Apple Macintosh laptops and Android phones, which run Unix-derived operating
systems). This property allows for a media archeology that can "lay bare" the
device, making good on the implied archeological metaphor: involving
excavation, surveying, and artifact discovery, at the machine level.

## Existing Literature

*Plain Text* makes a theoretical intervention in the cluster of media studies-
and digital humanities-related fields that include science and technology
studies, platform studies, history of data, software and critical code studies,
and media archeology. Recent comparable books in this space include: *Paper
Knowledge*, by Lisa Gitelman (Duke University Press, 2014); *Coding Freedom:
The Ethics and Aesthetics of Hacking*, by Gabriella Coleman (Princeton
University Press, 2012); *Mechanisms: New Media and the Forensic Imagination*,
by Matthew G. Kirschenbaum (MIT Press, 2012); *Files: Law and Media
Technology*, by Cornelia Vismann (Stanford University Press, 2008); *Programmed
Visions: Software and Memory*, by Wendy Hui Kyong Chun (MIT, 2013); *Beautiful
Data: A History of Vision and Reason since 1945*, by Orit Halpern (Duke, 2015);
and several titles in the Electronic Mediations series at Minnesota University
Press, which published Lori Emerson's *Reading Writing Interfaces* in 2014.

My work extends the research program represented in these volumes in several
important directions. While committed to broadly theoretical concerns---that
is, ideas that can guide or challenge the way we study texts, their production,
their meaning, and their impact on the people who use and produce them---my
argument also dwells in the realm of traditional philosophy and (more narrowly)
philosophy of text and technology. Furthermore, more than a decade of
professional experience in software development grounds my thought in the
fields of software and electrical engineering to an extent greater than one
would generally expect to find in similar manuscripts. Finally, my sources may
betray academic training in comparative literature. The reader should not be
surprised to encounter original translations and texts that undercut the
preponderance of North American material.

Consider, for example, my first chapter, called "We Have Always Been Digital,"
which commences with a discussion of "digital representation" as philosophers
Nelson Goodman and John Haugeland define it in the analytic tradition. I
proceed by testing their intuitions on the basis of something called the "soap
opera effect" particular to modern liquid crystal displays (LCDs) and in the
related video-processing technique of "motion-compensated frame interpolation."
The resulting analysis clarifies the various (and often conflicting) meanings
of the word digital in media studies and in the digital humanities. In the
second chapter of the book, "Literature Down to the Pixel," I visit late
nineteenth- and early twentieth-century U.S. and European patent archives to
argue that universal Turing machines, usually viewed as computational
algorithms, should also be considered in the genealogy of communications and
text processing equipment, as devices that cannot quite shake the material
legacy of paper and pencil. Similarly, in the third chapter of the book,
"Laying Bare the Device," I take a deep dive into Russian formalist aesthetics
and resurface to examine the Hegelian legacy in the development of the
Document--Object Model (DOM).

Although I do not mean to engage in the debate on disciplinary formation, I
prefer to describe my work as "computational culture studies," both in the
sense of "the study of computational culture" and as "computational approaches
to the study of culture." It is important for me to make the case for the
reciprocal motion between the constituent elements of "computation" and
"culture." Too often rhetoric around the digital humanities resembles a one-way
street, in which computational methods are promised to reform the humanities
unilaterally.

Books like Alexander Galloway's *Laruelle: Against the Digital* (University of
Minnesota Press, 2014), Matthew Fuller's *Evil Media* (MIT Press, 2012), and
Johanna Drucker's *What Is?* (Cuneiform Press, 2013) represent the sharp edge
of a critical counter-movement to digital positivism. But this response, too,
could be balanced against the constructive potential of the digital humanities,
which extend humanistic inquiry into new and exciting directions. As was the
case with the "linguistic turn" in the decades prior, almost all fields of
human knowledge are now experiencing a turn towards computational methods that
offer insights at previously unavailable scales of analysis. (Witness the
emerging fields of computational biology, computational chemistry,
computational linguistics, computational geometry, computational archeology,
computational architectural design, computational philosophy, computational
social science, and the list goes on.) In *Plain Text*, I stake out a middle
ground between Stephen Ramsey's laudatory *Reading Machines* (University of
Illinois Press, 2011) and David Golumbia's disparaging *The Cultural Logic of
Computation* (Harvard University Press, 2009). Ultimately, I argue in favor of
a transformative use of technology in the humanities, with reciprocal effects
that promise mutual enrichment.

## Audience and Market

As is the case for most of my work, *Plain Text* appeals to several key
audiences. The *first* comprises media scholars interested in the history of
data and computing in the twentieth century. The *second* audience can be
located in textual studies, among scholars seeking to understand the impact of
technology on literary theory or book history. *Finally*, the manuscript
targets the broader audience of digital humanities and information science
practitioners (particularly in the field of human--computer interaction)
actively engaged in using and creating textual interfaces that shape
contemporary reading and writing praxis.

As a former software engineer and now a literary scholar, I make sure that my
research bridges the (perceived) gap between the "two cultures" of science and
the humanities. My courses at Columbia University, which include Code & Poetry
(Fall 2014), Computing in Context (Spring 2015), and Foundations of Computing
for Journalists (Summer 2014), attract a diverse body of students from various
disciplines (and particularly from departments of English, history, and
computer science). I lecture widely in language departments, in schools of
engineering, and in front of publishers, architects, artists, and librarians.
As one of the founders of Columbia's Group for Experimental Methods in the
Humanities, I encourage my students to consider technology reflectively,
combining critical theory with a measure of critical practice.

In this spirit, my group has organized workshops on online security for
activists; we have reached out to an online community of engineers to help us
write media history as a project in citizen humanities; and we are set to teach
critical making at Rikers Island in July 2015. I am inspired in these endeavors
by collaborators from Digital Humanities labs across the country and by my
colleagues in the English Department and at the Berkman Center for Internet &
Society, where I am an active faculty associate. I keep these manifold
audiences in mind as I complete *Plain Text*. The book exposes intellectual
frameworks that bolster my research and teaching activities. I write to
strengthen these projects and to give back to the community that has supported
me so generously. I hope to rely on the same goodwill and support networks in
reaching out to promote my book.

In my teaching career, I am often asked to create workshops, courses, and
certificate programs for graduate students in the humanities interested in
computational studies. These have included seminars at the Digital Humanities
Summer Institute (U. Victoria), courses for the Lede Program in the Journalism
School (Columbia), and workshops for students at CUNY Digital Praxis and in the
NYU Digital Internship Program. Texts usually assigned in courses like that are
either volumes published by technical presses for a professional audience or
theory-based readings in new media studies related only loosely to teaching the
fundamentals of computer science in context. The (optional) technical appendix
could serve to supplement the main body of the work with a series of
"experiments" that illustrate theoretical concepts in action, at the keyboard,
making the text applicable to a greater variety of educational environments
(beyond the conventional classroom).

## Length and Format

I am writing the book as a traditional volume, expounding a sustained thesis
across six chapters (along with a short introduction). At this point, I am
aiming for a manuscript of around 80,000 words (not including citations),
allotting around 10,000 to 15,000 words per chapter. The chapters tend to have
five to seven more granular subsections that help to clearly demarcate chapter
structure.

The book could include an optional appendix, discussed in detail in the
Annotated Table of Contents. The appendix does not require any special
treatment. In addition, the manuscript contains 15--20 figures, primarily as
black-and-white diagrams from technical literature and images created by the
author. I have received a modest subvention to offset any costs associated with
image production, publishing, and preparation of the manuscript.

## Relationship to Dissertation and Other Published Work

The book bears a resemblance to my doctoral dissertation in the subtitle only. Several
paragraphs from the embargoed dissertation did make it into *Plain Text* in an
ad-hoc manner, but the book as a whole represents a completely new framework
and a new direction in my thinking about the subject.

With the approval of the press, I plan to place two of the book's shorter, more
peripheral chapters, in their redacted form, into journals that would help
promote and expand an audience for the book as a whole. I also plan to present
the same chapters (in an even more compact form) at several upcoming
conferences, including CHI, the Conference on Human Factors in Computing
Systems (a significant publication organ in the field of human-computer
interaction).

## Schedule to Completion

The research for this book was enabled by a year-long fellowship at the Berkman
Center for Internet & Society. Consequent to the research phase, I taught
several classes on the subject, which helped refine my thinking and provided
further notes and primary material. As of today, the manuscript stands at
roughly 60,000 words, with three chapters completed in their draft form. I am
writing actively and plan to have a first draft of the manuscript ready in the
summer of 2015. I am on leave next academic year, having cleared my schedule,
with plans of seeing this project through to publication.
