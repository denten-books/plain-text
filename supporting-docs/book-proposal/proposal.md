---
title: "Plain Text: Poetics of Human-computer Interaction (Book Proposal)"
author: Dennis Tenen

---

\newpage

*Plain Text* occupies the space between literary theory and the digital
humanities, challenging received notions of digital being through the media
history of a singular file format (```.txt```).

# Theme and Argument

Plain text is a file format and a frame of mind. A fundamental concept in the
development of computing, plain text stands in opposition to closed platforms,
rarefied knowledge, and black-box devices. Instead, it offers a vision of data
that is human-readable by design: portable, concise, and universal. This book
contains an argument for plain text. In doing so, it seeks to convene a
community interested in reflecting critically on the ideas, tools, and
practices that shape our daily encounter with computation. My argument starts
with foundational principles at the intersection of media theory and
information science. I ask: what is at stake in the difference between digital
and analog media?  What contains more information a novel or a block of wood?
What separates meaning, form, and formatting? How do pixels form into texts?
Where does data end and meta-data begin? To what extent media determine the
message? The formulation of these broadly theoretical concerns about the
poetics of human-computer interaction opens the way to a discussion about the
social impact of textual technology, as it relates to applied dynamics of
online agency, deliberation, consensus-making, and dissent.

A secondary aim of this volume is to convince computer "users" to view their
computational environments as a literary system of sorts. I mean a "literary
system" in opposition to what one might conventionally mistake for a "binary"
or "digital" one, however imprecise those terms are in everyday use. In
clarifying usage, I will ask mere "users" to become close readers, thinkers,
and makers of technology, able to apply the same critical acuity to reading
code and platform as they do to close reading of prose and poetry. Ultimately,
the book makes a case for the recovery of textual roots latent in the
mechanisms of modern computing.

# Field Significance

The book aims to redress a weakness in the field of Digital Humanities,
particularly as it relates to literary studies. Much scholarship in this space
is often criticized for being ahistorical or atheoretical, abandoning the deep
traditions of literary theory and criticism, even where such traditions would
help bolster the case for emerging scholarship. The nominally related field of
New Media Studies has the opposite problem. Although theoretically
sophisticated, it sometimes produces research far removed from the actual
practice of creating new media (the archetypal example given by Katherine
Hayles is one of a contemporary photography critic not familiar with the use of
the "layers" tool in Photoshop). By contrast, I situate *Plain Text* at the
intersection of theory and practice: somewhere between "technical literacy for
new media studies" and "philosophical bases for computing in the humanities."

# Existing Literature

*Plain Text* makes a theoretical intervention in the cluster of media studies-
and digital humanities-related fields that include Science and Technology
Studies, Platform Studies, Critical Code Studies, and Media Archeology.
Recent comparable books in these fields include *Paper Knowledge* by Lisa
Gitelman published by Duke UP in 2014, *Coding Freedom: The Ethics and
Aesthetics of Hacking*, by Gabriella Coleman from Princeton UP in 2012,
*Mechanisms: New Media and the Forensic Imagination* by Matthew G. Kirschenbaum
from MIT Press in 2012, *Files: Law and Media Technology* by Cornelia Vismann
from Stanford UP in 2008 and several titles in the *Electronic Mediations*
series at Minnesota UP, which published Lori Emerson's *Reading Writing
Interfaces* in 2014.

My work differs from these volumes in that it is both more philosophical
(rather than strictly "theoretical") and more grounded in the fields of
software an electrical engineering. To give you an example of what I mean,
consider my third chapter, "Phenomenology of a Photocopier." It begins with the
discussion of the Hegelian distinction between "form" and "content," ending
with a case study from the history of word processing, in a technical
explanation of the way "content" and "form" are encoded into modern HTML and
Markdown documents. Similarly, my first chapter, "We Have Always Been Digital,"
begins with a discussion of "digital representation" as philosophers Nelson
Goodman and John Haugeland define it in the analytic tradition. I proceed by
testing their intuitions on the basis of something called the "Soap Opera
Effect" particular to modern Liquid Crystal Displays (LCDs) and in the related
video-processing technique of "motion-compensated frame interpolation." The
resulting analysis clarifies the various (and often conflicting) meanings of
the word "digital" in Media Studies and in the Digital Humanities.

Complimentary to both of these fields, I prefer to describe my work as
"computational culture studies," understood in two ways: first as the study of
"computational culture," and, second, as computational approaches to "culture
studies." *Plain Text* falls firmly into the first category, with some elements
of the book edging on light computational methods (particularly if you consider
the proposed digital companion, discussed shortly).

Although I do not mean to engage in the debate on disciplinary formation, it is
important for me to insist on the reciprocal motion between the constituent
elements of "computation" and "culture." Too often rhetoric around the digital
humanities resembles a one-way street, in which computational methods are
promised to reform the humanities unilaterally. Books like Alexander Galloway's
*Laruelle: Against the Digital* (UMinn Press, 2014), Matthew Fuller's *Evil
Media* (MIT, 2012) and Johanna Drucker's *What Is?* (Cuneiform Press, 2013)
represent the beginning of a critical counter-movement. But this response too
must be balanced against emendatory potential of the Digital Humanities
program. As was the case with the "linguistic turn" in the decades prior,
almost all fields of human knowledge are now experiencing a turn towards
computational methods, which offer new insight at previously unavailable scales
of analysis. (Witness the emerging fields of computational biology,
computational chemistry, computational linguistics, computational geometry,
computational archeology, computational architecture design, computational
philosophy, computational social science, and the list goes on.) In *Plain
Text* I stake out a middle ground between Stephen Ramsey's laudatory *Reading
Machines* (University of Illinois Press, 2011) and David Golumbia's disparaging
*The Cultural Logic of Computation* (Harvard UP, 2009), making the case for
transformative use of technology in the humanities, in a reciprocal way that
assures mutual enrichment.

# Audience and Market

As is the case for most of my work, *Plain Text* appeals to two key audiences.
The first comprises digital, media, and literary scholars interested in the
material aspects of knowledge production. The second is composed of "knowledge
workers" that do not usually view their everyday practice in its historical,
philosophical, or political contexts. Software developers, graphic designers,
system administrators, and project managers routinely affect technologies that
have deep cultural significance: from the ways in which we relate to our family
and friends to the formation of shared cultural archives. For this reason,
technical decisions like choosing a text editor, a filing system, or a social
networking platform cannot be adequately addressed in shallow instrumental
terms limited to efficacy, speed, or performance.

A former software engineer and now a literary scholar, my research bridges the
(perceived) gap between the "two cultures" of science and the humanities. My
courses at Columbia University, which include *Code & Poetry* (Fall 2014),
*Computing in Context* (Spring 2015), and *Foundations of Computing for
Journalists* (Summer 2014), attract a diverse body of students from multiple
disciplines (and particularly from departments of English, History, and
Computer Science). I lecture widely in language departments, in schools of
engineering, in front of publishers, architects, artists, and librarians. As
one of the founders of Columbia's Group for Experimental Methods in the
Humanities, I believe in critical engagement with technology and in exposing my
students to real-world problems. My group has organized workshops on online
security for activists; we have reached out to an online community of engineers
to help us write media history as a project in citizen humanities; and we have
plans in place to teach digital literacy at Rikers Island.[^ln-xp] In the fall
of 2014, the group's activity became the basis for Columbia's capital fund
raising efforts around digital humanities, in the form of a proposal for the
Center of Culture and Computation. The proposal was met with wide approval from
the university deans, the provost, and the president. In January 2015, the
group was encouraged to apply for the presidential "global innovation" fund to
organize workshops on "digital justice" in Beijing, Mumbai, and in Amman,
Jordan.

I am inspired in these endeavors by my colleagues at Columbia English and at
Harvard's Berkman Center for Internet and Society, where I am an active faculty
associate. I keep these manifold audiences in mind as I am finishing *Plain
Text.* The book exposes the intellectual foundations that bolster my research
and teaching activities. I write to strengthen these projects and to give back
to the community that has supported me so generously. I hope to rely on the
same good will and on the same support networks in reaching out to promote my
book.

In the course of my teaching career, I have been asked to create courses and
certificate programs for graduate students in the humanities interested in
computational studies, including courses at the Digital Humanities Summer
Institute at the University of Victoria and in the Lede program at Columbia's
Journalism School. Texts usually assigned in these environments are either
volumes published by technical presses for a professional audience or
theory-based readings in New Media Studies that are related to the task of
teaching the fundamentals of Computer Science in context only loosely. My book
furnishes the intellectual foundations motivating the study of computing
fundamentals, explaining not just the how of Digital Humanities but the why.
With the proposed technical appendix and a possible companion site (explained
in the next section), I hope to supplement the main body of the work with
a series of "experiments" that illustrate theoretical concepts in practice, at
the keyboard. Such blend of theory and practice defines my method. As many
major universities invest in programs related to Digital Humanities,
Computational Social Science, and Computational Journalism,[^ln-dh] I hope for
*Plain Text* to become a standard text that introduces faculty and advanced
graduate students to the notion of critical practice in humanities computing.

[^ln-dh]: For example: In February 2015, U. Penn received $7,000,000 from Penn
Arts and Sciences Overseer to establish the Price Lab for the Digital
Humanities. In December 2014, Yale announced an award in the amount of
$3,000,000 from The Goizueta Foundation to inaugurate the Digital Humanities
Laboratory. UC Berkeley announced $2,000,000 from the Andrew W. Mellon
Foundation to advance digital humanities. Stanford has recently launched the
Center for Spatial and Textual Analysis. University of Michigan opened six new
tenure-track searches in a hiring cluster under the rubric of Public Humanities
in a Digital World in 2012. In May of 2014, Bard College was Awarded $800,000
from Andrew W. Mellon Foundation to support Experimental Humanities.

[^ln-xp]: A detailed account of these an other of projects can be found at
[xpmethod.plaintext.in/strains.html](http://xpmethod.plaintext.in/strains.html).

# Length and Format

I am writing the book as a traditional volume, expounding a sustained thesis
across seven chapters. I tend to write concisely--a style that complements the
subject matter. At this point, I am aiming for a manuscript of around 80-90k
words, which would allot around 10-15k words per chapter.

Although, ostensibly, a work on the history and philosophy of computational
culture, the book argues for the advancement of theory through practice. In
writing it, I continually test my intuitions against the reality of
contemporary computing devices: laptops, servers, and mobile phones. It is one
thing, for example, to maintain that "the media is the message," and quite
another to ask how different modalities like sound, image, and video are
encoded on level of the operating system (Chapter 4). Similarly, the difference
between binary and plain-text formats can be best explored by peeking "under
the hood" of an Adobe Acrobat file (as I do in Chapter 1).

Although not required for the understanding of the book, I would like to
include a supplementary appendix that expands on the theoretical insight from
each chapter with "experiments" that can be done in the command line (a textual
human-computer interface built into most operating systems). These exercises
could exist on paper (as an Appendix) or as a complimentary website. Training
in computational methods often begins with packaged tools that obscure the
underlying complexity of the method. For example, while writing a relatively
complex piece of code a journalist in my class once confessed to being confused
about the relationship between files and folders. *Plain Text* is a book
*about* files and folders: it is about textuality as encoded in specific ways
on machines that have a shared engineering pedigree. The hidden (but very much
intended) side-effect of *Plain Text* is a measure of technical proficiency
with Unix-based operating systems. Higher-level notions that address the
political reality of computation, be they "open access," "freedom of
information," or "online privacy", begin at that system level. The book's
Technical Appendix would help build a practical foundation to the arguments
advanced within.

The book assumes no prior knowledge. It can be read sequentially as a
conventional piece of scholarship in textual theory or new media studies. But
for those willing to take the plunge, I will often illustrate abstract
theoretical concepts by asking the reader to type some commands into their
terminals. Detailed instructions on how to set up this *augmented reading
environment*: related experiments, exercises, and explanations will be found
in the Technical Appendix.

In addition to the Appendix, the book contains several (8-12) tables and
illustrations. I intend to apply for the "first book" subsidy offered by
Harvard's Department of Comparative Literature to offset any costs associated
with the preparation of the manuscript.

# Annotated Table of Contents

## Chapter 1: We Have Always Been Digital

The first chapter begins with popular intuitions about the "look and feel" of
digital aesthetics. Discourse around the digital humanities employs the term
most central to its stated research program inconsistently and often without
theoretical reflection. A case study of television motion blur (and the related
"soap opera effect") undermines the initial ease with which notions of the
digital are overdetermined to stand in for a range of often conflicting
modalities. The next section deals with the analytic tradition of dissecting
media into analog and digital categories (via Nelson Goodman in particular). My
summary of that tradition shows that language and text are already "born
digital," that is, discrete and differentiated throughout (by the analytical
definition). Furthermore, digitality depends on "reliable processes of copying
and preservation." From that insight I take it that "being digital" is not an
ontological condition, but rather structure imposed from without. Digital being
ends up being an instrumental condition imposed onto the "user" of media (and
not property of the media itself), manifesting in specific technological
affordances. "Is it copyable?" becomes "can I copy it?" The chapter ends with a
history of Morse, Hughes, and Baudot character encodings, where the dichotomy
between digital and analog is superseded by the distinction between binary and
plain text formats.

## Chapter 2: Phenomenology of a Photocopier

The chapter builds on the history of character encoding to include two major
standards underlying the contemporary encounter with digital text: ASCII and
Unicode. I approach the subject through the confused history of the distinction
between form and content: two theoretical concepts crucial to understanding the
distinction between binary and plain text. In the theoretical discussion, I
find that going back to Plato and Hegel, "form" is at times used to indicate
physical structure, and, at other times, to indicate immaterial categories in
the ideal realm. A critical treatment of a more contemporary conversation on
"surface" and "depth" of meaning reveals form as a mediating concept between
thought and matter. A case study in extreme surface reading, in the bowls of a
photocopier, opens the way to the distinction between print (where matter,
form, and content are literally fused) and screen (where the three layers come
apart, providing only the illusion of flattened textuality).

## Chapter 3: Literature Down to the Pixel

Having established the grounds for digital textuality in the history of
character encoding, I begin the work of moving from "low level" first-order
concepts like "text" and "code" up to second-order concepts like "literature"
and "canon." The chapter starts by developing a theory of "microanalysis," a
closest possible kind of reading that pays attention to the material contexts
of knowledge production. I argue here that the concern with value in literary
criticism detracts from the explicit movement of control and power intimately
connected to digital textuality. Unlike scholars in the foucauldian tradition
(who often trace the machinations of power through discourse), I concentrate my
analysis on mechanisms of control at the material roots of literary practice.
In doing a media history through primary sources on early development of modern
computing, I show the explicit admixture of content and code: one meant to
communicate messages to humans and the other to program universal machines.
This history is not entirely critical: rather, it reveals an alternative
genealogy of computing, contrary to the widely accepted notions of computer as
a device for reductive "mathesis" (in the words of Johanna Drucker). I conclude
to argue that Turing machines were anticipated not just by the Babbage
calculator, but in a series of advances in communications, word processing, and
media storage.  A notion of text (as opposed to number) is hence "baked into"
the system.

## Chapter 4: Media are Not the Message

In this chapter I make two significant interventions in media studies and
literary theory. First, I argue that the medium should not be confused with the
message. Starting with the legacy of Marshall McLuhan, I examine several models
of communication, including ones proposed by Charles Sanders Peirce, Ferdinand
de Saussure, Harry Nyquist, Claude Shannon, Roman Jacobson, and Gregory
Bateson. Corresponding to the mediating of role of "form" in the previous
chapter, I find that the "modality" of the written communication act does
something akin to "attuning" the reader to the encoding of the message (and to
the corresponding cultural techniques of apprehension). At the center of the
chapter is a case study based on a real-world text "written" by a troupe of
Sulawesi macaques as part of an art project at Britain's Paignton Zoo. (The
text was subsequently published under the title *Notes Towards The Complete
Works of Shakespeare*, Vivaria 2002). The possibility of a randomly-generated
zoo-text points to the second half of the chapter, containing an argument
against the "systems" definition of information. In what Shannon calls a
"strange feature" of this communication theory, information is defined as
amount of "freedom" or entropy in the system. By contrast, I want to insist on
an agency-based model of information processing, in which texts mean things for
conscious beings.

## Chapter 5: Recursive Encounters with Oneself

Understanding the document as a vector. The problem of drafts and versions.
What is being transmitted through the vector? The appointment with one self:
Beckett and Sartre. Pipes and I/O serialization. What does it mean to "know"
something? Extended cognition. Pen, typewriter, and word processor (with
detours to Kittler and Heidegger). Writing together.  Models of co-authorship
(and why we should pay attention).  The massively multi-authored online novel
(*Wu Ming* and *Lo zar non Ã¨ morto*).

## Chapter 6: Bad Links

In this chapter I would like to triangulate between (1) structuralist notions
of literary intertextuality, (2) the promises and failures of hyper-textuality
in the 1990s, and (3) contemporary paradigms in network analysis. In each of
these historically distinct theoretical moments, I find a similar promise of
emergence: the idea that a pattern or design can appear spontaneously as the
result of simple interactions on the level of the system. Once again, I would
like to argue against the systems view generally, and in literary studies in
particular. In examining the heavily hyper-linked and intertextual art of Gwern,
a mysterious "researcher, self-experimenter, and writer" I contrast the
structure of links to that of lists. A discussion on "linked lists" in computer
science leads to the conclusion that deals with the aesthetics of code and the
role of the individual in relation to the assemblage.

## Chapter 7: Engineering for Dissent

Encrypted literature. Revealment and dissimulation. Notions of textuality as
embroiled in contemporary ideas of privacy, secrecy, and transparency.
Computation does not necessarily work for the military-industrial apparatus (as
argued by Golumbia, Lennon, and McPherson). Recovering and preserving
textuality in computing.

## Tech Appendix

A series of "experiments" that illustrate key theoretical concepts form each
chapter in the command line, to create an "augmented reading environment."

# Relationship to Dissertation

The book bears a resemblance to the dissertation in the subtitle only. Several
paragraphs from the embargoed dissertation did make it into *Plain Text* in an
ad-hoc manner, but the book as a whole represents a completely new
framework and a new direction in my thinking about the subject. The bulk of the
re-written dissertation made it into another project, called *Algorithmic
Imagination*, to be completed in the winter of 2015/16.

# Schedule to Completion

As of today, the manuscript stands at 40k words. During the academic year, I
average around 250 words per day, a number that more than quadruples when freed
from teaching obligations.[^ln-progress] Assuming a manuscript of around 80k
words, a conservative estimate of my schedule would place the final draft
somewhere towards the middle of summer, 2015.

[^ln-progress]: I wrote a word-counter that beacons my progress online at
[xpmethod.plaintext.in/minimal-computing/plaintext.html](http://xpmethod.plaintext.in/minimal-computing/plaintext.html).

