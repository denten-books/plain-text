---
title: "Plain Text: Poetics of Human-computer Interaction (Book Proposal)"
author: Dennis Tenen

---
\newpage

Much of today's textual and literary activity passes through some form of
digital being. Yet, the material contexts of digital knowledge production
remain obscured and in need of increased critical scrutiny. In peeling back the
layers of technology and cultural technique that constitute contemporary
encounter with text, "Plain Text" seeks to reconfigure familiar terms that
motivate the study of new media, literature, and the digital humanities. The
book unfolds a philosophical investigation of digital being through a media
history of plain text, a foundational file format still extant on every digital
device destined for human-computer interaction.

## Theme and Argument

Plain text is a file format and a frame of mind. A fundamental concept in the
development of computing, plain text stands in opposition to ways of thinking
that have produced closed platforms, rarefied knowledge, and black-box devices.
Instead, it offers a vision of data that is human-readable by design: portable,
concise, and universal. This book contains an argument in defense of plain
text. In making my case, I seek to convene a community interested in reflecting
critically on the ideas, tools, and practices that shape our daily encounter
with computation.

I build on those principles to ask: what is at stake in the difference between
digital and analog media? What contains more information a novel or a block of
wood?  What separates meaning, form, and formatting? How do pixels form into
texts? Where does data end and meta-data begin? To what extent media determine
the message? The formulation of these broadly theoretical concerns about the
poetics of human-computer interaction opens the way to a discussion about the
social impact of textual technology, as it relates to applied dynamics of
online agency, deliberation, consensus-making, and dissent.

A secondary aim of this volume is to convince computer "users" to view their
computational environments as a literary system of sorts. I mean a "literary
system" differently to what one might conventionally mistake for a "binary" or
"digital" one, however imprecise those terms are in everyday use. In clarifying
usage, I ask those who may have considered themselves mere "users" to become
close readers, thinkers, and makers of technology, able to apply the same
critical acuity to reading code and platform as they do to close reading of
prose and poetry. Ultimately, the book makes a case for the recovery of textual
roots always and already latent in the mechanisms of modern computing.

## Field Significance

The book seeks to redress a weakness in the field of digital humanities,
particularly at the point of its relevance to literary studies. Scholarship at
the intersection of these two fields is sometimes criticized for being
ahistorical or atheoretical, abandoning deep traditions of literary theory and
criticism, even where such traditions would help bolster the case for the
digital humanities. The nominally related field of new media studies has the
opposite problem. Although theoretically sophisticated, it sometimes produces
research far removed from the actual practice of creating new media (the
archetypal example given by Katherine Hayles is that of a contemporary
photography critic not familiar with the use of the "layers" tool in
Photoshop). By contrast, I situate "Plain Text" at the intersection of theory
and practice: somewhere between "technical literacy for new media studies" and
"philosophical bases for computing in the humanities."

In pursuing the above strategy I make several decisive contributions to the
fields of media studies, literary theory, and the digital humanities:

First, my approach is *strongly materialist.* Throughout my argument, I seek to
recover the literal material contexts (paper, silicon, and magnetic storage
media) that give support to higher level social and cognitive phenomena (like
literature, text, and discourse).

Second, the book contains a strong undercurrent of humanism. In making explicit
the ways in which changes in the material substratum affect higher-order
cultural techniques (of knowledge production and literary dissemination) I
insist on the reinstatement of human agency in a conversation that has largely
turned towards the object, the system, and the post-human.

Finally, my work is experimental in that it affects history and theory through
practice. Because engineering is an evolutionary practice, contemporary
computational devices contain within them traces of their early development.
This means that lines of code from software running Unix systems in the 1970s
are still in some real sense present on modern machines (like Apple Macintosh
laptops and Android phones, which run Unix-derive operating systems). This
property allows for a media archeology that can "lay bare" the device, making
good on the implied metaphor, involving literal excavation, surveying, and
artifact discovery (although enacted locally, at the level of the device).

## Existing Literature

"Plain Text" makes a theoretical intervention in the cluster of media studies-
and digital humanities-related fields that include science and technology
studies, platform studies, critical code studies, and media archeology. Recent
comparable books in these fields include *Paper Knowledge* by Lisa Gitelman
(Duke University Press, 2014); *Coding Freedom: The Ethics and Aesthetics of
Hacking*, by Gabriella Coleman (Princeton University Press, 2012); *Mechanisms:
New Media and the Forensic Imagination*, by Matthew G. Kirschenbaum (MIT Press,
2012); *Files: Law and Media Technology*, by Cornelia Vismann (Stanford
University Press, 2008); and several titles in the Electronic Mediations series
at Minnesota University Press, which published Lori Emerson's *Reading Writing
Interfaces* in 2014.

My work differs from these volumes in two major ways. First, while committed to
theoretical concerns---that is, ideas that can guide or challenge the way we
study texts, their production, their meaning, and their impact on the people
who use and produce them---my argument also dwells in the realm of traditional
philosophy. Second, my argument is deeply grounded in the fields of software
and electrical engineering. Consider my first chapter called  "We Have Always
Been Digital," which begins with a discussion of "digital representation" as
philosophers Nelson Goodman and John Haugeland define it in the analytic
tradition. I proceed by testing their intuitions on the basis of something
called the "Soap Opera Effect" particular to modern liquid crystal displays
(LCDs) and in the related video-processing technique of "motion-compensated
frame interpolation." The resulting analysis clarifies the various (and often
conflicting) meanings of the word digital in media studies and in the digital
humanities. Similarly, in the second chapter of the book, "Literature Down to
the Pixel," I argue that universal Turing Machines, usually viewed as
algorithms, should be considered in the context of word processing, as devices
that cannot quite shake the material legacy of paper and pencil.

Although I do not mean to engage in the debate on disciplinary formation, I
prefer to describe my work as "computational culture studies," both in the
sense of "the study of computational culture," and as "computational approaches
to the study of culture." It is important for me to insist on the reciprocal
motion between the constituent elements of "computation" and "culture." Too
often rhetoric around the digital humanities resembles a one-way street, in
which computational methods are promised to reform the humanities unilaterally.

Books like Alexander Galloway's *Laruelle: Against the Digital* (University of
Minnesota Press, 2014), Matthew Fuller's *Evil Media* (MIT Press, 2012), and
Johanna Drucker's *What Is?* (Cuneiform Press, 2013) represent the sharp edge
of a critical counter-movement to digital positivism. But this response, too,
must be balanced against the constructive potential of the digital humanities
to extend humanistic inquiry into new and exciting directions. As was the case
with the "linguistic turn" in the decades prior, almost all fields of human
knowledge are now experiencing a turn towards computational methods, which
offer insights at previously unavailable scales of analysis. (Witness the
emerging fields of computational biology, computational chemistry,
computational linguistics, computational geometry, computational archeology,
computational architecture design, computational philosophy, computational
social science, and the list goes on.) In "Plain Text," I stake out a middle
ground between Stephen Ramsey's laudatory *Reading Machines* (University of
Illinois Press, 2011) and David Golumbia's disparaging *The Cultural Logic of
Computation* (Harvard University Press, 2009). Ultimately, I argue in favor of
transformative use of technology in the humanities, with reciprocal effects
that promise mutual enrichment.

## Audience and Market

As is the case for most of my work, "Plain Text" appeals to two key audiences.
The first comprises digital, media, and literary scholars interested in the
material aspects of knowledge production. The second is composed of knowledge
workers that do not usually view their everyday practice in its historical,
philosophical, or political contexts. Software developers, graphic designers,
system administrators, and project managers routinely architect technologies
that have deep cultural significance, affecting a range of cultural practices:
from the ways we relate to our family and friends to the formation of shared
cultural archives. Because such "cultural techniques" are formative of our
culture, supposedly technical decisions like choosing a text editor, a filing
system, or a social networking platform cannot be adequately addressed in
shallow instrumental terms limited to efficacy, speed, or performance.

As a former software engineer and now a literary scholar, I make sure that my
research bridges the (perceived) gap between the "two cultures" of science and
the humanities. My courses at Columbia University, which include Code & Poetry
(Fall 2014), Computing in Context (Spring 2015), and Foundations of Computing
for Journalists (Summer 2014), attract a diverse body of students from various
disciplines (and particularly from departments of English, history, and
computer science). I lecture widely in language departments, in schools of
engineering, in front of publishers, architects, artists, and librarians. As
one of the founders of Columbia's Group for Experimental Methods in the
Humanities, I encourage my students to consider technology reflectively,
combining critical theory with a measure of critical practice.

In this spirit, my group has organized workshops on online security for
activists; we have reached out to an online community of engineers to help us
write media history as a project in citizen humanities; and we are set to teach
critical making at Rikers Island this summer. I am inspired in these endeavors
by my colleagues at the Columbia University Department of English and at
Harvard University's Berkman Center for Internet & Society, where I am an
active faculty associate. I keep these manifold audiences in mind as I complete
"Plain Text." The book exposes intellectual frameworks that bolster my research
and teaching activities. I write to strengthen these projects and to give back
to the community that has supported me so generously. I hope to rely on the
same good will and support networks in reaching out to promote my book.

In my teaching career, I have been asked to create courses and certificate
programs for graduate students in the humanities interested in computational
studies, including courses at the Digital Humanities Summer Institute at the
University of Victoria and in the Lede Program at the Columbia University
Graduate School of Journalism. Texts usually assigned in courses like that are
either volumes published by technical presses for a professional audience or
theory-based readings in new media studies related only loosely to teaching the
fundamentals of computer science in context. My book furnishes the intellectual
foundations for the practice of digital humanities. With the proposed technical
appendix and a possible companion site (explained in the next section), I hope
to supplement the main body of the work with a series of "experiments" that
illustrate theoretical concepts in action, at the keyboard. Such a blend of
theory and practice defines my method. As many major universities invest in
programs related to digital humanities, computational social science, and
computational journalism, I hope for "Plain Text" to become a standard text
that introduces faculty and advanced graduate students to the notion of
critical practice in humanities computing.

## Length and Format

I am writing the book as a traditional volume, expounding a sustained thesis
across seven chapters. At this point, I am aiming for a manuscript of around
80,000 to 90,000 words, allotting around 10,000 to 15,000 words per chapter.

While "Plain Text" is a work on the history and philosophy of computational
culture, the book also argues for the advancement of theory through practice.
In writing it, I continually test my intuitions against the reality of
contemporary computing devices such as laptops, servers, and mobile phones. It
is one thing, for example, to maintain that "the medium is the message," and
quite another to ask how different modalities like sound, image, and video are
encoded on the level of the operating system (Chapter 4). Similarly, the
difference between binary and plain-text formats can be best explored by
peeking "under the hood" of an Adobe Acrobat file, as I do in Chapter 3.

Although not required for the understanding of the book, I would like to
include a supplementary technical appendix that expands on the theoretical
insight from each chapter with "experiments" that can be done in the command
line (a textual human-computer interface built into most operating systems).
These exercises could exist on paper (as a written appendix) or as a
complementary website. Training in computational methods often begins with
packaged tools that obscure the underlying complexity of the method. For
example, while writing a relatively complex piece of code, a journalist in my
class once confessed to being confused about the relationship between files and
folders. "Plain Text" is a book about files and folders: it is about textuality
as encoded in specific ways on machines that have a shared engineering
pedigree. The hidden (but very much intended) side-effect of "Plain Text" is a
measure of technical proficiency with Unix-based operating systems.
Sophisticated understanding of the political issues around computation, be they
"open access," "freedom of information," or "online privacy," begin at that
system level. The book's technical appendix would help build a practical
foundation for the arguments advanced within.

The book assumes no prior knowledge. It can be read sequentially as a
conventional piece of scholarship in textual theory or new media studies. But
for those willing to take the plunge, I will often illustrate abstract
theoretical concepts by asking readers to type some commands into their
terminals. Detailed instructions on how to set up this augmented reading
environment, along with related experiments, exercises, and explanations, will
be found in the technical appendix.

In addition, the book will contain 15-20 tables and illustrations. I intend to
apply for the "first book" subsidy offered by Harvard University's Department
of Comparative Literature to offset any costs associated with the preparation
of the manuscript.

## Annotated Table of Contents

### Chapter 1: We Have Always Been Digital

Discourse around the digital humanities needs a robust sense of the digital.
The first chapter begins with popular intuitions about the "look and feel" of
digital aesthetics. A case study of television motion blur (and the related
"soap opera effect") undermines the initial ease with which notions of the
digital are overdetermined to stand in for a range of often conflicting
modalities. The next section of this chapter deals with the analytic tradition
of discussing media in terms of analog and digital representation (via Nelson
Goodman and John Haugeland). My summary of that tradition shows that language
and text are already in a sense "born digital," that is, discrete and
differentiated throughout. Furthermore, digitality depends on "reliable
processes of copying and preservation"---processes that can mean something
different to a philosopher than to a librarian. From these insights I take it
that "being digital" is not an intrinsic ontological condition, but rather a
structure imposed from without. That structure is further imposed on the media
"consumer." It does not necessarily constitute a property of the medium itself,
manifesting in specific technological affordances. "Is it copyable?" becomes
"Can I copy it?" The chapter ends with a history of Morse, Hughes, and Baudot
character encodings, in which the dichotomy between digital and analog is
superseded by the distinction between binary and plain text formats.

### Chapter 2: Literature Down to the Pixel

Having established the grounds for digital textuality in the history of
character encoding, I begin the work of moving from "low level" first-order
concepts such as "text" and "code" up to second-order concepts such as
"literature" and "canon." The chapter starts by developing a theory of
"microanalysis," the closest possible kind of reading that pays attention to
the material contexts of knowledge production. I argue here that the concern
with value in literary criticism detracts from the explicit movement of control
and power intimately connected to digital textuality. Unlike scholars in the
foucauldian tradition (who often trace the machinations of power through
discourse, on the level of representation), I concentrate my analysis on
mechanisms of control at the material roots of literary practice. In doing a
media history through primary sources on early development of modern computing,
I show the explicit admixture of content and code: one meant to communicate
messages to humans and the other to program universal machines. This history is
not entirely critical: rather, it reveals an alternative genealogy of
computing, contrary to the widely accepted notions of computer as a device for
reductive "mathesis" (in the words of Johanna Drucker). I conclude to argue
that Turing machines were anticipated not just by the Babbage calculator, but
also by a series of advances in communications, word processing, and media
storage. A notion of text (as opposed to number) is hence "baked into" the
system.

### Chapter 3: WYSINWYAG (What You See Is Not What You Always Get)

The chapter builds on the history of character encoding to include two major
standards underlying the contemporary encounter with digital text: ASCII and
Unicode. I approach the subject by noting the confused history of the
distinction between form and content: two theoretical concepts crucial to
understanding the distinction between binary and plain text. I find that going
back to Plato and Hegel, "form" is used at times to indicate physical structure
and at other times to indicate immaterial categories in the ideal realm. A
critical treatment of a more contemporary conversation about "surface" and
"depth" of meaning reveals "form" as a mediating concept between thought and
matter. A case study in extreme surface reading, in the bowels of a
photocopier, opens a distinction between print (in which matter, form, and
content lie flat) and screen (where the three layers come apart, providing only
the illusion of flattened textuality).

### Chapter 4: Media are Not the Message

In this chapter I make two significant interventions in media studies and
literary theory. First, I argue that media should not be confused with
messages.  Starting with the (sometimes oversimplified) legacy of Marshall
McLuhan, I examine several models of communication, including ones proposed by
semioticians (Charles Sanders Peirce, Ferdinand de Saussure, Roman Jakobson),
engineers (Harry Nyquist, Claude Shannon), and animal anthropologists (Jakob
von Uexk√ºll, Gregory Bateson, and Thomas Sebok). Corresponding to the mediating
of role of "form" in the previous chapter, I find that the "mode" of the
written communication act does something akin to "attuning" the reader
(receiver) to the encoding of the message (and to the corresponding cultural
techniques of apprehension). At the center of the chapter is a case study based
on a real-world text "written" by a troupe of Sulawesi macaques as part of an
art project at Britain's Paignton Zoo. (The text was subsequently published
under the title *Notes Towards The Complete Works of Shakespeare*, Vivaria
Press, 2002). The possibility of a randomly generated zoo-text points to the
conclusion of the chapter, containing an argument against a "systems"
definition of information. In what Shannon calls a "strange feature" of this
communication theory, information is defined as amount of "freedom" or entropy
in the system. By contrast, I want to insist on an agency-based model of
information processing, in which texts mean things *for* free and conscious
beings.

### Chapter 5: Recursive Encounters with Oneself

This chapter begins with a close reading of Beckett's *Krapp's Last Tape*,
positing magnetic storage media as the "media being" of the play's title
character. Krapp helps to unfold the problematic of documents: not as passive
completed texts, but as vectors that mutate and move through time and space.
Documents, as I argue here, constitute a platform for what I call "media
being." Pushing off the communication model described in the last chapter, I
ask: What is being communicated? And answer: It is the subject itself. The
chapter ends with an explication of Sartre's idea of an "appointment with
oneself," which I take to happen at the document level, through a series of
encounters with one's own external manifestations of consciousness (i.e. tape
and other portable storage media).

### Chapter 6: Bad Links

Where do the document vectors lead? In this chapter I triangulate between (1)
structuralist notions of literary intertextuality, (2) the promises and
failures of hypertextuality in the 1990s, and (3) contemporary paradigms in
network analysis. In each of these historically distinct theoretical moments, I
find a similar promise of emergence: the idea that order can appear
spontaneously as the aggregate result of simple interactions at the level of
the system (the idea also at the core of Callon and Latour's actor-network
theory). Once again, I make the rhetorical move against the systems view, both
generally, and particularly as that view is applied in literary studies. In
examining the heavily hyperlinked and intertextual essays of Gwern, a
mysterious contemporary "researcher, self-experimenter, and writer," I contrast
the structure of links to that of lists. A discussion on "linked lists" in
computer science leads to a conclusion that relates to aesthetics of code and
to the role of an individual in relation to the assemblage.

### Chapter 7: Engineering for Dissent

In this final chapter, I argue for the recovery and the preservation of plain
textuality in the day-to-day practice of modern computing. Returning to the
history of the `.txt` file format, I find that early documents from the
International Telecommunication Union (and later international bodies governing
the exchange of information across state borders) display unease with
encrypted, non-human-readable formats of information exchange. A theoretical
treatment of technological skepticism from Heidegger, to Mumford, Kittler, and
Golumbia concludes with a discussion about the subject's role in actively
shaping the material conditions of media being. As documents reflecting
externalized states of consciousness become increasingly more transparent, they
are susceptible to increased surveillance and control. Plain text allows
political subjects to decouple externalized mental states from mechanisms of
governance. (In other words, to decide actively when to opt in and when to opt
out). This affordance is not, however, a deterministic property of literature,
the Internet, or any other information exchange system. Rather, the design of
complex systems must itself become critical practice, which, in complement to
critical theory, can actively engineer for textual mechanisms that make
individual dissent possible.

## Tech Appendix

A series of "experiments" that illustrate key theoretical concepts from each
chapter, starting commands to be entered in the command line, to create an
"augmented reading environment."

## Relationship to Dissertation

The book bears a resemblance to the dissertation in the subtitle only. Several
paragraphs from the embargoed dissertation did make it into "Plain Text" in an
ad-hoc manner, but the book as a whole represents a completely new framework
and a new direction in my thinking about the subject.

## Schedule to Completion

The research for this book was enabled by a year-long fellowship at the Berkman
Center for Internet and Society. Consequent to the research phase, I taught
several classes on the subject, which helped refine my thinking and provided
further notes and primary material. As of today, the manuscript stands at
roughly 55,000 words, with three chapters close to completion. I am writing
actively and plan to have a first draft of the manuscript ready by June-July of
2015. I am on leave next academic year, having cleared my schedule, with plans
      of dedicating all of my attention to seeing this project through to
publication.
