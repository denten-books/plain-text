---
title: "Plain Text: Poetics of Human-computer Interaction (Book Proposal)"
author: Dennis Tenen

---

\newpage

*Plain Text* occupies the space between literary theory and the digital
humanities, challenging received notions of digital being through the media
history of a singular file format (`.txt`).

# Theme and Argument

Plain text is a file format and a frame of mind. A fundamental concept in the
development of computing, plain text stands in opposition to closed platforms,
rarefied knowledge, and black-box devices. Instead, it offers a vision of data
that is human-readable by design: portable, concise, and universal. This book
contains an argument for plain text. In doing so, it seeks to convene a
community interested in reflecting critically on the ideas, tools, and
practices that shape our daily encounter with computation. My argument starts
with foundational principles at the intersection of media theory and
information science. I ask: what is at stake in the difference between digital
and analog media?  What contains more information a novel or a block of wood?
What separates meaning, form, and formatting? How do pixels form into texts?
Where does data end and meta-data begin? To what extent media determine the
message? The formulation of these broadly theoretical concerns about the
poetics of human-computer interaction opens the way to a discussion about the
social impact of textual technology, as it relates to applied dynamics of
online agency, deliberation, consensus-making, and dissent.

A secondary aim of this volume is to convince computer "users" to view their
computational environments as a literary system of sorts. I mean a "literary
system" in opposition to what one might conventionally mistake for a "binary"
or "digital" one, however imprecise those terms are in everyday use. In
clarifying usage, I will ask mere "users" to become close readers, thinkers,
and makers of technology, able to apply the same critical acuity to reading
code and platform as they do to close reading of prose and poetry. Ultimately,
the book makes a case for the recovery of textual roots latent in the
mechanisms of modern computing.

# Field Significance

The book aims to redress a weakness in the field of Digital Humanities,
particularly as it relates to literary studies. Much scholarship in this space
is often criticized for being ahistorical or atheoretical, abandoning the deep
traditions of literary theory and criticism, even where such traditions would
help bolster the case for emerging scholarship. The nominally related field of
New Media Studies has the opposite problem. Although theoretically
sophisticated, it sometimes produces research far removed from the actual
practice of creating new media (the archetypal example given by Katherine
Hayles is one of a contemporary photography critic not familiar with the use of
the "layers" tool in Photoshop). By contrast, I situate *Plain Text* at the
intersection of theory and practice: somewhere between "technical literacy for
new media studies" and "philosophical bases for computing in the humanities."

# Existing Literature

*Plain Text* makes a theoretical intervention in the cluster of media studies-
and digital humanities-related fields that include Science and Technology
Studies, Platform Studies, Critical Code Studies, and Media Archeology.
Recent comparable books in these fields include *Paper Knowledge* by Lisa
Gitelman published by Duke UP in 2014, *Coding Freedom: The Ethics and
Aesthetics of Hacking*, by Gabriella Coleman from Princeton UP in 2012,
*Mechanisms: New Media and the Forensic Imagination* by Matthew G. Kirschenbaum
from MIT Press in 2012, *Files: Law and Media Technology* by Cornelia Vismann
from Stanford UP in 2008 and several titles in the *Electronic Mediations*
series at Minnesota UP, which published Lori Emerson's *Reading Writing
Interfaces* in 2014.

My work differs from these volumes in that it is both more philosophical
(rather than strictly "theoretical") and more grounded in the fields of
software an electrical engineering. To give you an example of what I mean,
consider my third chapter, "Phenomenology of a Photocopier." It begins with the
discussion of the Hegelian distinction between "form" and "content." It ends
with a case study from the history of word processing, in a technical
explanation of the ways in which forms and content manifest themselves in
modern HTML and Markdown documents. Similarly, my first chapter, "We Have
Always Been Digital," begins with a discussion of "digital representation" as
philosophers Nelson Goodman and John Haugeland define it in the analytic
tradition. I proceed by testing their intuitions on the basis of something
called the "Soap Opera Effect" particular to modern Liquid Crystal Displays
(LCDs) and in the related video-processing technique of "motion-compensated
frame interpolation." The resulting analysis clarifies the various (and often
conflicting) meanings of the word "digital" in Media Studies and in the Digital
Humanities.

Complimentary to both of these fields, I prefer to describe my work as
"computational culture studies," understood in two ways: first as the study of
"computational culture," and, second, as computational approaches to "culture
studies." *Plain Text* falls firmly into the first category, with some elements
of the book edging on light computational methods (particularly if you consider
the proposed digital companion, discussed shortly).

Although I do not mean to engage in the debate on disciplinary formation, it is
important for me to insist on the reciprocal motion between the constituent
elements of "computation" and "culture." Too often rhetoric around the digital
humanities resembles a one-way street, in which computational methods are
promised to reform the humanities unilaterally. Books like Alexander Galloway's
*Laruelle: Against the Digital* (UMinn Press, 2014), Matthew Fuller's *Evil
Media* (MIT, 2012) and Johanna Drucker's *What Is?* (Cuneiform Press, 2013)
represent the beginning of a critical counter-movement. But this response too
must be balanced against emendatory potential of the Digital Humanities
program. As was the case with the "linguistic turn" in the decades prior,
almost all fields of human knowledge are now experiencing a turn towards
computational methods, which offer new insight at previously unavailable scales
of analysis. (Witness the emerging fields of computational biology,
computational chemistry, computational linguistics, computational geometry,
computational archeology, computational architecture design, computational
philosophy, computational social science, and the list goes on.) In *Plain
Text* I stake out a middle ground between Stephen Ramsey's laudatory *Reading
Machines* (University of Illinois Press, 2011) and David Golumbia's disparaging
*The Cultural Logic of Computation* (Harvard UP, 2009), making the case for
transformative use of technology in the humanities, in a reciprocal way that
assures mutual enrichment.

# Audience and Market

As is the case for most of my work, *Plain Text* appeals to two key audiences.
The first comprises digital, media, and literary scholars interested in the
material aspects of knowledge production. The second is composed of "knowledge
workers" that do not usually view their everyday practice in its historical,
philosophical, or political contexts. Software developers, graphic designers,
system administrators, and project managers routinely affect technologies that
have deep cultural significance: from the ways in which we relate to our family
and friends to the formation of shared cultural archives. For this reason,
technical decisions like choosing a text editor, a filing system, or a social
networking platform cannot be adequately addressed in shallow instrumental
terms limited to efficacy, speed, or performance.

A former software engineer and now a literary scholar, my research bridges the
(perceived) gap between the "two cultures" of science and the humanities. My
courses at Columbia University, which include *Code & Poetry* (Fall 2014),
*Computing in Context* (Spring 2015), and *Foundations of Computing for
Journalists* (Summer 2014), attract a diverse body of students from multiple
disciplines (and particularly from departments of English, History, and
Computer Science). I lecture widely in language departments, in schools of
engineering, in front of publishers, architects, artists, and librarians. As
one of the founders of Columbia's Group for Experimental Methods in the
Humanities, I believe in critical engagement with technology and in exposing my
students to real-world problems. My group has organized workshops on online
security for activists; we have reached out to an online community of engineers
to help us write media history as a project in citizen humanities; and we have
plans in place to teach digital literacy at Rikers Island.[^ln-xp] In the fall
of 2014, the group's activity became the basis for Columbia's capital fund
raising efforts around digital humanities, in the form of a proposal for the
Center of Culture and Computation. The proposal was met with wide approval from
the university deans, the provost, and the president. In January 2015, the
group was encouraged to apply for the presidential "global innovation" fund to
organize workshops on "digital justice" in Beijing, Mumbai, and in Amman,
Jordan.

I am inspired in these endeavors by my colleagues at Columbia English and at
Harvard's Berkman Center for Internet and Society, where I am an active faculty
associate. I keep these manifold audiences in mind as I am finishing *Plain
Text.* The book exposes the intellectual frameworks that bolster my research
and teaching activities. I write to strengthen these projects and to give back
to the community that has supported me so generously. I hope to rely on the
same good will and on the same support networks in reaching out to promote my
book.

In the course of my teaching career, I have been asked to create courses and
certificate programs for graduate students in the humanities interested in
computational studies, including courses at the Digital Humanities Summer
Institute at the University of Victoria and in the Lede program at Columbia's
Journalism School. Texts usually assigned in these environments are either
volumes published by technical presses for a professional audience or
theory-based readings in New Media Studies that are related to the task of
teaching the fundamentals of Computer Science in context only loosely. My book
furnishes the intellectual foundations motivating the study of computing
fundamentals, explaining not just the how of Digital Humanities but the why.
With the proposed technical appendix and a possible companion site (explained
in the next section), I hope to supplement the main body of the work with
a series of "experiments" that illustrate theoretical concepts in practice, at
the keyboard. Such blend of theory and practice defines my method. As many
major universities invest in programs related to Digital Humanities,
Computational Social Science, and Computational Journalism,[^ln-dh] I hope for
*Plain Text* to become a standard text that introduces faculty and advanced
graduate students to the notion of critical practice in humanities computing.

[^ln-dh]: For example: In February 2015, U. Penn received $7,000,000 from Penn
Arts and Sciences Overseer to establish the Price Lab for the Digital
Humanities. In December 2014, Yale announced an award in the amount of
$3,000,000 from The Goizueta Foundation to inaugurate the Digital Humanities
Laboratory. UC Berkeley announced $2,000,000 from the Andrew W. Mellon
Foundation to advance digital humanities. Stanford has recently launched the
Center for Spatial and Textual Analysis. University of Michigan opened six new
tenure-track searches in a hiring cluster under the rubric of Public Humanities
in a Digital World in 2012. In May of 2014, Bard College was Awarded $800,000
from Andrew W. Mellon Foundation to support Experimental Humanities.

[^ln-xp]: A detailed account of these an other of projects can be found at
[xpmethod.plaintext.in/strains.html](http://xpmethod.plaintext.in/strains.html).

# Length and Format

I am writing the book as a traditional volume, expounding a sustained thesis
across seven chapters. I tend to write concisely--a style that complements the
subject matter. At this point, I am aiming for a manuscript of around 80-90k
words, which would allot around 10-15k words per chapter.

Although, ostensibly, a work on the history and philosophy of computational
culture, the book argues for the advancement of theory through practice. In
writing it, I continually test my intuitions against the reality of
contemporary computing devices: laptops, servers, and mobile phones. It is one
thing, for example, to maintain that "the medium is the message," and quite
another to ask how different modalities like sound, image, and video are
encoded on the level of the operating system (Chapter 4). Similarly, the
difference between binary and plain-text formats can be best explored by
peeking "under the hood" of an Adobe Acrobat file (as I do in Chapter 2).

Although not required for the understanding of the book, I would like to
include a supplementary appendix that expands on the theoretical insight from
each chapter with "experiments" that can be done in the command line (a textual
human-computer interface built into most operating systems). These exercises
could exist on paper (as an Appendix) or as a complimentary website. Training
in computational methods often begins with packaged tools that obscure the
underlying complexity of the method. For example, while writing a relatively
complex piece of code a journalist in my class once confessed to being confused
about the relationship between files and folders. *Plain Text* is a book
*about* files and folders: it is about textuality as encoded in specific ways
on machines that have a shared engineering pedigree. The hidden (but very much
intended) side-effect of *Plain Text* is a measure of technical proficiency
with Unix-based operating systems. Higher-level notions that address the
political reality of computation, be they "open access," "freedom of
information," or "online privacy", begin at that system level. The book's
Technical Appendix would help build a practical foundation for the arguments
advanced within.

The book assumes no prior knowledge. It can be read sequentially as a
conventional piece of scholarship in textual theory or new media studies. But
for those willing to take the plunge, I will often illustrate abstract
theoretical concepts by asking the reader to type some commands into their
terminals. Detailed instructions on how to set up this *augmented reading
environment*, along with related experiments, exercises, and explanations will
be found in the Technical Appendix.

In addition to the Appendix, the book contains several (8-12) tables and
illustrations. I intend to apply for the "first book" subsidy offered by
Harvard's Department of Comparative Literature to offset any costs associated
with the preparation of the manuscript.

# Annotated Table of Contents

## Chapter 1: We Have Always Been Digital

Discourse around the digital humanities needs a robust sense of the digital.
The first chapter begins with popular intuitions about the "look and feel" of
digital aesthetics. A case study of television motion blur (and the related
"soap opera effect") undermines the initial ease with which notions of the
digital are overdetermined to stand in for a range of often conflicting
modalities. The next section of this chapter deals with the analytic tradition
of discussing media in terms of analog and digital representation (via Nelson
Goodman and John Haugeland). My summary of that tradition shows that language
and text are already "born digital," that is, discrete and differentiated
throughout (by the formal definition). Furthermore, digitality depends on
"reliable processes of copying and preservation," (properties that can mean
something different to a philosopher than to a librarian). From these insights
I take it that "being digital" is not an intrinsic ontological condition, but
rather structure imposed from without. That structure is further imposed onto
the media "consumer," and does necessarily constitute a property of the medium
itself, manifesting in specific technological affordances. "Is it copyable?"
becomes "can I copy it?" The chapter ends with a history of Morse, Hughes, and
Baudot character encodings, where the dichotomy between digital and analog is
superseded by the distinction between binary and plain text formats.

## Chapter 2: Phenomenology of a Photocopier

The chapter builds on the history of character encoding to include two major
standards underlying the contemporary encounter with digital text: ASCII and
Unicode. I approach the subject through the confused history of the distinction
between form and content: two theoretical concepts crucial to understanding the
distinction between binary and plain text. In the theoretical discussion, I
find that going back to Plato and Hegel, "form" is at times used to indicate
physical structure, and, at other times, to indicate immaterial categories in
the ideal realm. A critical treatment of a more contemporary conversation on
"surface" and "depth" of meaning reveals "form" as a mediating concept between
thought and matter. A case study in extreme surface reading, in the bowels of a
photocopier, opens a way to the distinction between print (where matter, form,
and content lie flat) and screen (where the three layers come apart, providing
only the illusion of flattened textuality).

## Chapter 3: Literature Down to the Pixel

Having established the grounds for digital textuality in the history of
character encoding, I begin the work of moving from "low level" first-order
concepts like "text" and "code" up to second-order concepts like "literature"
and "canon." The chapter starts by developing a theory of "microanalysis," the
closest possible kind of reading that pays attention to the material contexts
of knowledge production. I argue here that the concern with value in literary
criticism detracts from the explicit movement of control and power intimately
connected to digital textuality. Unlike scholars in the foucauldian tradition
(who often trace the machinations of power through discourse, on the level of
representation), I concentrate my analysis on mechanisms of control at the
material roots of literary practice. In doing a media history through primary
sources on early development of modern computing, I show the explicit admixture
of content and code: one meant to communicate messages to humans and the other
to program universal machines. This history is not entirely critical: rather,
it reveals an alternative genealogy of computing, contrary to the widely
accepted notions of computer as a device for reductive "mathesis" (in the words
of Johanna Drucker). I conclude to argue that Turing machines were anticipated
not just by the Babbage calculator, but in a series of advances in
communications, word processing, and media storage. A notion of text (as
opposed to number) is hence "baked into" the system.

## Chapter 4: Media are Not the Message

In this chapter I make two significant interventions in media studies and
literary theory. First, I argue that media should not be confused with
messages. Starting with the (sometimes oversimplified) legacy of Marshall
McLuhan, I examine several models of communication, including ones proposed by
semioticians (Charles Sanders Peirce, Ferdinand de Saussure, Roman Jacobson),
engineers (Harry Nyquist, Claude Shannon), and animal anthropologists (Jakob
von Uexküll, Gregory Bateson, and Thomas Sebok). Corresponding to the mediating
of role of "form" in the previous chapter, I find that the "mode" of the
written communication act does something akin to "attuning" the reader
(receiver) to the encoding of the message (and to the corresponding cultural
techniques of apprehension). At the center of the chapter is a case study based
on a real-world text "written" by a troupe of Sulawesi macaques as part of an
art project at Britain's Paignton Zoo. (The text was subsequently published
under the title *Notes Towards The Complete Works of Shakespeare*, Vivaria
2002). The possibility of a randomly-generated zoo-text points to the
conclusion of the chapter, containing an argument against a "systems"
definition of information. In what Shannon calls a "strange feature" of this
communication theory, information is defined as amount of "freedom" or entropy
in the system. By contrast, I want to insist on an agency-based model of
information processing, in which texts mean things for free and conscious
beings.

## Chapter 5: Recursive Encounters with Oneself

This chapter begins with a close-reading of Beckett's *Krapp's Last Tape*,
positing magnetic storage media as the "media being" of the play's title
character. Krapp helps to unfold the problematic of documents: not as passive
completed texts, but as vectors that mutate and move through time and space.
Documents, as I argue here, constitute a "platform" for media being. Pushing
off the communication model described in the last chapter, I ask: What is being
communicated? And answer: It is the subject itself. The chapter ends with an
explication of Sartre's idea of an "appointment with oneself," which I take to
happen at the document level, through a series of encounters with one's own
external manifestations of consciousness (i.e. tape and other portable storage
media).

## Chapter 6: Bad Links

Where do the document vectors lead? In this chapter I triangulate between (1)
structuralist notions of literary intertextuality, (2) the promises and
failures of hyper-textuality in the 1990s, and (3) contemporary paradigms in
network analysis. In each of these historically distinct theoretical moments, I
find a similar promise of emergence: the idea that order can appear
spontaneously as the aggregate result of simple interactions at the level of
the system (the idea also at the core of Callon and Latour's actor-network
theory). Once again, I make the rhetorical move against the systems view
generally, and in literary studies in particular. In examining the heavily
hyper-linked and intertextual essays of Gwern, a mysterious contemporary
"researcher, self-experimenter, and writer," I contrast the structure of links
to that of lists. A discussion on "linked lists" in computer science leads to a
conclusion that relates to aesthetics of code and to the role of an individual
in relation to the assemblage.

## Chapter 7: Engineering for Dissent

In this final chapter, I argue for the recovery and the preservation of plain
textuality in the day-to-day practice of modern computing. Returning to the
history of the `.txt` file format, I find that early documents from the
International Telecommunication Union (and later international bodies governing
the exchange of information across state borders) display unease with
encrypted, non-human-readable formats of information exchange. A theoretical
treatment of technological skepticism from Heidegger, to Mumford, Kittler, and
Golumbia concludes in a discussion about the subject's role in actively shaping
the material conditions of media being. As documents reflecting externalized
states of consciousness become increasingly more transparent, they are
susceptible to increased surveillance and control (through encryption, by
example). Plain text allows political subjects to decouple externalized mental
states from mechanisms of governance.  (In other words, to actively decide when
to opt in and when to opt out). This affordance is not however a deterministic
property of literature, the internet, or any other information exchange system.
Rather, the design of complex systems must itself become critical practice,
which, in complement to critical theory, can actively engineer for textual
mechanisms that make individual dissent possible.

## Tech Appendix

A series of "experiments" that illustrate key theoretical concepts form each
chapter in the command line, to create an "augmented reading environment."

# Relationship to Dissertation

The book bears a resemblance to the dissertation in the subtitle only. Several
paragraphs from the embargoed dissertation did make it into *Plain Text* in an
ad-hoc manner, but the book as a whole represents a completely new framework
and a new direction in my thinking about the subject.

# Schedule to Completion

The research for this book was enabled by a year-long fellowship at the Berkman
Center for Internet and Society. Consequent to the research phase, I taught
multiple classes on the subject, which helped refine my thinking and provided
further notes and primary material. As of today, the manuscript stands at
roughly 50k words, with three chapters close to completion. I am writing
actively and plan to have a first draft of the manuscript ready by June-July of
2015. I am on leave next academic year, having cleared my schedule with plans
of dedicating all of my attention to seeing this project through to
publication.
