---
title: "Plain Text: The Poetics of Human-computer Interaction (Book Proposal)
author: Dennis Tenen

---

Annotated table of contents that clearly develops the content and structure
of each chapter

Assessment of the work's fit with existing literature, comparison with
published books on the topic, and discussion of the intended audiences and
market for the book

Statement of the anticipated length of the manuscript; plans for tables,
figures, or other illustrations; and schedule for completion

If your manuscript is based on a dissertation, please discuss how the
material and research has been developed, reframed, or otherwise revised.

## Theme and Argument

Plain text is a file format and a frame of mind. A fundamental concept in the
development of computing, plain text stands in opposition to closed platforms,
rarefied knowledge, and black-box devices. Instead, it offers a vision of data
that is human-readable by design: portable, concise, and universal. This book
contains an argument for plain text. In doing so, it seeks to convene a
community interested in reflecting critically on the ideas, tools, and
practices that shape our daily encounter with computation. My argument starts
with foundational principles at the intersection of media theory and
information science. I ask: what is at stake in the difference between digital
and analog media?  What contains more information a novel or a block of wood?
What separates meaning, form, and formatting? How do pixels form into texts?
Where does data end and meta-data begin? To what extent media determine the
message? The formulation of these broadly theoretical concerns about the
poetics of human-computer interaction opens the way to a discussion about the
social impact of textual technology, as it relates to applied dynamics of
online agency, deliberation, consensus-making, and dissent.

A secondary aim of this volume is to convince computer "users" to view their
computational environments as a literary system of sorts. I mean a "literary
system" in opposition to what one might conventionally mistake for a "binary"
or "digital" one, however imprecise those terms are in everyday use. In
clarifying usage, I will ask mere "users" to become close readers, thinkers,
and makers of technology, able to apply the same critical acuity to reading
code and platform as they do to close reading of prose and poetry. Ultimately,
the book makes a case for the recovery of textual roots latent in the
mechanisms of modern computing.

## Field Significance

The book aims to redress a weakness in the field of Digital Humanities,
particularly as it relates to literary studies. Much scholarship in this space
is often criticized for being ahistorical or atheoretical, abandoning the deep
traditions of literary theory and criticism, even where such traditions would
help bolster the case for emerging scholarship. The nominally related field of
New Media Studies has the opposite problem. It is theoretically sophisticated,
but often produces research that is far removed from the actual practice of
using new media (the archetypal example given by Katherine Hayles is one of a
contemporary photography critic not familiar with the use of the "layers" tool
in Photoshop). I situate *Plain Text* at the intersection of theory and
practice: somewhere between "technological foundations for new media studies,"
and "philosophical bases for computing in the humanities."

## Existing Literature

In situating computing as critical, textual, and literary practice, I hope to
make a theoretical intervention in the cluster of fields sometimes designated
as "media studies," a cluster that can include subfields like "science and
technology Studies," "new media studies," "platform studies," "critical code
studies," and "media archeology." Recent comparable books in these fields
include *Paper Knowledge* by Lisa Gitelman published by Duke UP in 2014,
*Coding Freedom: The Ethics and Aesthetics of Hacking*, by Gabriella Coleman
from Princeton UP in 2012, *Mechanisms: New Media and the Forensic Imagination*
by Matthew G. Kirschenbaum from MIT Press in 2012, *Files: Law and Media
Technology* by Cornelia Vismann from Stanford UP in 2008 and several titles in
the *Electronic Mediations* series at Minnesota UP, which published Lori
Emerson's *Reading Writing Interfaces* in 2014.

My work differs from these volumes in that it is both more philosophical
(rather than strictly "theoretical") and more grounded in the fields of
software an electrical engineering. To give you an example of what I mean,
consider my first chapter, "Phenomenology of a Photocopier." It begins with the
discussion of the Hegelian distinction between "form" and content," ending with
a case study from the history of word processing, in a technical explanation of
the way "content" and "form" are encoded into modern HTML and Markdown
documents. Similarly, my third chapter, "We Have Always Been Digital," begins
with a discussion of "digital representation" as philosophers Nelson Goodman
and John Haugeland define it in the analytic tradition. I proceed by testing
their intuitions on the on the basis of something called the "Soap Opera
Effect" particular to modern Liquid Crystal Displays (LCDs) and in the related
"motion interpolation" technology." The resulting analysis clarifies the
various (and often conflicting) meanings of the word "digital" in media studies
and in the digital humanities.

Complimentary to both of these fields, I prefer to describe my work as
"computational culture studies." I understand the field in two ways: first as
the study of "computational culture," and, second, as computational approaches
to "culture studies". *Plain Text* falls firmly into the first category,
with some elements of the book edging on light computational methods
(particularly if you consider the proposed digital companion, discussed shortly).

Although I do not mean to engage in the debate on disciplinary formation, it is
important for me to insist on the reciprocal motion between the constituent
elements of "computation" and "culture." Too often rhetoric around the digital
humanities resembles a one-way street, in which computational methods are
promised to reform the humanities unilaterally. Books by scholars like
Alexander Galloway (*Laruelle: Against the Digital*, 2014 UMinn Press), David
Golumbia (*The Cultural Logic of Computation*, 2009 Harvard UP), Matthew Fuller
(*Evil Media, 2012 MIT) and Johanna Drucker (*What Is?, 2013 Cuneiform Press)
represent the beginning of a critical counter-movement. But this response too
must be balanced against transformative potential of the digital humanities
program. As was the case with the "linguistic turn" in the decades prior,
almost all fields of human knowledge are now experiencing a turn towards
computational methods, which offer new insight at previously unavailable scales
of analysis. (Witness the emerging fields of computational biology,
computational chemistry, computational linguistics, computational geometry,
computational archeology, computational architecture design, computational
philosophy, computational social science, and the list goes on.)

## Audience and Market

I strive to attain the balance balance between advancement and critique in my
teaching, in Columbia University courses like *Code & Poetry* (Fall 2014),
*Computing in Context* (Spring 2015), and *Foundations of Computing for
Journalists* (Summer 2014). My courses attract students from sciences and the
humanities alike. I lecture widely in language departments and in schools of
engineering on the importance of bridging the gap between the two cultures. As
a natural continuation of these efforts, I mean for *Plain Text* to reach the
same two key audiences.

The first comprises digital, media, and literary scholars interested in the
material aspects of knowledge production. The second is composed of "knowledge
workers" that do not usually view their everyday practice in its historical,
philosophical, or political contexts. Software developers, graphic designers,
system administrators, and project managers routinely affect technologies that
have deep cultural significance: from the ways in which we relate to our family
and friends to the formation of shared cultural archives. For this reason,
technical decisions like choosing a text editor, a filing system, or a social
networking platform cannot be adequately addressed in shallow instrumental
terms limited to efficacy, speed, or performance.

As a former fellow, and now a faculty associate of the Berkman Center for
Internet and Society I am inspired to reach for a wide audience 


## Format

The book is a book, but also a tool. Readers will get much more out of it if
they are able to actively follow along using their terminal emulator of choice.
If these words mean nothing to you, rest assured that the text assumes no prior
technical knowledge. It can be read sequentially as a conventional piece of
scholarship in textual theory or "new media" studies. For those willing to take
the plunge, I will often illustrate abstract theoretical concepts by asking the
reader to type some commands into their terminals. Detailed instructions on how
to set up this "augmented" reading environment, tutorials, and explanations can
be found in the technical appendix and on the forthcoming companion website.

Both novices and experts alike can benefit from exposure to ideas in the
command line, on the level of the operating system. At the very least, the
reader will walk way from this book with concepts and skills foundational to
computing as critical thought and critical practice. But I hope that many
readers will go beyond the basics, gaining deep-seated, intuitive, "hands-on"
understanding of operational concepts like files, filing systems, networks,
search tools, servers, and encryption technologies.  Developing an intuitive
understanding of systems that structure so much of our daily activity has the
potential to radically transform one's experience with text, media, and digital
devices.

Unmoored theoretical concepts like "text" and "media" gain a palpable form when
explored in the context of their instantiation. This is both model and method
structuring the inquiry advanced here. Allow me to spend the next few
paragraphs in laying bare the reasoning and the history behind this approach to
the study technology, texts, and people.

## Theory & Method

The idea that "meaning" is always in some sense "operational meaning" is a
proposition implicit in several related philosophical traditions. The first of
these is pragmatism, broadly conceived. William James articulates that view
when he writes that "reality is seen to be grounded in a perfect jungle of
concrete expediencies [@james-pragmatism-conceptionoftruth]." For James (and,
to some extent, for his fellow travellers in American pragmatism, Charles
Sanders Peirce and John Dewey)[^ln-pragma-truth] the pragmatic answer to the
question of truth could be reduced to the questions efficient causes and
effects. In his essay "Pragmatism's Conception of Truth," James asks: "How will
the truth be realized? What concrete difference will its being true make in
anyone's actual life? What experiences will be different from those which would
obtain if the belief were false?" Frank Ramsey, the young British philosopher
close to Ludwig Wittgenstein, was influenced by the Americans and would later
write that meaning "is to be defined by reference to the actions to which
asserting it would lead [@ramsey-foundationsofmath p.155]."

[^ln11-pragma-truth]: For a more thorough discussion on the topic see
@seigfried, @pihlström, and @putnam-james-theory.

For the pragmatist, truth-carrying propositions of the shape "X is" (as in,
"the author is dead" or "art is transcendent") beg the questions of "Where?,"
"When?," "For Whom?," and "What's at stake in maintaining that?" Following
James's and Ramsey's pragmatic insight, I will maintain throughout that
abstract categories like "text" cannot possibly be (although they often are)
reduced to a number of essential, structural features. Rather, to borrow from a
conversation on categories in Wittgenstein's *Philosophic Investigations*,
categories denote a related "family" of practices, which may or may not share
in any given familial characteristic
[@Wittgestein-philo-invest].[^ln11-more-witt] To visualize this "familial"
model, imagine a Venn diagram, where overlapping fields (of textuality, in our
case) intersect and diverge in a historically (culturally, practically)
contingent and arbitrary ways. These fields lie in relation to specific
communities of practice, which often do not in themselves employ a controlled
vocabulary. What counts for "code" and "poetry" in one domain, like computer
science, may not account for the very same in another domain, like creative
writing. An engineer's evocation of code as poetry can diverge from a poet's.
There's no sense in trying to reconciling divergent languages, where concepts
like "poetry" exist only in their social instantiation. The language of poetry
morphs from literary period to literary period: those who write code by day and
poetry by night might employ differing if not outright contradictory
vocabularies.

[^ln11-more-witt]: For more on the connection between Wittgenstein and James
see @goodman-wittandjames.

The intellectual legacy of pragmatism is wide-ranging and diffuse. It is
perhaps most pronounced in the teacher colleges, where James and Dewey are
still read widely, which could explain the ascendancy of such pedagogical terms
as "situated cognition"[^@lave&wenger, @johnseelybrown] and "experiential
learning"[^@kolb]: both terms denoting some sense of necessary synthesis
between of knowing and doing. In the field of linguistics, philosophy of
language, and communication studies, pragmatics are well-encapsulated by the
"language-as-action tradition," which harkens back to the Oxford language
philosophers like J.L. Austin, Paul Grice, and John Searle [^@Trueswell].
Austin's "How to Do Things with Words," is perhaps the paradigmatic formulation
of the idea that words don't just mean things, but that they enact change in
the world.

When applied to task of writing media theory, history of science, or the
philosophy of technology, the pragmatic tradition suggests we move beyond
intellectual history, that is beyond mere words, into the examination of
real-world materials, practices, and institutions that sustain ideas.

Several broad intellectual movements tangentially related to pragmatism
influenced my approach to writing this book. The first is experimentalism.
Writing in the mid-19th century against the tradition of inductive
"generalizers," Claude Bernard, a pioneer in experimental medicine, argues for
the necessity of both theory and practice. "We cannot separate the two things,"
he writes, "head and hand." The "science of life" he writes, "is a superb and
dazzlingly lighted hall which may be reached only by passing through a long and
ghastly kitchen ." "We shall reach really fruitful and luminous generalizations
about vital phenomena only in so far as we ourselves experiment and, in
hospitals, amphitheaters, or laboratories stir the fetid or throbbing ground of
life [^@bernard, p.3-15]."

It is my belief also that the lighted halls of contemporary literary and media
theory can be best through the long and ghastly kitchen of everyday practice.
Take the example of a media scholar analyzing the last two decades of film
production or photography without grasping the fundamentals of electronic
photodetectors, RAW image formats, complementary metal–oxide–semiconductor
(CMOS), digital editing tools, computer-generated imagery (CGI), or Photoshop
image manipulation techniques. Such a study is in great peril of being terribly
misguided by theoretical models that have no basis on reality. It is not that
one cannot say anything about photography without knowing these things, but
rather that one can say much more when he does. To my mind, theory must be
continually checked and refined against practice, just as practice must be
continually checked and refined against generalized insight. Similarly, it is
my contention here that the fundamental theoretical concepts driving literary
studies--word, text, narrative, discourse, author, story, book, archive--are
thoroughly enmeshed in the underlying physical substratum of paper and pixels
(but also ink, wood, and integrated circuit). These operational concepts cannot
attain their full expressive potential without an internalized understanding of
the technology and the daily practice that gives them rise. This book is an
attempt to develop knowledge "at hand" and "fingertip knowledge" (both
discussed in the later chapters).

It is likely that this line of reasoning is itself a part of experimental and
material "turns" steering the academy toward critical practice, especially in
fields long-dominated by theoretical reflection. The turn represents a
generation's dissatisfaction with "armchair" philosophizing.  Recall the
burning armchair, the symbol of "experimental philosophy" movement proposed by
Joshua Knobe and Shaun Nichols, who write that "many of the deepest questions
of philosophy can only be properly addressed by immersion oneself in the messy,
contingent, highly variable truths about how human being really are
[^@knobe-nichols, p3]." In the field of media and literary studies, it is almost
impossible to avoid the influence of "archeology of knowledge," as advanced in
its many permutations by Michel Foucault and his followers. Yet, such
archeologies deal with "artifacts" and "excavations" only as metaphors for what
remains, methodologically, a history of ideas.

In the recent decade or so, a number of scholars are "making good" on the
metaphor by turning their attention to actual artifacts and excavations, in
what sometimes they dub as the history of craft or "artifactual knowledge." In
preface to a recent volume on *Ways of Making and Knowing*, edited by Pamela
Smith, Amy Meyers, and Harold Cook, the editors write that the "history of
science is not a history of concepts, or at least not that alone, but a history
of the making and using of objects to understand the world [^@smith, p12]." As a
historian of science in the Early Modern period, Smith translates that insight
in the laboratory, where along with her students she bakes bread and smelts
iron to recreate long-lost artisanal techniques. The major insight from Smith
and her colleagues is that traditional "book" knowledge--the kind of
information that finds itself into novels, textbooks, and technical
manuals--represents only a small part of the sum total of human expertise. Much
of our knowledge is instead secreted into the artifacts and institutions where
it unfolds in daily practice. For literary and media scholars interested in key
operational concepts that means supplementing theoretical insight with a robust
sense of curiosity about the world. Digital technology, from typesetting
software to e-book readers and word processors, shapes our everyday encounters
with literature and textuality. That medium, as I will argue throughout, should
not be taken as a value-neutral conduit of information. Typesetting software,
e-book readers, and word processors contain in themselves implicit models of
text and discourse-formation.  They very literally contain system-level
definitions of what a word is or what counts for a document. It is our job then
to recover latent forms of textuality still extent on devices from mobile
phones, to laptops, and super computers, and to expose them to critical
interrogation. The task of media archeology on the level of the operating
system is a literary scholar's version of baking bread and smelting iron.

### Materialism

Finally, this book, and any notion of critical practice, owes a debt to the
legacy of critical theory. In the past few decades, the project of critical
theory (and related "schools" like cultural studies) has lost some of its
evocative power. Rather than rehashing a dry academic debate, allow me
enumerate some reasons for its decline in my own thinking. The first is the
movement's overt political goals. Patently the "stock" of Marxism, socialism,
communism and related ideologies has declined. Major critical theorists like
Roberto Unger and Michel Berube are now legitimately writing about the left's
political crisis [^@cite]. Moreover, the political aspirations of critical
theorists were always somewhat difficult to defend in the face of other,
contradictory academic values like objectivity, neutrality, and critical
thinking. However problematic those terms are in themselves, we must
acknowledge that they represent a set of deeply-seated beliefs about the nature
of scholarship. Already present in Socratic or Confucian models of rhetoric,
these values place an emphasis on questioning received knowledge and on
empowering students to arrive at their own conclusions. In that light, the task
of critique should be to expose political assumptions rather than to promote a
particular political ideology.

As journals, departments, and libraries struggle financially, a whole industry
of middlemen thrives on the monetization of knowledge that rightly belongs to
the public domain. Libraries spend inordinate amount of money to essentially
buy back the research produced within their own community. Academic journals
that operate on principles of peer review and volunteer labor are then entered
into private circulation. Prices of $30-60 per article in the humanities are
not unusual. In perpetuating these conditions we reduce the notion of critique
to a meaningless rhetorical trope. The examination of our own immediate
material contexts of knowledge production and dissemination are crucial to
conversations about "world literature," "public discourse," "collective
memory," or "politics in the archive."

*Plain Text* is an attempt to repay the debt of materialism. The alienation, as
I will argue here, begins with the roots of my profession: namely the
production of textuality in everyday life. It is quite likely that most of
readers spend the majority of their waking hours in front of a personal
computer, typing letters on a screen (among other things worthy of their own
examination, but outside of the scope of this book). My goal then is to reclaim
the ordinary material contexts of a dominant mode of knowledge production and
dissemination. It is one thing to theorize about notions of form and content,
and it is quite another to see how form and content are encoded in .txt and
.pdf formats and to further how these distinctions then affect material
divisions of labor between "knowledge workers," "content producers,"
typesetting sweatshops, and international conglomerates that control vectors of
literary distribution.



## Structure

I tend to write concisely--a style that I think fits well with the subject
matter, and something that should appeal to the audience. At this point, I am
aiming for a manuscript of around 60-80k words, which would allot around 5-7k
words per chapter (around 20-30 book pages).

### Part I: Text

The first three chapters constitute a part of the book concerned with
textuality itself, building the vocabulary necessary for a discussion of
textual technology in Part II and the resulting social structures in Part III
of the book.

#### Chapter 1: Phenomenology of a Photocopier

Chapter 1, "Phenomenology of a Photocopier," deals with the confused history of
the distinction between form and content. I find that going back to Plato and
Hegel, "form" is at times used to indicate physical structure, and, at other
times, to indicate immaterial categories in the ideal realm. A critical
treatment of a more contemporary conversation on "surface" and "depth" of
meaning reveals form as a mediating concept between thought and matter. A case
study in extreme surface reading, in the bowls of a photocopier, opens the way
to the distinction between print, where matter, form, and content are literally
fused, and screen, where the three layers come apart, providing only the
illusion of flattened textuality.

#### Chapter 2: Literature Down to the Pixel

The second chapter, "Literature Down to the Pixel," begins the work of moving
from "high level" concepts like "text" and "literature" down to their atomic,
constituent elements like fonts and the modulation of electric current. I argue
here that the concern with value in literary criticism detracts from the
explicit movement of control and power intimately connected to digital
textuality. Unlike scholars in the foucauldian tradition (who often trace the
machinations of power through discourse), I concentrate my analysis on
mechanisms of control at the material roots of literary practice. In doing a
media history through primary sources on early development of modern computing,
I show the explicit admixture of content and code: one meant to communicate
messages to humans and the other to program universal machines. This history is
not entirely critical: rather, it reveals a genealogy of computing alternative
to the widely held notion of computer as a device for reductive "mathesis" (in
the words of Johanna Drucker). I argue that the Turing machine is anticipated
not just by the Babbage calculator, but in a series of advances in
communications, word processing, and media storage. A notion of text
(as opposed to number) is "baked into" the system.

#### Chapter 3: We Have Always Been Digital

The "microanalysis" in the second chapter, gives way to the opposite movement:
from pixel, to text, and up to meaning. The third chapter, "We Have Always Been
Digital," begins with popular intuitions about the "look and feel" of digital
aesthetics. Discourse around the digital humanities employs the term most
central to its stated research program inconsistently and often without
theoretical reflection. A case study of television motion blur (and the related
"soap opera effect") undermines the initial ease with which notions of the
digital are overdetermined to stand in for a range of often conflicting
modalities. The next section deals with the analytic tradition of dissecting
media into analog and digital categories (Nelson Goodman in particular). My
summary of that tradition shows that language and text are already "born
digital," that is, discrete and differentiated throughout by the analytical
definition. Furthermore, digitality depends on "reliable processes of copying
and preservation." From that insight I take it that "being digital" is not an
ontological condition, but rather imposed structure that manifests in specific
material affordances. "Is it copyable?" becomes "can I copy it?" Digital being
ends up being an instrumental condition imposed onto the "user" of media (and
not property of the media itself). The chapter ends with a history of encoding,
where the distinction between binary and plain text formats supersedes the
dichotomy between digital and analog.

### Part II: Technology

#### Chapter 4: The Media Is Not the Message

Media is defiantly not the message. Sources of that confusion and what's at
stake. Encoding of the media on magnetic storage. A revisiting of modalities
written and oral.  Derrida and Ong.

The next chapter (not attached) will discuss differences between media, mode,
and message in the confluence between semiotics (Saussure and Peirce) and
information theory (Shannon, Weaver). The media is resolutely not the message,
I argue.

#### Chapter 5: Freedom of Information

Containing an argument against the "systems" definition of information advanced
by Shannon and Weaver. In what Shannon calls a "strange feature" of this
communication theory, information is defined as amount of "freedom" or entropy
in the system. By contrast, I want to insist on the agency (freedom) of the
sender and the receiver. Incidentally, we get some clarity on the differing
ways in which information is invoked in different discourses.

#### Chapter 6: Bad Links

Why links are bad. The long history of intertextuality. The excitement of the
90s about it. The intertextual art of Gwern. Erudition and analogical thinking.
The difference between hard-coded and symbolic links. Just a bit about the
nature of knowledge or what is meant by "I've read that book." Snapshots, the
Internet Archive, and the future of Wikipedia.

### Part III: People

#### Chapter 7: Recursive Encounters with Oneself

Understanding the document as a vector. The problem of drafts and versions.
What is being transmitted through the vector? The appointment with one self:
Beckett and Sartre. Pipes and I/O serialization.

#### Chapter 8: Shared Knowledge

Text as an interface between human and machine. The notions of an interface
(Galloway). Incompatible modes of understanding. Anne Finch and the liminal
space between species.  What you see is not what you get.  Isomorphism. Search.
The brief moment of desktop publishing. Pen, typewriter, and word processor
(with detours to Kittler and Heidegger).

Writing together. Models of co-authorship (and why we should pay attention).
The massively multi-authored online novel (Wu Ming and Lo zar non è morto).


#### Chapter 9: Secrecy, Surveillance

Encrypted literature. Surveillance and counter-surveillance. Notions of
textuality as embroiled in contemporary ideas of privacy, secrecy, and
transparency.

#### Conclusion: New Humanism

Computation does not necessarily work for the military-industrial apparatus (as
argued by Golumbia, Lennon, and McPherson). Recovering and preserving
textuality in computing. Engineering for dissent.

### Tech Appendix

## Timeline

I keep detailed logs of my daily writing practice. During the academic year, I
average around 250 words per day, a number that more than quadruples when freed
from teaching obligations. Assuming a manuscript of around 80k words (and
discounting the fact that large portions of this book are anticipated in my
dissertation), a conservative estimate of my schedule would place the final
draft somewhere towards the second half of summer, 2015.

