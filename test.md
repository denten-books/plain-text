
## Chapter 2: Literature Down to the Pixel
`literature, content, control`

### 2.0 Abstract

Having established the grounds for digital textuality in the history of
character encoding, I begin the work of moving from "low level" first-order
concepts like "text" and "code" up to second-order concepts like "literature"
and "canon." The chapter starts by developing a theory of "microanalysis," the
closest possible kind of reading that pays attention to the material contexts
of knowledge production. I argue here that the concern with value in literary
criticism detracts from the explicit movement of control and power intimately
connected to digital textuality. Unlike scholars in the foucauldian tradition
(who often trace the machinations of power through discourse, on the level of
representation), I concentrate my analysis on mechanisms of control at the
material roots of literary practice. In doing a media history through primary
sources on early development of modern computing, I show the explicit admixture
of content and code: one meant to communicate messages to humans and the other
to program universal machines. This history is not entirely critical: rather,
it reveals an alternative genealogy of computing, contrary to the widely
accepted notions of computer as a device for reductive "mathesis" (in the words
of Johanna Drucker). I conclude to argue that Turing machines were anticipated
not just by the Babbage calculator, but in a series of advances in
communications, word processing, and media storage. A notion of text (as
opposed to number) is hence "baked into" the system.

### 2.1 In Search of a Subject

"Media determine our situation," Friedrich Kittler wrote in his seminal
*Gramophone, Film, Typewriter*[@kittler1999, xxxix]. The book channels its
metaphysical angst about the changing conditions of literary production from
the typewriter into the personal computer. Kittler concludes the book to say
that "under the conditions of high technology, literature has nothing more to
say [...] an automated discourse analysis has taken command [...] and while
professors are still reluctantly trading in their typewriters for word
processors, the NSA is preparing for the future: from nursery school
mathematics, which continues to be fully sufficient for books, to
charge-coupled devices, surface-wave filters, [and] digital signal processors
[@kittler1999, 263]." I share Kittler's interests in books, charges, waves,
filters, and signals but not what his translators call his penchant for
"technological apocalypse [@kittler_gramophone_1999, xxxiv]." Those who knew
Kittler personally often insist that his sometimes giddy trans-humanism was
rather a playful stance, performed in the spirit of Nietzschean irony
[@conway_solving_1988]. Whatever the author's intention, Kittler's text leaves
little room for apathy. Readers must either acquiesce to automated discourse or
themselves take command. As Geoffrey Winthrop-Young and Michael Wutz put it,
Kittler's work highlights the reader's inability to even pose (much less
answer) "the question of the subject [@kittler_gramophone_1999, xxxiv.]"

I take Arno Schmidt's letter (which concludes Kittler's book), then, as an
early waypoint (and a provocation) on the road to recovering the subject--a
sense of communal "us" in Kittler's lament about data flows "once confined to
books" but now increasingly "disappearing into black holes and black boxes
[...] as artificial intelligences are bidding us farewell on the way to
nameless high commands [@kittler_gramophone_1999, as xxxix]." It does not have
to be that way. For now, the action of the key press seems to vanish into a
machined rabbit hole, only to reappear shortly as a remote alphanumeric
character on someone else's screen. The task of microanalysis will be to
recover a measure of material context underlying textual production and
dissemination. To trace the flows of governance and control that lurk beneath
(and of which Kittler warns us) must mean more than passive apprehension of
footprints indicating the "evanescent absence" of life ("the sign about which
Robinson Crusoe would make no mistake", in Lacan's words) [@lacan_seminar_1997,
167], but rather it would entail the proactive deployment of tools like
*tracert*, *pcap*, *ssh*, or *traceroute*: tools that "hop" across, "sniff"
packets in, burrow through, survey, traverse, and flood network topographies,
channeling itinerant streams of data back into mangroves of readability and
comprehension *for* readers and writers (as the very subjects whose loss
Kittler laments). Only in these encrypted tunnels and secure shells can
anything like the digital humanities take root.

"Algorithms are inherently fascistic, because they give the comforting illusion
of an alterity to human affairs," Stephen Marche wrote in his now
widely-discussed piece for the Los Angeles book review. "Algorithms have
replaced laws of human nature, the vital distinction being that nobody can read
them," he concludes [@marche_literature_2012]. But although a number of
prominent voices (that include Stephen Hawkins, Catherine Hayles, and Elon
Musk) have independently echoed Marche's metaphysical concern about algorithmic
alterity, his insistence on the elision of the subject does not strictly hold
true in the practice of writing and reading algorithms. But such writing and
reading entails forms of literacy beyond Marche's "handmade insight." The
question then becomes not one of alterity (as laws are always extraneous to an
individual), but one of legibility. An algorithmic regime redraws lines of
human agency, influence, and prestige, moving power towards a privileged class
of readers and writers capable of comprehending and further emending this
otherwise obscure form of regimental textuality. It is vital then to expand
notions of human literacy to include the ability to read, write, and comprehend
algorithms.

We are accustomed to think of modern computing as a pinnacle of calculating,
computational devices. That is the story the reader gets in Marche's essay, in
Kittler's media history, and in David Golumbia's provocative *Cultural Logic of
Computation.* The cultural logic of computation inevitably leads to a bleak
vision of of Johanna Drucker calls "mathesis," the triumph of deterministic
quantification over indeterminate, anti-positivist forces of the humanities.
The historical links between computation and the military industrial complex
cannot be denied. But the links are contingent rather than necessary
connections. In moving towards a notion of critical computing, we are able to
draw on a multiplicity of historical practices that suggest an alternative mode
of engagement with the computational environment, one in which literature has
plenty to say.

### 2.2 Microanalysis

In a study on the construction of scientific knowledge, Bruno Latour and Steve
Woolgar perform what they call an "ethnography" of a science lab, showing the
passage of ideas from applied experiment to textual inscription. For Woolgar
and Latour, the laboratory functions as a factory of sorts, ingesting matter
and artifact to produce fact [@latour1986]. That process of inscription comes
to the fore explicitly: the output of science lab is, in some real ways,
measured by its publication record [@latour1986]. A scientist may object that
her publication record is merely a byproduct of her research activity. Yet
Latour shows pretty convincingly that the laboratory participants closest to
the material conditions of knowledge production are at the same time, most
marginalized members of the group. Lab technicians handle the matter of mass
spectrometers and bioassays. Ph.D. holders, by contrast, spend most of
their time handling inscription devices like printers and computer consoles.

Although much cited in the study of culture, science, and technology,
*Laboratory Life* had curiously little impact on the study of literature, even
though Latour and Woolgar borrowed much the other way around. In the 1986
postscript to the American edition, they remark on the broad trend in literary
theory to treat texts as objects of interpretation, disavowing the kind of
criticism aimed at the "real meaning of texts," at "what the text says," "what
really happened," or "what the authors intended [@latour1986, 273]." Similarly,
research in the social study of science should be aimed at the "contingent
character of objectification practices." Science, on their view, is itself a
type of a literary system, "generating texts whose fate (status, value,
utility, facticity) depends subsequently on interpretation [@latour1986, 273]."
Throughout the book, Woolgar and Latour use the term literature to mean
something "inscribed, printed, or published," a combination of verbs that
coincides with "literature" consistently throughout [@latour1986, 47-53].
Viewed as a work of literary theory, *Laboratory Life* reifies and advances the
research program started by the post-structuralists. Where Jacques Derrida could
offhand remark that "everything is a text," Woolgar and Latour make concrete the
literal transformation of matter into text. No longer was literature an
illustration of something that happens in the laboratory: Latour and Woolgar
showed the laboratory to function as a literary system.

The conventional way to understand Woolgar and Latour brings the interpreter
into the laboratory, breaking the illusion of scientific objectivity: instead
of "discovering" facts, scientist socially construct them. Paradoxically, as
Latour and Woolgar worked to undermine the empirical unity of science (turning
it into a kind of hermeneutics), they performed literary analysis as an
empirical method. To "read" the laboratory, the researchers had to be there, in
person, observing and taking notes. The laboratory opens up to them through a
careful micro-ethnography. The reader gets to examine floor plans, research
samples, dialog transcriptions, publication patterns, and other evidentiary
materials.

To view Woolgar and Latour as scholars of literature is to open up the
floodgates of textuality. The laboratory, the court, the hospital--these
institutions cannot be reduced to text, but they are in part, literary systems.
Beneath every scientific discovery, every popular song, every film, radio, or
television serial, every art or architectural project, every political office,
every legal judgment, every restaurant meal, every doctor's prescription, every
website, and every machine manual--beneath that giant and perplexing pile of
human activity lies a complex network of textual material. The task of the
literary scholar becomes to make visible that textual undercurrent, to follow
and to map its movements across culture, to expose the mechanisms of its
mutation and locomotion. It is an exciting prospect: to view the hospital as a
library, where cadres of physicians and administrators convert human stories
into diagnoses, into controlled vocabularies, into files, into billable codes,
into inscriptions, and into archives. Or to see the legal system as a process
of literary discovery, were legions of attorneys and paralegals pore over
terabytes of textual material, transforming inscription into evidence. That is
not all these institutions are, but they are also that.

Such textual proliferation comes at a cost. Until recently, the bread and
butter of literary scholarship has been close reading. Close reading like
critical thinking is an idea that is easier to perform than to explain, because
the details become contentious as soon as they are
formalized.[^ln12-closereading] I will tread carefully by committing myself to
some notion of deliberate and reflective practice aimed at deep comprehension.
The Partnership for Assessment of Readiness for College and Careers (PARCC),
the organization responsible for the Common Core standards being implemented in
schools across the United States today, promotes close reading as "thorough,"
"methodical," and "analytical" practice that "enables students to reflect on
the meanings of individual words and sentences; the order in which sentences
unfold; and the development of ideas over the course of the text, which
ultimately leads students to arrive at an understanding of the text as a whole
[@PARCC2012, 7]." The general movement here is from "text" on the page to
"work" (text as a whole, in the language of the report). The model of
textuality implicit in the project of close reading assumes an environment of a
well-defined literary canon, naturally accessible to the human intellect. For
the duration of the "Gutenberg galaxy," the age of print, a well-educated
person might have been expected to internalize some several hundred or perhaps
thousands major texts constituting the canon.

The expansion of the textual field and its relative liberation from physical
media have radically increased the cognitive demands of literary engagement.
The pipeline between text and work has lengthened considerably. On the one
side, the matter of canon formation can no longer be relegated to stable,
long-term systems of social filtration. Seen form the perspective of a literary
interface, the database, the social stream, and the search engine are tools for
dynamic, "on the fly," generative canon-formation. Consider the task of finding
an unknown factoid online, as was the case with my previous aside on the status
of literacy in medieval Muslim world. Not being a medieval scholar, I construct
a search query, using resources that I believe will return a reasonably
authoritative list of texts on the subject. The search engine in effect
substitutes centuries-long processes of canon formation, for a
near-instantaneous list of results that become my ephemeral, but nevertheless
authoritative, collection of relevant literature. Each text still requires the
instrumentation of close, analytical interpretation. However, the same ideals
need to be exercised on the higher level of orchestration. Where to search?
Using what engine? How to construct the query?

The academic question of canon-formation, transforms into a (not yet critical)
practice of rapid, iterative, generative canon making. Whatever ideals motivate
close reading between "text" and "work" surely must drive the process on the
level of corpus composition. Almost every field of human activity has responded
to the condition in which canons are no longer accessible, in their entirety,
to the unaided (natural) human intellect: distant reading and macroanalysis in
literary studies[@jockers2013, @moretti2013], culturomics in economy
[@aiden2014], e-discovery in law [@capra2009], and medical informatics in
medicine [@shortliffe2012], among others. At the foundations of these nascent
disciplines is a shared toolkit of statistical natural language processing
[@manning1999, @jurafsky2008], automatic summarization [@nenkova2011], machine
learning [@flach2012], network analysis [@], and topic modeling [@blei2012].

```

                       .------------------ canon
                       |                   corpus
                       v                   database
                                           search
        Author > Text > Reader > Work      library
                                           recommendation engine
               ^               ^
MICROANALYSIS  |               |           DISTANT READING (MACROANALYSIS)
               |               |
                               |  CLOSE READING
    personal computer          |
    file system                . _ _  interpretation
    word processor                    construction of meaning
    file format (encoding)
    physical media (paper and pixel)

```

Where distant reading and macroanalysis are concerned with text aggregates,
microanalysis, of the type suggested (but not carried out to its logical
conclusion) by Latour and Woolgar, occupies the other side of the text-work
equation. There's relatively little space between author, text, and, reader in
the traditional model of literary transmission. Were I to write these words in
front of you, on paper, I would simply pass the page into your hands. In
receiving this text, you could be fairly certain that no third party meddled in
the process of passing the note. When communicating in this way, it makes quite
a bit of sense to talk about notions like "authorial intent" and "fidelity of
the original," because the author and his work occupy contiguous space and
time. The advent of cheap mechanical reproduction of print (for the sake of
brevity let's say the printing press) introduces a range of instrumentation
that mediate between the author and the reader. Distance, time, and mediation
weakens any notion of fidelity and authorial intent. At the very least, we know
that editorial practices, publishing markets, and communication technologies
introduce an element of noise into the original text. At worst, long-distance,
asynchronous communication is susceptible to "man-in-the-middle" attacks, by
which the content of communication is maliciously altered by a third party
[@needham1978].

Changing material conditions of textual transmission push against the familiar
literary critical conceptual apparatus. For example, as mechanical reproduction
of print weakens the material basis for authorship attribution, the notion of
authorship itself undergoes change. That is not to say, again with Barthes,
that the author dies. Authors continue live and collect royalties from the sale
of their works, but the reconfiguration of the authorship function makes
certain ways of talking about things like "authorial intent" and "fidelity to
the original" difficult to sustain. Massively collaborative writing projects
like Wikipedia and procedural narrative generation further erode ideas of
authorial production based on personal human agency.

The aim of microanalysis then is to reconstruct the material conditions of
textuality, paying sustained attention to the atomic particulars of encoding,
transmission, storage, and decoding of text at the sites of its application to
the human experience.

[^ln12-survey]: I can only give anecdotal evidence here, as I often put this
question before my graduate students at the beginning of the semester,
with the reported results.

[^ln12-closereadging]: See [@lentricchia2003] and [@fish2011].

### 2.3 Ghost in the Machine

The reading of short stories, novels, poetry, and plays is at grave risk,
concluded the last survey of Public Participation in the Arts conducted by the
Census Bureau on the behest of the National Endowments for the Arts (NEA). "For
the first time in modern history, less than half of the adult population now
reads literature, and those trends reflect a larger decline in other sorts of
reading. Anyone who loves literature, or values the cultural, intellectual, and
political importance of active and engaged literacy [...] should be gravely
concerned [@readingrisk2004]." I, for one, am not concerned about the report,
because the numbers about other forms of reading--textuality at large--tell
quite a different and an entirely more optimistic story.

On an average day in 2008, at home, an average American read around 100,500
words a day. At 250 words per page, that is around 402 printed pages. Between
the years of 1980 and 2008, the consumption of information in bytes--a measure
that would obviously privilege storage-heavy content like sound and video--grew
at a modest 5.4% per year. Reading, in decline until the advent of the
internet, has tripled in the same period. Reading in print accounted for 26% of
verbal information consumed in 1960. That number fell to 9% in 2008, but the
consumption of words digitally increased to 27% of total consumption, which
means that reading has increased its share of the overall household attention
span [@bohn2009, @hilbert2012]. The first decade of the 21st century saw a 20%
increase in library visitation [@imls2013]. According to UNESCO data, literacy
rates continue to rise globally, as the world calibrates imbalances of access
along the lines of gender and geography [@unesco2013]. By a conservative
estimate, the number of scientific publication grows at about 4.7% per year,
which means that amount of published research roughly doubles every 15 years or
so (and the numbers are much, much higher in some fields) [@larsen2010,
@archambault2005, @crespi2008] The number of books published in the United
States almost tripled from 2005 to 2009 [@bowker2010]. All measures point to a
drastic expansion of the textual field.

Something does not add up. As a society we are reading, writing, and publishing
more each year. With these figures in mind, I ask: What is at risk, literature,
or merely one of its many definitions? Does the diffusion of the textual field
somehow cheapen the literary enterprise of art for art's sake, or protect it?
What is the source of modern metaphysical anxiety about the status of a
literary text?

In the 1990s, the ire of the metaphysically disaffected coursed against visual
culture and television [@stephens1998, @merrin199]. A decade later it turned
against the internet and computation, which to many threaten the very
foundations of humanity and the humanities [@golumbia2009, @marche2012,
@fish2012].[^ln12-internetet] My thesis, presented here and throughout, is that
the aims of such metaphysical angst are misaligned. Reconstructing the physical
conditions of textuality, the work began in the first chapter of the book,
makes plain the profound alienation from material contexts of literary
production on the part of readers and authors. Although literacy thrives, the
very nature of what constitutes a literary text changes with the advent of new
material conditions. Changing circumstances destabilize established modes of
literary production, access, and distribution tied to the circulation of paper
and ink.

[^ln12-internet]: The NEA study has this to say on the topic of "What is
responsible for the decline of literary reading?": "If the 2002 data represent
a declining trend, it is tempting to suggest that fewer people are reading
literature and now prefer visual and audio entertainment. Again, the data –
both from SPPA and other sources – do not readily quantify this explanation
[...] the Internet, however, could have played a role. During the time period
when the literature participation rates declined, home Internet use soared
[@nea2004, 30]."

In his 2004 *Humanism and Democratic Criticism*, Edward Said wrote about the
"fundamental irreconcilability between the aesthetic and the non-aesthetic,"
which must be sustained "as a necessary condition of our work as humanists."
The aesthetic exists in opposition to "quotidian" experiences that we all
share, writes Said: "To read Tolstoy, Mahfouz, or Melville, to listen to Bach,
Duke Ellington, or Elliott Carter, is to do something different from reading
the newspaper or listening to the taped music you get while the phone company
or your doctor puts you on hold." A corollary to my main thesis is an argument
against that commonly-held belief about the relationship between literature and
aesthetics. To my mind, the distinction is impossible to define, let alone
maintain in practice. Bias in favor of the aesthetic is bias in favor of a
specific, historically- and materially- contingent idea of the aesthetic. That
preconceived idea carries with it a hidden cost, introducing numerous blind
spots into the study of literature at large, limiting the critical task in
scope and relevance. Textual diffusion threatens not the humanity (or the
humanities), but the existing socio-economic order that governs who gets to
access, to interpret, and to archive literature as text and document. I begin
with several propositions then: one that the literary field, understood in
broad terms, is expanding, and two, that with a quantitative expansion comes
qualitative change in the material make-up of the literary text. Where I start
with a top-down, theoretical reconceptualization of literature as an
epistemological category, I end with a bottom-up media history that traces the
passage of pixel into text.

When surveying the introductory literature on literature, the reader will
invariably find a version of Said's "fundamental irreconcilability" position.
For example, in Austin Warren and Renee Wellek's *Theory of Literature* the
authors write that the "term 'literature' seems best if we limit it to the art
of literature, that is, to imaginative literature […] the main distinctions to
be drawn are between the literary, the everyday, and the scientific uses of
language [@wellek, 22]." Similarly, Raymond Williams defines it to mean "mainly
poems and plays and novels," as opposed to other kinds of "serious" writing
that are "general," "discursive," or "the sub-literary [@william1976,
152-153]."[^ln12-engell] Sharing the bias in favor of aesthetic reading with my
colleagues, I would rather spend my free time reading Kincaid or Coetzee than
poring over arcane printer and telegraph manuals. Implicit in that bias is
an idea--professed by Immanuel Kant, Friedrich Schiller, and, more recently,
Tzvetan Todorov and Elaine Scarry--the belief in the connection between beauty
and justice, beauty and the good, beauty and the upright moral life. I wish to
avoid disturbing the foundations of that intellectual edifice, and only want to
point out that, for some, beauty can also reside outside of artful discourse,
and that others find beauty in the strangest of places, and that texts--their
movement within and across cultures--present many problems not exhausted by the
ethic or the aesthetic realms. Reading fiction (of a kind) for pleasure (of a
kind) may indeed be a vanishing pastime, but that is not to say that our
society has gotten interested in literature of all kinds. On the contrary,
whole new disciplines have sprung up to deal with textual saturation: natural
language processing, narrative generation, automatic translation and
summarization, computational text analysis, discourse analysis, corpus
linguistics, and digital humanities among others.

Matters always central to the field of literary studies remain vitally
important to the functioning of modern society. Literature broadly conceived
provides fertile grounds for collaboration between diverse disciplines
interested in the changing dynamics of narrative, interpretation, language,
form, prosody, composition, dialog, discourse, story, genre, authorship,
influence, and text. It is in this core conceptual cluster of operational terms
that I want to locate a notion of poetics, poetics capable of addressing not
only the machinations of poetry and prose, but also of code, document,
inscription, file, record, note, manual, journal, list, script, and archive.
Rather than picky eaters, I imagine my fellow travelers as voracious omnivores
of text.

This is not to say that I advocate ignorance, nor do I follow the Russian
formalist poet Vladimir Mayakovksy who in the beginning of the last century
called on his peers to "cast Pushkin, Dostoevsky, Tolstoy, et.al. overboard
from the ship of Modernity." I hold instead that in the implicit perusal of the
subjectively beautiful discourse literary scholars have abandoned vast and
fecund textual vistas. In privileging the aesthetic, the study of Literature
has tied its fate to that of an increasingly vanishing pastime--the leisurely
reading of poetry and fiction, of a specific kind, holding a privileged form,
sold in preferred manner.

Hampered by an artificially-limited field of activity, the study of literature
excavates a small, sacred patch on tip of an immense textual iceberg.  The
results of all that iceberg-digging sink to join the mass of its submerged and
unexcavated bottom. Texts multiply and produce other texts: that's more or less
the point of Roland Barthes's 250-page exegesis of a 30 page short story by
Honoré de Balzac. Despite my fond appreciation of Barthes as a scholar, I
suspect that exponential growth and diminishing returns will eventually (if not
already) make such relentlessly exegesic mode of scholarship unsustainable. The
value of each new "Barthes" and associated commentary plummets as texts
multiply exponentially, quickly reaching the human limits of comprehension.
There are no winners in such an arm race between footnotes, only eventual
irrelevance.

Non-positivistic disciplines like literary studies lack a methodology for
definitively settling any research program. For this reason, citation patterns
in the humanities journals privilege well-established sources
[@tenen2014-displacement], where the sciences will usually defer to the "last
word" on any given question. Without robust mechanisms for condensing and for
pruning accepted knowledge the archive expands precipitously. Its combined
weight necessarily devalues the literature of the present, creating a curious
kind of a gap in contemporary material. A "modernist" in literary studies
denotes a specialist who works on documents which are now more than a century
old. And most departments of literature cover only a limited range of
best-sellers from the "post WWII period." A competent graduate literature
student can likely name dozen or so contemporaneous literary movements emerging
at the turn of the twentieth century. Few in the field are likely to name more
than a handful at the turn of the twenty-first.[^ln12-survey]

The logic of Said's eloquent defense of the aesthetic merely continues a
tradition of exclusionary distinction-making, which, before Said, denied
writers like Naguib Mahfouz and musicians like Duke Ellington a place in the
curriculum. The logic of exclusion on aesthetic basis places the critic in the
awkward role of a taste-maker, limiting research to texts that in a sense were
already "blessed" by the establishment. Scholars working on "lesser" or
"unknown" texts and writers must expand considerable energy justifying the
aesthetic merits of their chosen subject. At stake in the binary logic of
aestheticism are the very notions of "beautiful" and "quotidian" as markers of
prestige--notions that encourage disciplinary infighting, tug-of-war style, by
which much energy is spent in struggling to pull material from one category
into the other. In the meantime, the shoring up of the aesthetic detracts from
the larger aims of literary scholarship, reducing literature to its ornamental
function, as a "neat illustration" for otherwise empirical accounts of the
human condition.[^ln12-menand]

[^ln12-menand]: I am echoing Louis Menand's "the version of the humanities that
would make many non-humanists most comfortable today is the version in which art
and literature are ornaments on or neat illustrations of empirical accounts of
human life" [@mendand2005, 10-17].

To limit the literary to a historically-contingent ideal of the aesthetic is to
limit the project of literary analysis to a moralizing, prescriptive
enterprise. A whole tradition of Marxist criticism reminds readers that
prescription cannot be ideologically neutral. "Literature does not exist in the
sense that insects do," Terry Eagleton writes in his introduction to literary
theory. "Value judgments by which it is constituted are historically variable,"
he concludes, having "a close relation to social ideologies." For Eagleton,
values are more than simple assertion of private taste, instead they refer to
assumptions "by which certain social group exercise and maintain control over
others [@eagleton1983, 15-47]. Who controls what? My interpretation of Marxist
materialism is considerably more literal than Eagleton's. I am interested here
in physical mechanisms of control, not only in their more ephemeral ideological
justifications.

In trying to understand the incongruity of NEA's dour prognosis with broad
theoretical models and demographic trends that describe the proliferation of
text, I am confronted with two possible explanations. The writers of the report
hint at the first themselves: to treat the ebbing of literary reading (in a
particular mode) as cause and symptom for the general decline of the arts (and
perhaps of civilization itself). I find that reading unacceptable, (1) because
it is depressing and (2) because it smacks of historical exceptionalism. I
strongly suspect that humanity's potential for innovation and creativity has
remained relatively stable through history. The material conditions for
creative expression may change (as in the times of political oppression,
pestilence, or war), where the spirit endures.

The second, much more palatable (and likely) explanation would question the
very definitions of literary reading. Consider the possibility of online or
digital literature that evolves according to a logic of its own, where previous
ideas of "genre" or "literary movements" no longer apply. To restrict literary
reading to "novels, short stories, plays, and poetry" is to deny some measure
of literariness to song-writing, computer game making, software design, and, in
an obvious way, to non-fiction or, let's say, to long stories of arbitrary
length. Imagine, for example, excluding reality television from the survey on
television habits, because the genre does not confirm to some normative sense
of what television should be. Do the survey makers consider literature as a
medium, like television, or a grab-bag of "serious genres" like comedy and
drama? As it turns out: both. The authors of the report concede the point in
sometimes classifying readers by the number of books read in any genre,
effectively equating literature with "books [@NEA2004]." But even that
concession confuses form with function. What if "reading a book" begins to
involve something other than moving one's eyes from left to right over
sequentially numbered blocks of text? Or what if books were more like pills, or
prosthetics, than parchment? The conundrum reveals a strong methodological bias
favoring a definition of literature as specific matter and form. A report that
initially looks like objective, quantitative, descriptive analysis (that's the
way things are) hides a qualitative, proscriptive program (thats the way they
should be).

[^ln12-engell]: See also @engell1988.

Staring with the late 1960s, a generation of scholars (Northrup Frye, Murray
Krieger, and E. D. Hirsch, among others) took up the problem value in literary
criticism in a concerted fashion. Their conversation splintered into three
camps: the separatists--those who viewed the production of value as a detriment
to the discipline; the inseparatists (to coin a term)--those who believed that
value creation is inseparable from the study of literature; and the
compatibilists, or those who attempted to integrate the two opposing positions.
Frye, for one, argued that value production should not be a part of the
scholar's task, because the concept of value itself is "individual,
unpredictable, variable, incommunicable, indemonstrable, and mainly an
intuitive reaction to knowledge." "The more consistently one conceives of
criticism as the pursuit of values," writes Frye, "the more firmly one becomes
attached to that great sect of anti-intellectualism [CITATION NEEDED]."

To this Murray Krieger responded in pointing out the inevitability of value
formation in the work of even the most analytically-inclined critic. Krieger
writes: "The categories [of the literary, the poetic] define their subject and
erect value criteria for admission, so that for the work to attain the
definition is for it to qualify as a valued individual in a valued class. The
work comes to be discovered, defined, and valued as poetry only by a way of a
preexisting generic characteristic which the critic began by adopting as his
perspective glass to envision it." All this to say that even for scholars like
Frye, the process of valuation happens before the encounter with the text. In
designating the poem as a poem, the critic already has brought with him a set
of preconceived notions that distinguish what constitutes poetry, and what does
not; who is in and who is out. On this view, abjuring the task of explicit
value formation borders on the irresponsible.

Critics like E. D. Hirsch offered a compromise between these two positions,
arguing that although value-making contaminates all acts of interpretation, the
scholar can and should adopt an interrogatory stance towards value creation as
such, which ultimately allows her to "form new judgments of value and
significance [@hirsch1968, 331]." In this camp we also find "Contingencies of
Value," by Barbara Herrnstein Smith, published in 1983. At the time, Smith
urged her peers to make the examination of social value-creating mechanisms a
part of their academic discipline. "What are commonly taken to be *signs* of
literary value, are also its *springs*," Smith writes [@smith1983, 30]. And the
springs unmistakably push the whole enterprise towards the Western,
economically privileged, model of the canon. Texts that survive in the teaching
and the selling of literature survive as long as they do not radically subvert
the prevailing ideology. Smith stops short from offering a way out of the
conundrum. For even as the canon grows to include previously excluded voices,
those voices take on the shape of soft acquiescence. The custodians of the
Western canon "cannot grasp or acknowledge" that alternative forms of
literariness and textuality can take on the functions of Dante or Homer for
others [@smith1983, 1-35]. I am appropriately galvanized by Smith's fiery
rhetoric, but must admit that not much has changed since the 1980s. The
compromise does not approach the very peculiarity of the question itself. Must
we value the object of our study?

To understand the peculiarity that question, try asking: Does an anthropologist
observing cock fighting, condone animal cruelty? Does a historian writing about
war, sanction violence? Does a philosopher working on the problem of other
minds, love solipsism? In each case, synonyms for value-making--loving,
sanctioning, condoning--appear out of place, incongruent with the inquiry at
hand. Something is rotten in the very formulation of the question. To put it
differently: Must we study *only* what we value? I cannot think of a way to
defend the answer to the affirmative. A zoology that singularly insists on the
study of beautiful animals is itself a strange creature.[^ln12-zoo] A more
radical compromise compels us to seek value in the importance of the inquiry
itself: in the quality of the questions raised, and in the relevance of the
answers given. If literary reading of a certain kind has the power ascribed to
it by the NEA report (increased civic participation, etc.), literary humanities
need not concern itself with the protection of the literary. The survival of
the field (and not of literature itself) depends on the freedom of its cadres
to pursue the literary function everywhere, indiscriminately, even in the
discarded textual detritus of human activity, however quotidian. A conception
of a more neutral, broad literary domain can include and preserve all existing
forms of textuality.

[^ln12-zoo]: There are several studies that explore the effect of perceived
aesthetics on zoology and conservation. See esp. @frynta2009 and @stokes2007.
The short of it: cute animals get more funding.

I am trying to burrow (impatiently, because the archive is thick, and the
bedrock far) from a big idea, like literature, down to its more modest textual
forms, and into the material substratum underlying all print media: paper,
digital or otherwise. For this, we'll need an operational definition
of literature, free (to the extent it is possible) from its normative baggage:
something like "the systemic application of textuality to human life," and
textuality as "the site of literary activity." But I am not yet ready to move
beyond the obvious tautology.

### 2.4 Content and Control

The personal computer governing the production of textuality today emerged from
an amalgam of automated devices, chief among them the telegraph, the typewriter
and the calculator. In his seminal 1937 paper "On Computable Numbers," Alan
Turing describes an automatic machine (a-machine) capable of transposing the
problem of calculability into "effective calculability." Where all previous
calculators were special-purpose mechanisms, engineered to augment a specific
type of computation, the a-machine was a universal device. In theory (and it
was conceived as a thought experiment, at first), it could imitate any other
mechanism for calculation because its internal state was represented as a
symbolic state. The device would receive input by means of a paper ribbon
containing discrete symbolic inputs. At its bare minimum, the device would need
only the ribbon, a means of "reading," and the means of "writing," the symbols
onto the tape. Mechanically, its movement could be restricted to one axis or to
the movement of the tape through he machine. What makes such a device a
*universal* Turing machine is its ability to internalize the symbol as part of
its mechanism. The symbolic input can symbolize computable data (letters and
numbers), but it can also function as control code ("move left," "erase mark")
altering the movement of the mechanism. In fact, the Turing machine does not
properly differentiate between content and control code. Its internal state
(Turing purposefully uses terms of sentient awareness throughout) "is
determined by m-configuration *q<sub>n</sub>* and the scanned symbol
[@turing1936, 231]." In other words, whatever initial configuration the
physical mechanism is in, its next state is defined by the its initial
configuration *and* the scanned symbol. The material and the symbolic fuse into
one. Software is also hardware. This transition of symbols into machine
configuration states effectively defines modern programming. A universal
machine, unlike other, definite, single-purpose and limited-state mechanisms
(like a clock for example), contains the ability to take on differing internal
symbolic configurations. It can imitate a clock, an abacus, a scale, and,
Turing thought that with time it would be able to imitate humans as well.

This ambiguity between hardware and software leads to some confusion in the
critical literature, as evidenced by Lev Manovich's playful response to
Kittler's "there is no software" argument. If I understand it correctly,
Kittler's short but often cited essay picks up the thread of Kittler's earlier
work to posit what he calls a "postmodern writing scene." "We do not write
anymore," writes Kittler: "human-made writing passes through microscopically
written inscriptions which, in contrast to all historical writing tools, are
able to read and write by themselves [@kittler_there_1995]." According to this
schema, Kittler sees the paper-bound design blueprints of the first integrated
microprocessor as the last "real" piece of writing. Everything after that point
is hardware. Manovich inverts Kittler's argument into "there is only software,"
by which he means that in a pragmatic sense, the affordances of a given medium
are determined by software. The printed page begins to differ from the screen
only when the readers are able to do something on the screen that they could
not on paper. To this end, Manovich encourages his readers to become active
developers of software, rather than passive consumers [@manovich_there_2011,
274]. Writing in the silicon age is possible, after all. Of course, Kittler
could object that a reader's agency is still limited to the chip architecture.
Many readers know how to code, but few have the resources to manufacture
hardware and on some level the layout of microprocessor circuitry is a type of
programming, miniaturized and mass produced at scales not accessible to the
general public. The link between hardware and software is indeed blurry at that
level.

[ln2-gurevich]: Kittler mistakingly attributes "Algorithms in the World of
Bounded Resources" to Brosl Hasslacher. The author is rather Yuri Gurevich,
Principle Researcher at Microsoft Research and then a professor at the
University of Michigan. Hasslacher's essay entitled "Beyond the Turing Machine"
appeared in the same volume of collected essays, @herken_universal_1988.

This ontological complication has its roots deep in Cartesian dualism. Where
does a brain end and a mind begin? Similarly, at some impreceptible point
software disappears into hardware. But before we ourselves get lost in that
liminal space between matter and idea, let us recover a measure of novelty
found in now familiar operation of the Turing machine. The first thing to note
is that Turing's original formulation happens at the level of a thought
experiment. The Turing machine is an idea, which takes on the structure of
other ideas expressed symbolically. The second thing to note is that although
Turing describes the Turing machine in the language of mathetmatics, giving it
its perfect, universal shape, his description also contains the bare minimum of
a mechanichal device. The critical litrature on Turing machines usually
confronts it only as a novel abstraction. But, as a mechanism, the Turing
machine borrows from a number of extant designs, that together and
incrementally bring the universal computer into material existence. Although
Turing's original article did not include any drawings, he describes the
a-machine in terms of paper tape medium, scanning, erasing, "writing down
notes," "external operators," and "moves" of the mechanism. From the
construction standpoint, Turing's machine can be reduced (1) to the mechanism
for "reading" and "writing" symbols onto and from paper storage media, and (2)
to the paper tape storage medium itself (as seen in Mike Davey's
reconstruction). In the United States, these elements would find prior art in
the mechanisms like the "Numeral adding or subtracting attachment for
type-writing machines" (US517735, filed in 1893 and issued in 1894), "Combined
Type-Writing and Computing Machine" (US990238, 1896-1897), "Computing
Attachment for Typewriters" (US1162730, 1908-1915), "Computing Mechanism"
(US1105170, 1908-1914), and "Combined Type-Writing and Adding Machine"
(US1244398, 1910-1917) among others.

![Universal Turing machine as a device. Reconstructed by Mike Davey, circa
2012.  Displayed in the Collection of Historical Scientific Instruments,
Harvard University. Go Ask A.L.I.C.E. Exhibition, 9/11-12/20/212. Photograph by
Rocky Acosta under CC Attribution 3.0 license.](images/turing.jpg)

![Universal Turing machine as an idea. "Nick Gardner's WireWorld multiplier,
via a Turing machine."](images/turing-idea.png)

By the end of the 19th century a number of lesser mechanisms anticipate the
functional elements of Turing's device. By 1930s, these inventions not only
anticipate the modern computer, but are brought to mass market in wide-spread
manufacture of computing scales, dial recorders, electric tabulating machines,
and computing typewriters made by companies like Underwood Computing Machine,
Electromatic, and International Business Machines (IBM). Rather than a singular
eureka moment, the invention of the universal machine should be viewed as a
gradual historical process that culminates with Turing's ideal specifications.
Three key stages, each in itself encompassing a long lineage of technological
development, stand out as absolutely necessary for the complete development:

1. The mechanization of type. With the invention of the movable type and the
   typewriter, the variability of hand-written script can be normalized to a
limited set of discrete characters. The process of normalization continues
today as contemporary technologies like natural language processing and optical
character recognition struggle deal with non-Western writing systems.

2. Remote communications (geographical displacement). Mechanical type can now
   be converted into electrical signals. The telegraph removes writing from its
immediate physical environment. Authorial presence, already weakened by writing
as such, is further distanced from the contexts of utterance. The telegraph
lengthens the chain of technological and political mediation in the
transmission of information. The telegram differs from the letter in that it is
transcribed, encoded, decoded, and transcribed again. The lengthening further
weakens the authorial function: where a letter must be "forged" to
mis-represent authorial intent, third parties (like the telegraph clerk) are
explicitly present in the act of telegraphic transmission. In the absence of
the identifying "hand," the telegraph clerk's (or the censor's) mark cannot be
distinguished from the author's (a common literary plot device in the fiction
of the period). The notion of the telegram's "fidelity" therefore becomes an
attribute of the communication channel as a whole.

3. Automation and programmable media (temporal displacement). Programmable
   media like perforated tape removes the human previously needed to mediate
between mechanical and electrical signal. "Content" intended for humans can now
be mixed with "control code" intended to alter the operation of the receiving
device.

Where the first two of these developments are well covered in the literature of
new media and communications studies, the third is most crucial to the
development of Turing machines, and least understood. A number of inventions at
the end of the 19th century pertain to "circuit-controlling devices controlled
by a traveling perforated strip or tape" (from US patent number 500226, filed
Charles Cutriss in 1893 [@cutriss1893]). Prior to perforated tape, the
transmission of messages by telegraph required the presence of a skilled
operator, able to transcribe messages from text to Morse code, and into the
physical motion of a lever-operated circuit. In this system, the human operator
acted as a mute interface between text and machine. The transcription of text
into signal, and back onto paper, required the real-time presence of human
encoders and a decoders. The perforated tape decoupled the human from the
machine. In US1187035 (filed 1911, issued 1916) on "Telegraphy" Albert and
Ralph Bumstead explain, "the object of our invention is to provide a system of
telegraphy which does not require skilled operators for the transmission and
reception of messages." Instead, the message was transcribed into perforation
mechanical means and then fed into the device. The tape mechanics of the type
writer could then be coupled with the electrics of the telegraph, with
perforated tape as the mediator between the two "worlds." A number of devices
emerged at the time with the aim of transforming mechanical action of the
typewriter into perforation, and, consequently, perforation into script,
completing the circuit between automated "encoding" and "decoding". As one
device converts human input into mechanical states, and into signal, another
device converts the signal into mechanical states and into a human-legible
message.

A flood of inventions appeared at the turn of the 20th century to capitalize on
such decoupling. These include machines for tape-controlled telegraphic
transmission (US158156, 1874-1874; US794242, 1905-1905; US1187035, 1911-1916),
tape-controlled printing (US985402, 1908-1911), printing telegraphs (US1721952,
1928-1929), and remote broadcast programming or radio and television content
(US1974062, 1932-1934; US2031074, 1931-1936). With the invention of punch cards
and perforated tape (also used in Jacquard looms, as early as 1801), a message
meant for another human became also a physical medium--bumps and holes--used to
mechanical movement of the transmission apparatus. For example, of the 33
asserted claims in the Bumstead brothers patent, the first 13 relate to the
"transmission of intelligence [...] adapted to initiate a succession of
electrical impulses all of which have a character representing significance, a
receiver adapted to detect variations in time intervals elapsing between
successive impulses, a plurality of interpreting relays selectively actuated by
said receiver, and a printed mechanism responsive for the combined action
[@bumstead1911, 12-13]." Up to this point, the patent describes a device for
transmission information, from type to print. Starting with clause 14, the
language changes to describe a more general "telegraph system," involving
"an impulse transmitter having means to vary the time intervals between
successive transmitted impulses, each time interval having a character
representing significance, a receiver responsive to said devices, and signal,
distributing, interpreting, and recording devices *responsive to the contiguous
significant time intervals define by the impulses* [@bumstead1911, 14 (emphasis
mine)]." For the automated telegraph the control code and the message are one:
that is, the arrangement of perforation on the ticker tape affects the internal
mechanical configuration of the machine. Another way to put this would be to
say that the state of the tape-driven telegraph machine at any given time is
also, in part, a message meant for the human recipient. The printing telegraph
of Brothers Bumstead, along with their subsequents and antecedents contain all
the necessary parts for a Turing machine: discrete symbolic language,
removable storage medium, and a device that can alter its internal state based on
the reading and writing passing symbols.

"If we disregard the small class of telegrams that merely express emotions, the
essence of telegraphy is control [...] Telegraph systems, therefore, belong not
to the class of producing or distributing, but to the class of controlling
mechanisms @murray_setting_1905, 556]."

Where we began with mechanisms of direct inscription (like pens and
typewriters) we end with machines that look like typewriters, but involve
"control circuits" capable of distinguishing "significance" and of
"interpretation." The telegraph signal that initially had one-to-one
correspondence of output to the human input is now processing signals. Content
intertwines with control. And the issue of who gets to control what comes to
the fore right away. In US1165663, filed in 1911 and issued in 1915, Hyman
Goldberg writes: "the object of the invention is to provide mechanism operable
by a control sheet which is legible to every person having sufficient education
to enable him to read." Goldberg invention involves a perforated "control
sheet in which the control characters are in the form of ordinary language."
Rather than using ticker tape, Goldberg's perforations form letters: the
language of machines and the language of humans coincide. But it is not until
much later, with the invention of higher level programming languages, that
literature for humans and literature for machines would coincide on one and the
same sheet. The early history of computing is punctuated by the ticker tape and
the punch card as the preferred control and storage media of choice.

With this history in mind, I am interested in reformulating the metaphysical
anxiety about literature and value formation in terms of text and control. If
you are reading these words on a screen, my message has reached you through a
long chain of transformations: from the mechanical action of my keyboard, to
the arrangement of electrons on magnetic storage media, to the modulation of
fiber-optic signal, to the shimmer of the flowing liquid crystal. Many, many,
third parties were involved in that act of communication. And some part of
that message was used to control the electrical circuits of the device in your
lap, in your hand, near to your eye, embedded, or embodied. Close reading
limited to the parsing of content risks missing the machinations of naked
control. It looks like you are reading a book, but this book changes its
internals as it receives instructions to inform and to control. For now I ask
only who controls the device? But if the book is a pill or fused with the
neural circuitry of the brain, do you know what you are reading?

### 2.5 WYSINAWYG (What You See Is Not Always What You Get)

By the 1960, multiple competing character encodings existed in the United
States and globally. 

ASCII.

By 1932 the ITU documents list two new standards, International Telegraphic
Alphabet No. 1 (ITA-1) and International Telegraphic Alphabet No. 2 (ITA-2).
The ITA formats, like the Baudot, represented latin characthers in a system of
fixed-lenght "bit" codes, For example, in ITA-1 the letter "A" and the number
"1" are both represented by `-++++`; "X" and "," by `+-++-`, and the "error"
code by `+++--` [@ITA1932, 34]. Because the devices are not synchronized, the
pause between each transmitted character can vary in length. For this reason
ITA-2 includes two extra bits for indicating "start" and stop" elements. The
letter "A" and a hyphen in ITA-2 would be encoded as `0110001`, where ones and
zeros represent the absence and the presence of current[@ITU1932,
36].[^ln13-current]

"You must acknowledge that this is readable without special training," reads
the schematic illustration to a Goldberg's 1911 patent for a "controller." "My
invention relates to all controllers," Goldberg writes. "The object of the
invention is to provide a mechanism operable by a control sheet which is
legible to every person having sufficient education to enable him to read. To
illustrate my invention, I produce a control sheet in which the control
characters are in the form of the letters of the ordinary English alphabet
[@godberg1911, 1:9-19]." The invention never caught on, but the patent makes it
clear that Goldberg, among others, was aware of the problem: the mechanization
of type, automation, and remote control required specialized training. With the
advent of the automated telegraph, content meant for people was now being
intermixed with machine-controlling code. To combat mutual unintelligibility,
Goldberg imagines using cards, perforated in the shape of the English alphabet.
Besides carrying (human-readable) content, the perforations do "double duty" to
mechanically manipulate the machine's "blocks," "handles," "terminal blades,"
and "plungers."

![Goldberg's Control Cards (US Patent 165663)](images/control-2.png "@goldberg1911")
Concern with human comprehension. 

Related to secrecy.  Plain language and

secret language (telegraph regulations 1932).  "Plain language is that which
presents an intelligible meaning in one or more of the languages authorized for
international telegraph correspondence, each word and each expression having
the meaning normally assigned to it in the language to which it belongs"
[@ITU1932, 12]. "Code language is composed either of artificial words, or of
real words not used in the meaning normally assigned to them in the language to
which they belong and consequently not forming intelligeble phrases" [@ITU1932,
12].  TROFF

Plain text in Unix. Human readability.
Further separation. On benefits of computer-aided document preparation. "The
style or format of the document can be decoupled from its content; the only
format control information that need be embedded is that describing textual
categories and boundaries, such as titles, section headings, paragraphs, and
the like [@kernighan1078, 2116]

Plain and fancy Text Unix system. What you see is not what you get. What is
plain text. Unix ideas of plain text. ASCII. From form and content to content,
semantic markup, and typesetting. Semantic markup as part of the
extra-linguistic meaning making.

http://www.unicode.org/reports/tr29/ The concept of Grapheme Clusters.

Semantic markup is interesting because it contains both material and ideal. The
way textuality is encoded mediates between idea and matter. Mediation. Visible
form and hidden form.

The theory of preaching. Lectures on homiletics (1892). Plain text vs. obscure
text.o

John Charles Ryle
Expository Thoughts on the Gospels: For Family and Private Use. With the ...

"One plain text is worth a thousand arguments."


Words: Their Use and Abuse
Say plain things in a plain way.

Time and truth reconciling the moral and religious world to Shakespeare: the
 By B. S. Naylor
"Thus it is, that the plain text, the obvious meaning, of Shakespeare, is
dostorted by Commentators and beclouded by Illustrators" (p64)

"One plain text is worth a thousand"

Signal Book, United States Army: 1916

We have now traveled from the pixel down to magnetic storage media and back
from the storage media to the screen. The passage opens up space between
visible content, media, and the imposed forms that govern any and all higher
notions of literary activity "floating" above this nominally "digital" layer.
Technology does not determine the literary space: it has only the potential to
hide implicit mechanisms of machine control, or, to offer possibilities for
transformation not otherwise available to other forms of textuality. Loosely
coupled to its material contexts, text can continue its relentless drive from
matter to idea and into other matters as long as its passage is not hampered by
regimes that prohibit further sharing, remixing, and transmediation. I say nothing
yet of the potential necessity of such regulation. Under certain conditions, in
the name of privacy, security, or property rights, it may become necessary to
flatten out and to treat text as more of an analog, media-bound modality of
communication, limited in its ability to move across minds and cultures. But,
it is also in our broadly human, civic interest to keep such mechanisms of
constraint visible to view, under continual scrutiny of critical, close, and
closest possible reading.


### 2.6 Algorithm as Fetish

I can only begin to work of re-inscribing the A more careful microanalysis of
the textual condition in the passage between typewriters and personal
computers, reveals plenty of space for human agency. Media determine our
situation only to the extent that we continue to treat the everyday
transformation of thought into pixel (and thereto into literary control systems
that structure human experience) as mystical and mystifying black-box
mechanism.

The real danger comes not from a metaphysical source, but from the superficial
similarity of print and pixel. Where the pen or the typewriter inscribe the
image directly into their paper medium, digital type passes through a series of
transformations between the keystroke and its corresponding pulse of the
cathode ray, or the flow of the liquid crystal. Secrecy and surveillance
technologies that rightly worry Kittler exist in the gap between text as medium
and text as content. They threaten literature (the something to be said) only
in so far as the mechanisms of literary production (how it is said) remain
hidden from the critical view. When the veil is lifted, we can begin to reclaim
the passage of textuality.  In this process, technology remains a relatively
neutral conduit, enabling liberation and oppression alike. The physical control
of expression cannot and should not be reduced to something as seemingly
self-contingent as "the conditions of high technology." Systems of control
naturally obscure the pathways of agency. Letting go of agency as a critical
concept plays into the hand of those (human agents) that benefit directly from
the illusion of disempowerment. I view "control" in that sense not as an
ideology, but as the ability to physically alter the material conditions of
literary production, dissemination, and consumption. The transition from
mechanical typewriter to electrical word processor precisely couples code as
content and code as control.

Derrida calls Marx "first thinkers of technics, or even, by far and from afar,
of the tele-technology [@derrida_specters_2012, 213].

"All these software mutations and 'new species' of software techniques are
social in a sense that they don't simply come from individual mnids or from some
'essential' proporties of a digital computer or network. They come from
software developed by groups of people and marketed to large number of users
[@manovich_there_2011, 272-273]."

"An [algorithm] is therefore a mysterious thing, simply because in it the social
character of men's [agreements] appears to them as an objective character stamped
upon the product of those [agreements]." "There it is a definite social relation
between men, that assumes, in their eyes, the fantastic form of a relation
between things." "The relations connecting the [agreements] of one individual with
that of the rest appear, not as direct social relations between individuals at
work, but as what they really are, material relations between person and social
relations between things." [@marx-english, 321] "These quantities vary
continually, independently of the will, foresight and action of the producers.
To them, their own social action takes the form of the action of objects, which
rule the producers instead of being ruled by them." To strip the "mystical
veil," algorithms as contracts between "freely associated men [@marx-english,
327]." "this automatism outside the head" .Necromancy.

Hijack the whole conversation from the

Derrida "If one follows the letter of the text, the critique of the ghost or of
spirits would thus be the critique of a subjective representation and an
abstraction, of what happens in the head, of what comes only out of the head,
that is, of what stays there, in the head, even as it has come out of there,
out of the head, and survives outside the head [@derrida_specters_2012, 215]." 

In religion people make their empirical world into an entity that is only
conceived, imagined, that confronts them as something foreign
[@marx_collected_1976, 159].

imagined commodity =
algorithm labor = contracts, agreements

