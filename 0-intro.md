# Simulated Text: an Introduction

As I write these words, a ceiling-mounted smoke detector in my kitchen emits a
loud noise every three minutes or so. And at every 15 minutes, a pleasant
female voice announces also "low battery." These precautions are stipulated by
US National Fire Alarm Code 72-108 11.6.6 (2013). The clause requiring a
"distinct audible signal before the battery is incapable of operating" is not
only required by law, it is *encoded into the device*. The device internalizes
legislation in its programming. Thus we obtain the condition where the two
meanings of code---as governance and machine instruction---coincide. Code
equals code.

I am not at home. But I know the alarm is happening because I receive
notifications of it on my phone---another small computational device, next to
my laptop. This mobile appliance also contains most of my book library. I pick
it up to read a book. But what I mean by "reading a book" obscures a metaphor
for series of odd actions. The "book" is a small, thin black rectangle: three
inches wide, five inches tall, and barely a few millimeters thick. A slab of
polished glass covers the front of the device, where the tiny eyes of a camera
and a light sensor also protrude. At the back, made of smooth soft plastic, we
find another, larger camera. At the foot of the device a grid of small
perforations indicate breathing room for a speaker and several microphones. To
"open" a book I touch the glass. The machine recognizes my fingerprint almost
instantly. I then tap and poke at the surface until I find a small image that
represents both my library and the book store, where I can "purchase books."
To "buy a book," I agree to a limited licence, granting me access to data
which the software then assembles into on-screen representation of books. I
tap again to begin reading. The screen dims to match room ambiance as words
fill the screen. One of the passages on the first page appears underlined: a
number of other readers in my social circle must have found the passage
notable. My finger slides along the glass surface to turn the page. The device
emits a muffled rustle, as if to remind me of the underlying analogy. The
image curls ever so slightly in a way that resembles a printed page as another
"page" slides into view. My tiny library metaphor holds uncounted thousands of
such page metaphors.

Despite the appearances, the electronic metaphor-making device next to my
computer has more in common with the smoke detector than it does with several
paper volumes also scattered across my desk. Both devices, the electronic book
and the smoke alarm, comprise printed circuit boards, capacitors, silicone
chips, and resistors. Both draw electric current. Both require firmware
updates and are governed by codes: political and computational. The smoke
alarm and the mobile phone connect to the network. They communicate with
remote data centers and with each other. And yet, I continue to read
electronic books as if they were familiar, immutable, and passive objects:
just books. I think of them as intimate artifacts---friends even---wholly
known to me, comforting, and warm. The electronic book is none of those
things. Besides prose, it keeps my memories, pictures, words, sounds, and
thoughts. It records my reading, sleeping, and consumption habits. It tries to
sell me things, showing me advertisements for cars, jewelry, and pills. It
comes with a manual and terms of service. It is my confidant, my dealer, my
spy.

## 0.1 Displacement

The aim of *Plain Text* is to dispel a pervasive illusion. The electronic book
is but a compelling metaphor that belies material realities of reading and
writing, transformed by the advent of computation. The device on my desk is
not a book, but a simulation of a book. And everything associated with
"reading" this metaphor must in itself be understood under the sign of
simulation. What does it mean to "study" a book metaphor? And what kind of a
metaphor is it? What is being compared to what? How did it come into being and
how does it affect practices of literary interpretation? In *Plain Text* I
attempt to come to terms with the conditions of simulated textuality.

To come to terms *Plain Text* enacts a reconciliation of vocabularies. It is
a response to a particular situation of a literary scholar encountering the
field of software engineering. For a long stretch of my professional life,
these two areas of activity remained separate. I worked at one and I studied
the other. At the time, I simply did not think that code had anything to do
with poetry. The idea for the book came to me in a moment of realization after
I was asked one of those naive but fundamental questions of the kind that can
set research in motion down a long and winding path. A childhood friend who
loves books asked about the difference between text in print and text on the
screen. It was in that struggle to articulate difference that I realized that
some of my deepest assumptions about literature relied on the centuries-long
stability of print media. Despite my professional experience as a programmer,
I could not readily explain the mechanisms by which keystrokes turned into
pixels, pixels into letters, and letters into words. I could recount technical
detail on some level, but my knowledge was also riddled with unexamined gaps.
It did not amount to a coherent story. I was, despite my best efforts,
surrounded by magical lanterns that cast shadows of code and poetry.

Initially, at the point of contact, the two selves---the scholar and the
engineer---spoke different languages. It was and continues to be a
disconcerting process by which things dear and familiar to me, in both worlds,
grew strange and unfamiliar, showing themselves to be sometimes less than and
sometimes more than I comfortably expected. Nothing could be assumed from the
start. Field specific jargon, down to naive foundations, had to be examined
for hidden assumptions that prevented dialog. With time, I saw that code and
poetry have much to do with one another. Writing this book has taught me to
embrace the remaining incongruence.

The Czech-born philosopher Vilém Flusser understood the condition of unease
that comes with emigration, both physical and mental, to be a type of
information processing. I relate to his work as a former refugee fleeing the
dissolution of the Soviet Union, a transplant into Silicon Valley culture from
a strict literary education, and now a lapsed engineer among humanists. These
vantage points offer a singular view onto the material conditions of
contemporary intellectual life.

Technology offers us an irresistible compromise, by which we trade critical
understanding for comfort. Habit covers the various homes we make for
ourselves in the world "like a fluffy cotton wool blanket," Flusser
wrote---"it smoothes the sharp edges of all phenomena that it covers so that I
no longer bump against them, but I am able to make use of them blindly." "When
I sit at my desk," he went on to write, "I don't see the papers and the books
that are lying all about because I'm used to them" [@flusser_freedom_2003, 13
and 82]. Familiar objects pass no information to their users, according to
Flusser. Like water that surrounds fish, they pass into the background of
experience. Mediums become media. Their function is to ultimately disappear
into the background: not to produce meaning but to cease production, to become
a stage for meaning-making, and like the stage to disappear from view.

The condition of exile, by contrast, allows the displaced to once again
transform habituated medium into meaningful information. In exile, "everything
is unusual," Flusser writes [@flusser_does_2011, 81]. The migrant experiences
the world as an ex-perience [*er-fahrung*], or literally a driving out.
Discovery, he concludes "begins as soon as the blanket is pulled away," where
the familiar objects can pass into view again [@flusser_does_2011, 86-7].

Our challenge today is to uproot ourselves from the comfort that rapidly
descends on the dwellings of our intellectual life. Dulling the senses,
seemingly inconspicuous conduits of information---electronic books and
desks---acquire a sense of agency of their own. Devices that "watch," "hear,"
"see," and "think" give rise to object-oriented phenomenology and the internet
of things. A new generation of artificially intelligent objects, things like
smart phones, smart light bulbs, and smart watches, Intels and Idea Pads,
enter the networked public sphere as seemingly independent agents.[^ln-winner]
Marx's "table that evolves grotesque ideas out of its wooden brain" can now be
re-branded into Microsoft *Surface* and PixelSense, product names from the
life cycle of an actual smart table [@marx_captial_1906, 82;
@wigdor_designing_2009].

I will argue here that if we hope to understand literature "under conditions
of high technology" (to echo Friedrich Kittler) we can only do so from the
position of humanism. One cannot at the same time lament the systematic
erasure of the human from the literary process and advocate for post- or
anti-humanism. Unlike Kittler, who once wrote that under conditions of high
technology "literature has nothing more to say," I believe that literature and
literary analysis continue to have a voice in contemporary life
[@kittler_gramophone_1999, 263]. Technology does not determine silence.
Rather, the grounds for all reflective textual activity slowly erode to return
to their natural and meaningless state of entropy. The literate return to
illiteracy. The instruments of literary analysis must consequently evolve to
continue producing meaning under new technological contingencies. Where texts
are encrypted, we must decrypt. If as Kittler writes, automated discourse
analysis threatens to take command, we will engineer automatons that command
on our behalf [@kittler_gramophone_1999, 263]. Such acts of resistance, small
and large, can recover a measure of agency from the ruling determinism
of---take your pick: markets, complex systems, unconscious drives, monistic
universes, *gaia* science, social networks, technology, the singularity,
bureaucracy, and war. Indeed, the possibility of human
erasure---nonsense---never strays far from reach, "like the face drawn in the
sand at the edge of the sea" [@foucault_order_1994, 387]. The fragility of
life compels not the Foucauldian wager on anti-humanism, but the need to
mobilize whatever modest means available to humans to persevere against great
odds.

By mistaking things for animate actors, we ourselves have become enmeshed in a
system of digital production that commodifies human experience. Objects that
surround us collect our reading habits, social interactions, and intimate
conversations.[^ln-exposed] But the actual living agents that benefit from the
trade in personal information are neither cyborgs nor post-human assemblages.
The bargain that trades critical understanding for comfort benefits specific
interests, like multinational corporations and government intelligence
agencies. The rhetoric around smart objects shifts our attention from the
seats of power to things dumb, powerless, and indifferent to our
protestations.

The internal exile that we must undergo for the smart book and the smart desk
to come into view, to become foreground, cannot compare in difficulty to the
experience of physical displacement that follows natural disaster, war,
poverty, or political instability. Yet, our systematic reluctance to take on
even those small intellectual discomforts that could lead to acts of localized
dissent and disobedience---to write using free software, to build open
archives, to share memories in private---cannot be said to exist apart from
the complex systems that perpetuate inequity and violence globally. The
emotional affirmation that accompanies exuberant social networking brings with
it the governing structures invoked widely in the name of law enforcement and
national security. Comfort and security constitute part and parcel of the same
ill-conceived bargain that leads to critical disempowerment. But where it is
difficult to imagine or to enact strategies of digital disobedience on a
universal scale, we can begin to address them through the numerous and
seemingly mundane series of micro transactions that ultimately embody the
material foundations of local intellectual life. A new humanism affirms small
differences in search for agency that can lead to action.

To pick up an electronic book and to take it apart may be against the law in
some jurisdictions [@fry_circumventing_2009]. However, given the extent to
which emergent thought-things like electronic books and personal communication
devices participate actively in the production of meaning, we can no longer
employ strategies of interpretation at the level of ideology alone. Close
reading, critical theory, and literary analysis must reach down to the silicon
bedrock that stages the very act of interpretation. Literary theory, a
discipline fundamentally engaged in the exegesis of all figurative tropes, is
crucial to the understanding new computational environments, which envelop
intellectual life through metaphoric substitution. I begin where Bernard
Harcourt's recent book on digital disobedience ends: with the possibility of
localized dissent, limited for now to the immediate physical contexts of
reading, writing, and finding knowledge. Following itinerant theorists of the
metaphor like Vilém Flusser, Viktor Shklovsky, and Hannah Arendt I propose to
proceed through systematic estrangement of computational metaphors, metaphors
that fade into transparency as to escape the critical gaze.

## 0.2 Metaphor

The language of computation employs a number of vivid metaphors. Like all
tropes, these lose their evocative power with frequent use. The task of a
literary scholar becomes then to explicate and to renew the metaphor, in
search for parallelisms that, as George Lakoff and Mark Johnson explain,
configure one conceptual system in terms of another [@lakoff_metaphors_1980,
3-14]. Why do we call some software programs "applications" for example? The
application of what to what? Apple's iconic *Human Interface Guidelines*, a
manual of style that heralded the era of "what you see is what you get"
interfaces, contains explicit echoes of Lakoff's thought. The manual urges the
designer to "convey meaning through representation" and to seek the metaphor
appropriate to the task [@apple_apple_1987, 11]. Do not ask the user to throw
"documents" into "jars," for example, the manual entreats: "dragging the
document icon to the Trash means the user wants to discard that document"
[@apple_apple_1987, 229] Simulated objects must "look like they do in real
world" [@apple_apple_1987]. In the words of cognitive scientists John Carroll
and John Thomas, whose work the Apple design guidelines also reference
extensively, "people employ metaphors in learning about computer systems."
Using the appropriate metaphors therefore provides "wide-ranging improvements
in learning ability and ease of use" [@carroll_metaphor_1982, 108]. What users
know about trashcans in the real world can be used to guide behavior in the
virtual one.

The seminal work on figurative speech undertaken by the Russian formalists at
the turn of the twentieth century reminds us that such metaphoric shortcuts to
learning also have their cognitive downside. Habituated experience passes into
the unconscious [@brik_poetika_1919, 104]. The thing "dries up" in Shklovsky's
words, first in perception and then in practice [@brik_poetika_1919, 38 and
104]. As a benefit, the metaphor acts to conserve mental energy. Once
internalized, it no longer attracts attention. The image becomes so familiar
that we cease thinking about it---an insight that was confirmed experimentally
almost a century later.[^ln-cog] Conceptual blending, a dynamic by which
images and paradigms from one domain are extended to another, thus improves
the learning of software systems. Flusser would say it makes experience more
"smooth." The user avoids "bumping into the sharp corners" of new and
complicated computational environments.

But the formalists also understood that metaphors so "automatized" diminish
the vitality of experience. Shklovsky quotes from the diaries of Lev Tolstoy,
who, while dusting his room, could not remember if he already dusted his sofa.
"Because actions like these are habituated and unconscious, I could not
remember and knew that it was impossible to remember," wrote Tolstoy. "And so
whether I dusted and forgot or just did so without thinking, it as if the
action never happened [..] thus when life passes without conscious reflection,
it passes as if one has not lived at all." Life disappears into nothingness,
when the "automatization of experience "consumes things, clothing, furniture,
your spouse, and the fear of war" [@brik_poetika_1919, 104].[^ln-translation]
Carroll's recommendations for design guidelines assumed new users, unfamiliar
with computing. But after more than three decades of computational interfaces,
we have not moved far from the interfaces the metaphor approach has
engendered. Like Tolstoy's mindless dusting, life at the keyboard passes
without reflection.

For Shklovsky and his cohort, estrangement could "resurrect" the stale image
and provoke the "experience of the making of the
thing"[@shklovsky_voskreshenie_1914; @shklovksy_sborniki_1917, 7]. They
applied it in art and analysis alike. For example, in his influential essay on
"How Gogol's *Overcoat* Was Made" Boris Eikhenbaum analyzes the humor of Gogol
with clinical precision [@brik_poetika_1919, 151-67]. What readers lose in
having the joke explained to them, they gain in understanding of the genre. I
take a similar approach in *Plain Text* by extending formal concerns with the
mechanics of the literary "device" and "technique" to literary devices and
technics proper.

I too am interested in how the literary thing is made.  Because computers, as
I will argue throughout, operate through symbolic and metaphorical
substitution, we can use the methodology of formal symbolic analysis to our
advantage. The overarching aim of the book is therefore to expose the illusion
of verisimilitude between text on paper and the simulated text on a screen.
The words may look the same, but the underlying affordances of the medium
differ in significant detail. As an example, consider a news report that
alters its content based on the reader's location. Imagine an e-book reader
that highlights popular passages of a novel in real time, shortening the less
popular passages down to their algorithmically distilled summaries.  For a
literary analyst, the instability of the digital medium means analysis cannot
be confined to reading for surface meaning alone. How can close reading
persist when reading devices reconfigure the text to fit individual tastes,
mood, or politics? How would we even agree on the fact that we are reading the
same text? The very possibility of interpretation comes into question.

The challenge of asking such questions lies in the relative paucity of our
critical vocabulary. We have many theoretical models to address literary
phenomena on the macro scale: period, genre, style, affect, world literature,
etc. Yet the particulates of literary production and dissemination on the
micro scale go for the most part unnamed. To come to terms with the book as a
device; to begin to understand the nature of the text simulation; to perceive
the particularity of the computed sign: these are the aims of *Plain Text*. A
strategy of deliberate defamiliarization reclaims the metaphor-device for
analysis.

Yet despite its power to recall the world anew, estrangement cannot be
practiced effectively in the mode of a monologue. To produce meaning, Flusser
reminds us, it must become dialogical practice. Perpetual exile is otherwise
uninhabitable [@flusser_freedom_2003, 81]. Estrangement thrusts the displaced
into the chaos of unsettled existence. With time, the displaced make a new
home, from which they can once again "receive noise as information" to produce
meaning. "I am embedded in the familiar," Flusser wrote, "so that I can reach
out toward the unfamiliar and create things yet unknown"
[@flusser_freedom_2003, 12]. The expellee and the settled inhabitant need each
other. Through what Flusser calls "creative dialogue," the dialectics of exile
can lead to "informed renewal" of shared space [@flusser_freedom_2003, 84].
Without the protection of one's home, everything turns to noise. There can no
information without a dwelling, Flusser wrote, "and without information, in a
chaotic world, one can neither feel nor think nor act" [@flusser_does_2011,
12]. By this dynamic displacement and habituation enter into a continual
dialog.

In *Plain Text*, I model the reciprocal movement to making strange on the
diverse practices of reverse engineering.[^ln-krsh] Unlike estrangement, the
reverse engineering of devices aims to bring the obscure to light. The
reciprocal motion to defamiliarization passes through a series of case
studies. The function of a case study in an engineer's education, as Henry
Petroski explains in his *Invention by Design*, is to understand the ways by
which one gets "from thought to thing" [@petroski_invention_1996, 3-7]. Along
with a metaphor, each of my chapters also contains at least one literary
"thought thing." Each also enacts a deconstruction---a literal taking
apart---of that device.

## 0.3 Scale

In the process of taking things and texts apart, I am under no illusion about
the possibility of finding perfect understanding. Technological systems that
give rise to complex social phenomena, from market trading to literary canon
formation, defy holistic comprehension. Writing in 1977, Langdon Winner warned
the scholar of science and technology to stay away from those "mystics" and
"crackpots" who offer vision of a new and totalizing synthesis. Quoting the
French poet and philosopher Paul Valéry, he wrote that "our means of
investigation and action have far outstripped our means of representation and
understanding" [@valery_collected_1962, 69; @winner_autonomous_1978, 290].
"Citizens of the modern age in this respect are less fortunate than children,"
he wrote. Children graduate from ignorance to understanding. But the adults
"never escape a fundamental bewilderment in the face of the complex world that
their senses report." They are not able to organize the vast body of
information that governs their lives into a "sensible whole." The best we can
do, he concludes, "is to master a few things in the immediate environment"
[@winner_autonomous_1978, 286].

Langdon Winner could scarcely anticipate the exponential growth in complexity
of computing systems that surround us today. The Unix operating system
announced on the pages of the Proceedings of the fourth ACM symposium on
Operating systems in 1972 comprised roughly 10,000 lines of code
[@ritchie_unix_1973; @ritchie_unix_1974; @mccandless_million_2015]. To give
the reader an idea of scale, consider the aggregate length of Shakespeare's
*Hamlet*, *Richard III*, and *Macbeth* put together. By contrast, *Google*
internet services comprise more than 2 billion lines of code, or roughly 700
million *Hamlets*.

Similarly, the growth of the literary sphere has mirrored the rapid inflation
of the code base. On an average day in 2008, at home, an average American read
around 100,500 words a day. At 250 words per page, that is around 402 printed
pages. Between the years of 1980 and 2008, the consumption of information in
bytes---a measure that would obviously privilege storage-heavy content like
sound and video---grew at a modest 5.4 percent per year. Reading, in decline
until the advent of the internet, has tripled in the same period. Reading in
print accounted for 26 percent of verbal information consumed in 1960. That
number fell to 9 percent in 2008, but the consumption of words digitally
increased to 27 percent of total consumption, which means that reading has
increased its share of the overall household attention span [@bohn_how_2009;
@hilbert_worlds_2011; @hilbert_info_2012]. The first decade of the
twenty-first century saw a 20 percent increase in library visitation
[@u.s._institute_of_museum_and_library_services_public_2010]. By a
conservative estimate, the number of scientific publications grows at about
4.7 percent per year, which means that the amount of published research
roughly doubles every 15 years or so (and the numbers are much, much higher in
some fields) [@archambault_welcome_2005; @crespi_empirical_2008;
@larsen_rate_2010]. The number of books published in the United States almost
tripled from 2005 to 2009 and 37,450 poetry and drama titles were published in the
United States between 1993 and 2006 [@dworkin_seja_2008; @bowker_u.s._2009].

Pointing to the sheer impossibility of processing the amount of published
information in nineteenth let alone the twenty-first century, the American
poet and critic Graig Dworkin urged his peers to abandon the dream of
comprehensive knowledge or the literary sphere altogether
[@dworkin_seja_2008]. Almost nothing could be said about it in aggregate, at
least not by conventional means. Models of analysis implicit in ideal of the
close reading assumes an environment of received literary canons, naturally
accessible to the human intellect. For the duration of the "Gutenberg galaxy,"
the age of print, a well-educated person might have been expected to
internalize some several hundred or perhaps thousands of major texts
constituting the canon. Close reading was honed to operate on that naturalized
scale.

The expansion of the textual field has radically increased the cognitive
demands of literary engagement. The pipeline between text and work has
lengthened considerably. On the one side, the matter of canon formation can no
longer be relegated to stable, long-term systems of social filtering. Seen
from the perspective of a literary interface, the database, the social stream,
and the search engine are tools for dynamic, "on the fly," generative
canon-formation. Consider the task of finding an unknown (to me) factoid
online, about philosophy in the times of Andalusian Spain, for example. Where
in the past I might have started with a subject catalog compiled by
librarians, today I construct a search query, using resources that I believe
will return a reasonably authoritative list of texts on the subject. The
search engine, in effect, replaces (or rather complements) centuries-long
processes of canon-formation. A near-instantaneous list of results now becomes
my ephemeral, but nevertheless authoritative, collection of relevant
literature.

Each text in the returned list still requires the instrumentation of close,
analytical interpretation. However, the same discipline of critical and
reflective deliberation exercised on the level of an individual text needs to
also be exercised on the level of procedurally generated search engine
results: Where to search? Using what engine? How to construct the query? What
are the implicit biases of the system?  The academic question of
canon-formation transforms into a (not yet critical) practice of rapid,
iterative, generative canon-making. Whatever ideals motivate close reading
between "text" and "work" surely must drive the process on the level of
dynamic corpus composition.

The various practices of distant reading arise from the condition in which
canons are no longer accessible, in their entirety, to the unaided (natural)
human intellect. These include distant reading and macroanalysis in literary
studies [@jockers_macroanalysis_2013; @moretti_distant_2013], culturomics in
economy [@aiden_uncharted:_2014], e-discovery in law
[@scheindlin_electronic_2004; @scheindlin_electronic_2009], automatic essay
evaluation in education [@shermis_handbook_2013], and medical informatics in
medicine [@shortliffe_biomedical_2013], among others. At the foundations of
these nascent disciplines is a shared toolkit of statistical natural language
processing [@manning_foundations_1999; @jurafsky_speech_2008], automatic
summarization [@radev_centroid-based_2004; @nenkova_pyramid_2007], machine
learning [@rasmussen_gaussian_2006; @flach_machine_2012], network analysis
[@opsahl_node_2010; @szell_measuring_2010; @takhteyev_geography_2012], and
topic modeling [@wallach_topic_2006; @blei_probabilistic_2012].

Where distant reading perceives patterns across large-scale corpora,
microanalysis breaks literary systems down to their minute constituent
components.

![Micro, macro, and close reading.](images/micro.png)

Where distant reading and macroanalysis constitute the study of mediation
between readers and text aggregates (like canons, corpora, collections,
libraries, archives, and database) microanalysis examines mediation at the
level of physical minutiae otherwise not readily observed in cursory
exploration. The instruments of microanalysis may coincide with computational
tools, designed to find hidden patterns lurking above or beneath a given
document.[^ln2-iarkho] The micro-instrumentation might also include a
screwdriver, a binding needle, or a soldering iron: sharp tools that help to
pry open and to scrutinize otherwise magical textual black boxes.
Local. My methodology thus lies in the dialectical tension
between making known and making strange. Each chapter unpacks metaphors like
"closing windows," "bookmarking a page, " and "dragging and dropping files" to
reveal the internals of a device. The metaphor and the machine help organize
the book and each of its chapters. Each chapter unpacks a metaphor to its
logical conclusion.

The digitally displaced hold on to the discomfort of the encounter with the
machine. Estrangement---always at the heart of immigrant or queer
poetics---reconciles without seeking wholeness or integration. I dedicate this
book then to queers and immigrants, literal and figurative---spatial,
literary, technological---to those being displaced unwillingly, to those
exiled within and without, to those who understand the need for
self-displacement, to those who transgress purposefully, and to those willing
to trespass.

## 0.5 Computational hermeneutics

The illusion of ephemerality that follows simulation comes at a price of
legibility. "Software's ghostly presence produces and defies apprehension,"
Wendy Chun writes in her *Programmed Visions* [@chun_programmed_2011, 3]. But
what happens when all text is a type of software? Friedrich Kittler ends his
book on a similar note. In his vision, literature has finally been defeated by
military-grade encryption, secrecy, and obfuscation [@kittler_gramophone_1999,
263]. Unlike censorship or the burning of books, such modes of governance
*over* the sign are less obvious and more pervasive. The simulation-producing
nature of computed text helps preserve the outward appearance of printed text,
while concealing the specifics of control. I mean control in the most direct
way possible, as a mode of physical regulation and barrier to access. The
difficulty of *Plain Text* will be in the description of such emerging but
often occluded technological contingencies.

A concern with the contemporary conditions of simulated textuality leads us to
a rich archive from the history of literary theory, semiotics, telegraphy, and
electrical engineering from the middle of the nineteenth to the middle of the
twentieth centuries. Drawing on a range of archival materials at the
intersection of literary thought and the history of modern computing, *Plain
Text* examines a number of key literary-theoretical constructs, recasting the
paper book as a metaphor machine and computational object. Reflecting on the
development of Morse Code in 1949 in the *Proceedings of the American
Philosophical Society*, Frank Halstead mentions the difficulty of finding a
home field in the arts or sciences for what he calls "code development." "It
is a matter somewhat related to the general art of cryptology," he writes,
"yet it is not wholly divorced from electrical engineering nor from general
philology" [@halstead_genesis_1949, 456]. As Halstead anticipated, research
for *Plain Text*, has led me to range of rare primary archival materials: from
the proceedings of the Association for Computing Machinery (ACM) to the United
States Patent and Trademark Office; from Bell Labs to early Soviet publishing
houses that heralded the advance of formalism; from studies on animal
communication behavior, to Unix manuals, to textbooks on semiotics, and to
foundational texts in the philosophy of aesthetics and literary theory.

Using archival material across disciplines, I will argue that extant theories
of interpretation construct a notion of close reading based on preexisting
properties and assumptions attached to static print media. By contrast,
electronic text changes dynamically to suit its reader, cultural context, and
geography. Consequently, I argue for the development of what I term
*computational hermeneutics* capable of reaching past the surface content to
reveal also the software platforms and the hardware infrastructures that
contribute to the production of meaning.

I have selected "plain text" as the title of this book to signal an affinity
with a particular mode of interpretation. In technical terms, *plain text*
identifies a file format and a frame of mind. As a file format, it contains
nothing but a "pure sequence of character codes." Plain text stands in
opposition to "fancy text," "text representation consisting of plain text plus
added information."[^ln-uni] In the tradition of American textual criticism,
"plain text" identifies an editorial method of text transcription which is
both "faithful to the source" and is "easier to read than the original
document" [@cook_time-bounded_1972]. Combining these two traditions, I mean
ultimately to argue for a kind of a systematic minimalism when it comes to our
use of computers---a minimalism that privileges legibility and human
comprehension. I do so in contrast with modes of human--computer interaction
that may privilege other system--centric ideals like speed, security, or
efficiency.

The use of plain text implies also an ethics of reading and writing. It
therefore further identifies a frame of mind or an interpretive stance one can
assume in relation to the making and the unmaking of literary artifacts.
Besides the visible content, all contemporary documents carry with them a
layer of hidden information. Originally used for typesetting, the formatting
layer can affect much more than innocuous document attributes like "font size"
or "line spacing." Increasingly, devices that mediate literary activity encode
forms of governance. Such devices tacitly censor, police intellectual property
laws, and carry out surveillance operations. For example, the Digital
Millennium Copyright act, passed in the United States in 1996, goes beyond
legislature to require the "management" of digital rights (DRM) at the level
of hardware. An electronic book governed by DRM may prevent the reader from
copying or sharing stored content, even for the purposes of academic
study.[^ln-dmca] Computational hermeneutics strives to make such control
structures available for critical reflection. Building on the recent work of
scholars like Wendy Kyong Chun, Tung-Hui Hu, Matthew Kirschenbaum, and Lisa
Gitelman I make the case for an empowered poetics, able regain a measure of
control over the material contexts of our knowledge production
[@chun_enduring_2008; @kirschenbaum_mechanisms_2008; @manovich_software_2013;
@gitelman_paper_2014; @hu_prehistory_2015].

The future of reading and writing has been inexorably intertwined with the
development of computer science and software engineering. Even if you are not
reading these words on or through a screen, my message has reached you through
a long chain of machine-mediated transformations: from the mechanical action
of the keyboard (on which I am now typing), to the arrangement of electrons on
magnetic storage media, to the modulation of fiber-optic signal, to the
shimmer of the flowing liquid crystal display rendering the text. Computation
occupies the space between the keyboard and the screen, which in turn
habituates social constructs, from the design of social media to the formation
of massive shared archives. Such "cultural techniques" are formative of our
society as a whole [@leroi-gourhan_gesture_1993, 83-84;
@siegert_cultural_2015]. Therefore, daily choices like choosing a text editor,
a filing system, or a social networking platform cannot be addressed in
shallow instrumental terms limited to efficacy, speed, or performance.
Complex computational systems cannot give rise to ethics any more than
financial markets can. Among the many available visions of human--computer
interaction, we must chose one that confirms to a humanist ethos, whatever the
reader's politics.

<!--- explain affordances --->

*Plain Text* makes a historical case for the recovery of textual thought
latent in the machinery of contemporary computing. Just as philology cannot
survive without an understanding of its computational present, the design of
software and hardware systems that facilitate knowledge production cannot
advance without deep reflection about the cultural consequences of
intellectual infrastructure. In his now seminal essay on technological
determinism, Langdon Winner has argued that 

Much is at stake in the material affordances of the literary artifact. The
political struggle for meaning-making---the very opportunity to engage in the
act of interpretation---begins with texts as material artifacts. In the West,
it is easy to forget the blunt effectiveness of physical control. Books that
are burned or redacted cannot be read at all. Elsewhere, global inequities of
access to knowledge compel readers to print their own books and build their
own libraries. Witness the so-called "shadow libraries" of Eastern Europe, the
street book vendors of India and Pakistan, and the gray market presses of
Nigeria arising form the country's "book famine."[@mahmood_copyright_2005;
@okiy_photocopying_2005; @liang_piracy_2009; @tenen_book_2014;
@bodo_short_2014; @nkiko_book_2014]. More than mere piracy, such
*samizdat*-like practices engage in the proactive preservation of the literary
sphere. Informal book exchange networks create reading publics that own the
means of textual production and dissemination. Under duress, readers build
homemade knowledge infrastructures: they duplicate, distribute, catalog, and
archive. In late-capitalist economies such infrastructures are commodified.
Consequently, they disappear from view. For many readers, technologies that
support reading, writing, and interpretation have passed from tools to fetish.
We have tender feelings for them and cradle them in our laps. No longer
comprehensible by the way of the pen or the printing press we imbue them with
magical powers and thus exist in the state of profound alienation from the
conditions closet to our mental activity.

The poetics of human--computer interaction reveal that not all texts are
created equal. In print, traditional distinctions between form and content lie
flat. The printing press firmly embeds ink into paper, leaving no space
between type and page. Media-minded critics like Johanna Drucker, Katherine
Hayles, Matthew Kirschenbaum, and Jerome McGann have urged literary scholars
to re-evaluate textuality in its media-specific contexts
[@drucker_digital_2001; @mcgann_radiant_2001; @hayles_print_2004]. Their work
reminds us that the flatness of digital text endures only in the guise of an
illusion. Low-level, operational intuitions governing textuality---ideas about
form, content, style, letter, and word---change profoundly as text shifts its
confines from paper to pixel. A substantial gap separates the visible text
from the source code that produces it. Forces of capital and control exploit
that gap, obscuring the workings of the device.[^ln-capital] 

The shifting affordances of digital text challenge some of our deep-seated
intuitions about literature. The word processor, operating system, and the
electronic book are some of the sites that stage the encounter between
literary theory and practice today. In *Plain Text* I introduce a method of
computational hermeneutics, which is a form of textual analysis strongly
influenced by materialist critique. Where "distant reading" and cultural
analytics perceive patterns across large-scale copora, computational
hermeneutics breaks textuality down into its minute constituent components. It
is at this scale that I find readers and writers becoming fundamentally
alienated from the immediate material contexts of knowledge production. Mine
is not however a post-human materialism of the kind that privileges an
object's point of view. On the contrary, the book aims to remove the aura of
fetishism that attaches itself to literary--computational artifacts and to
complex systems that mediate the textual encounter.

Extant models of literary transmission assume movement through passive and
immutable media. Paper offers the document of record, which, once archived,
does not change its contents. By using analytic techniques like genetic
criticism and forensic reading, it is therefore possible to reconstruct if not
"authorial intent," then at least a trace of the author's hand. In some
contexts, we may even ascribe properties like "fidelity" to "original" works
of art. When media are immutable, we imagine a sort of a causal custody chain
between works and their creators, which at some point of creation occupy the
same contiguous space and time. Mechanical reproduction of print has
introduced a range of devices that mediate between the author and the reader.
Distance, time, and mediation subsequently weaken facile notions of authorial
fidelity or intent. At the very least, we know that editorial practices,
publishing markets, and communication technologies introduce an unintended
element of noise into the channel. But we have yet to understand the impact of
the computed, dynamic inscription. Unlike pen and paper which come in direct
contact with each other, the contact between keyboard and screen is
fundamentally a simulated, programmed experience. At its worst, the connection
suffers from intractable "man-in-the-middle" attacks, by which a third party
maliciously alters the content of communication [@needham_using_1978].

Changing material conditions of textual transmission push against familiar
literary critical ideas. For example, as the mechanical reproduction of print
weakens the material basis for authorship attribution, the notion of
authorship itself undergoes change. That is not say that the author is dead,
as Barthes would have it. Authors continue to live and to collect royalties
from the sale of their works. The weakening of the authorship function merely
makes certain ways of talking about things like "authorial intent" and
"fidelity to the original" difficult to sustain. Massively collaborative
writing projects like Wikipedia and procedural narrative generation (machine
writing) further erode ideas of authorial production based on individual human
agency. Yet, it would be a mistake to believe that the myth of autopoiesis
(the literature that "writes itself," writing that writes, discourse that
speaks, etc.)[^ln2-varela] can displace the myth of the author. A discipline
of close attention to the atomic particulars of encoding, transmission,
storage, and the decoding of text at the site if its application to the human
condition ultimately aims to reclaim subjective agency, in motion. This may
seem strange at first: to recover the subject in the physical minutiae of the
literary--technological encounter. Yet the point of contact is crucial, for it
is here that the subject seems to disappear, in a compressed moment of time
that needs to be unpacked with some precision.

In *Plain Text*, I will argue that some of the higher--level political
afflictions of the contemporary public sphere---mass surveillance and online
censorship, for example---relate to our failure as readers and writers to come
to terms with the changing conditions of digital textuality. A society that
cares about the long-term preservation of complex discursive formations like
free speech, privacy, or online deliberation, would do well to take heed of the
textual building blocks at their foundation. The structure of discursive
formations---documents and narratives---has long been at the center of both
computer science and literary theory. Using primary sources from both
disciplines, *Plain Text* uncovers the shared history of literary machines,
bringing computation closer to its humanistic roots.

<!-- first sentence is weak, already used this structure -->

My challenge in this book lies in the relative paucity of our critical
vocabulary. We have many theoretical models to address literary phenomena on
the macro scale: period, genre, style, affect, world literature, etc. Yet the
particulates of literary production and dissemination on the micro scale go
for the most part unnamed. To come to terms with the book as a device; to
begin to understand the nature of the text simulation; to perceive the
particularity of the computed sign: these are the aims of *Plain Text*.

In drawing the history of character encoding, I want to describe gains and
losses accrued in the process of translation between machine and human
languages. And among a multitude of compromises, *legibility* stands as the
one that entails all others. I am going to repeat a version of the same thesis
for the duration of the manuscript: without legibility there can be no
understanding. Most attempts to articulate a humanist ethos, whether built on
consensus, narrative, discourse, or contract law, presuppose legibility.
Without legibility there can be no consensus, narrative, discourse or
contract. Yet, since advent the Gutenberg press and the vernacular reforms of
the Lutheran reformation, Western thought has taken legibility for granted.
<!-- footnote --> This is because until the advent of simulated text, print
had remained a remarkably stable medium.  Attempts to silence print through
book burning or censorship are viscerally obvious and met with nearly
universal disapproval. <!-- what the hell is that occludes, embeds ,
discretely--> Device textuality <!-- this needs a name remove threaten--> threatens to return
us to pre-Lutheran times, where legibility was the domain of the select few
and interpretation the privilege of the chosen. <!-- we cannot read to some
extent anymore -->

The advent of computational textuality necessitates a computational
hermeneutics, which enables unfettered access to text, code, platform, and
infrastructure. For now, commands like *xxd*, *pcap*, *ssh*, and *traceroute*
resemble arcane incantations that elicit hidden, symbolic action. Those who
wield them gain the metaphorical power to "hop" across, to "sniff" packets, to
"survey," to "traverse," and to "flood" network topographies. Computational
hermeneutics empower the reader to resist hard-wired models of machine-bound
interpretation. Yet today, resistance remains within the purview of the few.
Plain text channels itinerant streams of data back into the tidal pools of
human agency and comprehension for all. There, code becomes intelligible for
the very subjects whose loss Kittler laments. Only in such encrypted tunnels
and secure shells can anything like the digital humanities take root.

Computational hermeneutics encourages "users" to become active thinkers,
tinkerers, and makers of technology. It treats "binary" and "digital"
environments as fluid literary systems, amenable to the construction and the
deconstruction of meaning. I further encourage those who may have considered
themselves mere "users" to apply the same critical acuity they employ in the
close reading of prose and poetry to the understanding of code and machine.
For text to render on the screen properly it must be "encoded" or "translated"
from machine-transmittable code into human-readable shape. Encoding
constitutes a primitive field of textual activity, at the crossroads of
computer science and the study of literature. Encoding matters because how
text is encoded, transmitted, and stored decides who gets to decode, to
receive, and to access.

## 0.6 Theory and Practice

My approach to writing *Plain Text* stems from the desire to enact theory
capable of addressing the grim picture Friedrich Kittler paints at the end of
his influential monograph.[^ln-kittler2] Kittler was neither a technological
romantic nor a Luddite. I read the concluding chapters of *Gramophone, Film,
Typewriter* as a call to action. When Kittler writes that "media determine our
situation," he challenges his reader to choose between complicity and defiance
[@kittler_gramophone_1999, xxxix]. What can we do to counteract technological
determinism that Kittler warns about? In what follows, I outline several
intellectual lineages that frame my approach to the problem. A hermeneutics
capable of addressing Kittler's challenge must be grounded in materialism that
is both pragmatic and experimental.

Critical theory, at its best, aims to see "the human bottom of non-human
things" [@horkheimer_critical_1982, 143]. As such, it is one of our most
powerful tools for analysis and resistance against technological
determinism.[^ln-determine] But as Max Horkheimer wrote, "the issue is not
simply the theory of emancipation; it is the practice of it as well"
[@horkheimer_critical_1982, 233]. Recently, scholars like Kathleen
Fitzpatrick, Tiziana Terranova, and Trebor Scholz have began to turn the tools
of critical theory towards the instrumental contexts of knowledge production
[@scholz_digital_2013; @fitzpatrick_planned_2011; @terranova_network_2004]. I
join them to argue that in treating the instruments of intellectual production
and consumption uncritically, all of us---readers and writers---accumulate an
ethical debt.

For example, it is one thing to theorize about the free movement of literary
tropes across cultures and continents, and quite another to have that theory
appear in print behind paywalls inaccessible to most global reading publics.
Similarly, a theoretical distinction between form and content, when
instantiated in specific file formats like Microsoft Word (`.docx`) or Adobe
Reader (`.pdf`), establishes divisions of labor between editors, proofreaders,
book sellers, and offshore typesetting firms.[^ln-sweatshop] One group trades
"content" in the economy of prestige, another "formatting" in the economy of
survival, and yet another controls distribution in the market economy, for
profit.

Distinctions of labor will remain in place as long as the conversation about
ideas like "form" and "content" persists in the abstract. Materialist critique
cannot achieve its stated aims without purchase on the material world. My hope
is that by grounding theory in practice, *Plain Text* can begin to repay the
debt accrued by materialism divorced from matter. Complex systems that support
the life of the mind today seem to lie beyond understanding or agency. In
almost a decade of teaching critical computing in the humanities, I routinely
encountered otherwise informed people who nevertheless felt hopelessly
estranged from the tools of their everyday intellectual activity. I suspect
that much of the metaphysical angst directed against computation in general is
really a symptom of that basic alienation. Contemporary knowledge workers
stare into rectangular black boxes for a considerable part of their days,
suspecting, in the absence of other feedback, that their gaze is met in bad
faith.

Connecting theories of meaning--making to the practices of meaning--making
offers a way out of the bad faith conundrum. Bad faith identifies a
misalignment between thought and action. The solution to connect "meaning"
with "operational meaning" belongs to a species of pragmatism. William James
articulated that view concisely when he wrote that "reality is seen to be
grounded in a perfect jungle of concrete expediencies"
[@james_pragmatisms_1907]. For James and other pragmatists, truth could not
exist in the abstract alone. It entailed also causes and effects that operate
in the world.[^ln-pragma-truth] In his essay "Pragmatism's Conception of
Truth," James asked: "How will the truth be realized? What concrete difference
will its being true make in anyone's actual life? What experiences will be
different from those which would obtain if the belief were false?" Frank
Ramsey, the young British philosopher close to Ludwig Wittgenstein would later
write in a similar vein about meaning as "defined by reference to the actions"
[@ramsey_foundations_2013, 155].[^ln-pragmatism]

For the pragmatist, truth-carrying propositions of the shape "X is Y" (as in,
"the author is dead" or "art is transcendent") beg the questions of "Where?,"
"When?," "For whom?," and "What's at stake in maintaining that?" Following the
pragmatic insight of James and Ramsey, I will proceed with the conviction that
abstract categories like "literature," "computation," and "text" cannot
possibly be (although they often are) reduced to a number of essential,
structural features. Rather, to borrow from Wittgenstein's *Philosophic
Investigations*, categories denote a set of "family" practices that may or may
not share in any given familial characteristic
[@wittgenstein_philosophical_2001, 67-77].[^ln-witt] To visualize the familial
model, imagine a tree diagram, where the tangled branches of computation and
textuality intersect and diverge in beautiful and yet arbitrary ways.

<!-- this section is way underdeveloped, bring in the citation into the text,
possibly dewey and technics and perhaps do bring this human-computer
interaction  connect to experimentalism-->

As a consequence of my commitment to a pragmatic materialism, *Plain Text*
shares in the experimental turn affecting a wide range of humanistic
disciplines. When beginning the project, I was initially inspired by the
writings of a little-known but influential ninetieth century French physician,
who was one of the first researchers to incorporate experimentation into
medical practice.[^ln-bernard] Writing against the tradition of "inductive
generalizers," Bernard believed that medicine could only advance by "direct
and rigorous application of reasoning to the facts furnished us by observation
and experiment." "We cannot separate the two things," he wrote, "head and
hand." He went on to write that "the science of life is a superb and
dazzlingly lighted hall which may be reached only by passing through a long
and ghastly kitchen." "We shall reach really fruitful and luminous
generalizations about vital phenomena only in so far as we ourselves
experiment and, in hospitals, amphitheaters, or laboratories stir the fetid or
throbbing ground of life" [@bernard_introduction_1957, 3-15]. Today, the
lighted halls of literary and media theory lead to the scholar's workshop.

[^ln-bernard]: On Bernard see @petit_claude_1987 and @sattar_aesthetics_2013.

In an approach to "doing" theory, *Plain Text* joins the experimental turn
steering the academy toward critical practice, especially in fields
long-dominated by purely speculative reflection. The experimental turn
represents a generation's dissatisfaction with "armchair" philosophizing.
Recall the burning armchair, the symbol for the experimental philosophy
movement. Joshua Knobe and Shaun Nichols, some of the early proponents of the
movement, explain that "many of the deepest questions of philosophy can only
be properly addressed by immersing oneself in the messy, contingent, highly
variable truths about how human beings really are" [@knobe_experimental_2008,
3]. The emergence of spaces where research in the humanities is done
exemplifies the same trend.  In naming the locations of their practice
"laboratories,"  "studios," and "workshops," humanists reach for new metaphors
of labor. These metaphors aim to reorganize the relationship between body,
space, idea, artifact, instrument, and inscription.

<!-- more on the experimental turn -->
<!-- highlight a specific project from pamela smith, describe pamela's project -->
<!-- expand -->

As an example of what I have been calling here the "experimental turn" in the
field of early modern history consider the preface to a recent volume on *Ways
of Making and Knowing*, edited by Pamela Smith, Amy Meyers, and Harold Cook.
The editors write that the "history of science is not a history of concepts, or
at least not that alone, but a history of the making and using of objects to
understand the world" [@smith_ways_2014, 12]. Smith translates that insight in
the laboratory, where, together with her students, she bakes bread and smelts
iron to recreate long--lost artisanal techniques. For those who experiment,
"book knowledge" and "artifactual knowledge" relate in practice.

Artifactual knowledge---from typesetting software to e-book readers and word
processors---shapes our everyday encounter with literature. Such technologies
should not be taken as value-neutral conduits of information. I follow Lewis
Mumford and Langdon Winner to argue that technology affects the exercise of
textual politics in subtle and profound ways. Artifacts cannot hold beliefs
about politics. Political power is rather exercised through them. Stairs do
not discriminate against the mobility impaired. The failure to enforce
accessibility through specific legal and architectural choices does.
Typesetting software, e-book readers, and word processors similarly embody
implicit communication models: ideas about deliberation, ethics of labor,
discursive values, and views about "natural" human aptitude for
interpretation. In this way, contemporary documents are capable of enforcing
limits to access by license, geography, or physical ability, for example.

To what extent does the book in front of you permit or enable access?
Whatever the answer, a function of understanding the text must include the
explication of its physical affordances. Experimentalism enables the critic to
"lay bare" the device. A literary scholar's version of baking bread and
smelting iron is to make literal the figure suggested by the idea of media
"archeology" on the level of the mechanism. In *Plain Text* we will dig
through, unearth, and excavate textual machines.

Similarly, it is my contention here that the cardinal literary-theoretical
concepts---such as word, text, narrative, discourse, author, story, book,
archive---are thoroughly enmeshed in the underlying physical substratum of
paper and pixel. It follows that any attempt to articulate the idea cannot
attain its full expressive potential without a thick description of its base
particulates.

Luckily for us, reading and writing are not esoteric activities. They are
readily available to phenomenological introspection. I will therefore
occasionally encourage readers to encounter the immediate contexts of their
reading anew: to put down the book or to lean away from the screen and to look
at these textual artifacts with strange eyes. In this movement of the body, I
want to disrupt the mind's habituated intuitions, pitting them against
knowledge "at hand" and fingertip knowledge: as when ruffling through the
pages or typing at a keyboard. To what extent is electronic textually
ephemeral, for example? (I discuss the question in full in chapter two.) The
idea of "ephemerality" can be made more palpable by asking: What can a reader
do with this text, here and now? Are readers able to copy and paste? Do they
have legal permissions to quote at length, to perform publicly, or to
otherwise trans-mediate?

## 0.7 Plan of the Present Work

In the chapters to follow, the electronic literary artifact---the thing next
to my laptop on my desk---will come fully into view as a computational device.
I will insist on our ability to read such devices with the aim of developing
what I call the *hermeneutics of computational reading*. I will argue (a) that
the material history of computing belongs to the history of text processing as
it does to the history of machine control, (b) that the computer stands at the
root of *simulated textuality*, and (c) that the simulated sign splits to
reside in at least two distinct locations, screen and storage medium: the
first conspicuous but ephemeral, the second enduring but opaque. The "illusion
of the simulated text" identifies an incongruity between the two sites of
reading. The later chapters of the book will deal with the cultural
consequences of that illusion.

It will take us the rest of the book to come to terms with ambiguous textuality
in its bifurcated form. For now, I begin merely with the peculiarity of its
location. Computed text resides at multiple sites at the same time. Device
reading happens on screens that refresh themselves at a rate of around 60
cycles per second (Hertz). Screen textuality is therefore by definition
ephemeral. It is technically an animation. It moves even as it appears to stand
still. And it disappears when the device loses power. Yet paradoxically,
simulated text is also pervasive. Embedded into "solid state" drives and
magnetic "hard disks," inscription attains the quality of what Wendy Hui Kyong
Chun calls the "enduring ephemeral" [@chun_enduring_2008, 148]. Precisely
because simulated text adheres lightly to its medium, it has a tendency to
multiply and to spread widely. Like the fine particles of glitter, it is
difficult to contain. The simulated text falls apart into bits and pixels that
replicate and tumble about the system.

<!-- needs more of a conclusion -->

<!-- new connector -->
<!-- rewrite -->

The tangled strains of thought and practice can exist only in relation to specific
communities of practice. A researcher cannot therefore expect to discover *the*
authoritative dictionary containing a neat summary of all terms needed for
analysis. What counts for "code" and "poetry" in one domain, like computer
science, may not account for the same in another domain, like creative writing.
An engineer's use of the words "code" and "poetry" differs from that of a
poet's. And yet an engineer by day can also become a poet by night. The
changing contexts evoke the corresponding shift in operational definitions.
Consequently, in *Plain Text*, I do not attempt to write a totalizing history
of modern computing nor a survey of literary theory. Rather, the argument
progresses from the action of the alphanumerical keyboard switch, through
copper and silicon, to liquid crystal and the floating gate, and on towards the
human and the community. It is but a single possible pass through a cavernous
black box. Each chapter reflects a waypoint along the journey.


The passage from keystroke to pixel runs a thread through the book. I begin
with a few simple questions that lay the grounds for digital textuality: What
exactly is a text? Where is it?  How will we find it?  My answers commence by
developing a theory of "microanalysis," the closest possible kind of reading
that pays attention to the material contexts of knowledge production.
Microanalysis opens the gap between text as it is embedded into storage media
and text as it is seen on the screen. I begin by laying out the case for
treating the contemporary "digital" book as a computational device. The nature
of computation

In the second chapter, I challenge conventional notions of digital media,
discovering properties at once "discrete" and "continuous." Both are
conventionally used to to imply a diminishment of the human capacity for
comprehension.  Which one is it, before or beyond the human?  A treatment of
Liquid Crystal Displays (LCDs) illustrates that all text is already in some
sense "born digital," that is, by formal definitions, "reproducible" and
"differentiated" throughout. Furthermore, digitality depends on "reliable
processes of copying and preservation"---attributes that can mean something
different to a philosopher than to a librarian. From these insights I take it
that "being digital" is not an intrinsic ontological property, but rather
structure imposed from without. A thing is not digital in itself---one makes
use of something in a digital way. Materials from the history of telegraphy in
the late ninetieth and early twentieth centuries help narrate the story of
character encoding---a key hidden component of digital textuality.

I begin the book's third chapter by outlining a recent discussion on surface
reading. I ask: What lies beneath the text, literally? Case studies from the
early history of removable storage media, ticker tape and floppy disk,
elucidate the movement of text from human-legible inscription on the page and
punch card to magnetic inscription invisible to the naked human eye. In print,
form and content lie flat. On the screen, the two layers occupy physically
distinct strata. The rise of the pervasive Document Object Model (DOM) in the
twentieth century introduces a third layer which, in addition to form and
content, encodes device control. The material history of the DOM dispels the
illusion of flattened textuality. The control layer dynamically changes the
mechanics of the literary encounter. Texts now carry both a message and a set
instructions that alter the textual device. Consequently, I argue that literary
interpretation must include the exposition of the control layer.

The fourth chapter reaches beyond the device to the human. I begin with a close
reading of Beckett's *Krapp's Last Tape*. The title character makes yearly
audio recordings of himself, only to revisit them and to enter into a sort of
dialog with his own voice from the past. I posit this archival encounter as
Krapp's "media being" and suggest that such encounters are commonplace. Writers
and book collectors regularly deposit "snapshots" of their consciousness into
files, bookshelves, and folders. Jean-Paul Sartre's idea of an "appointment
with oneself" helps us see this external construction of files, folders, and
library furnishings as cognitive extension, in need of delicate pruning and
arrangement. In this light, I show that documents exist not as completed works,
but as "vectors" that mutate and move through time and space. Finally, I ask:
What is being externalized, communicated, and preserved? And answer: It is not
simply a message, but the subject itself.

In the last chapter of the book I tackle the apparent immateriality of digital
text. Ephemeral media brings promise of epistemological (social) and even
phenomenological (personal) transformation. But it also has a major practical
drawback. Inscription on magnetic tape cannot be assumed to correspond to the
composite screen image. Forms of governance like Digital Rights Management
(DRM) are embedded deep within the "data object" itself. A type of
steganography, or secret writing, hides the control layer from
view---precluding, and sometimes making illegal outright, the possibility of
interpretation. The chapter ends with a stark image illustrating the contrast
between screen surface and the underlying bit structure. To produce the image,
I inject malicious code into an Adobe Acrobat file (`.pdf`). The deformed text
threatens to damage the literary device. A thick description of the book, now
as weapon and instrument, brings legibility to the fore of reading ethics. The
design and usage of literary devices must itself become critical practice which
can, in complement to critical theory, actively engineer for human agency. A
discussion of technological dissent through the new humanisms of Hannah Arendt
and Franz Fanon concludes the volume.

In the process of textual production, printing and typesetting, it is certain
that my words were mixed with machine language, which in turn changed the
structure of the devices in your lap, in your hand, near to your eye,
embedded, or embodied. I could say that I bear no responsibility for extending
the reach of machine language so close to the reader. But that would be
factually incorrect. The choice of my writing implements and my channels of
communication affect deeply the contexts of interpretation.  Such choices, in
aggregate, define the shared ecosystem of knowledge that surrounds us.
Traditional strategies of close reading which limit interpretation to the
parsing of visible content risk missing the concealed machinations of naked
circuit control. It looks like you are reading a book, but this book may
change depending on the readers race, gender, ethnicity, geography, or
political affiliation. Who has agency program the device? Were a book also a
pill or fused with the neural circuitry of the brain, would you know what and
whom you were reading?


[^ln-flusser]: The work of @flusser_does_2011 has been similarly influential.

[^ln-cog]: See for example @gibbs_categorization_1992; @blasko_effects_1993;
@gibbs_poetics_1994; @neal_role_1997, 441-463; @gentner_alignment_1997.

[^ln-hegel]: I discuss the topic at length in Chapter 3.

[^ln-uni]: The Unicode Consortium. *The Unicode Standard: Worldwide Character
Encoding*, Version 1.0, Volume 1. Reading, Mass.: Addison-Wesley, 1990.

[^ln-lacan]: The evanescent absence of life that Lacan mentions as "the sign
about which Robinson Crusoe would make no mistake" [@lacan_seminar_1997, 167].

[^ln-pragma-truth]: For a more thorough discussion on the topic see
@seigfried_william_1990, @pihlstrom_structuring_1996, and @putnam_jamess_1997.

[^ln-pragmatism]: The intellectual legacy of pragmatism is wide-ranging and diffuse. It is
perhaps most pronounced in the teacher colleges, where James and Dewey are
still read widely, which could explain the ascendancy of such pedagogical terms
as "situated cognition"[@brown_situated_1988, @lave_situated_1991] and
"experiential learning"[@kolb_hegel_1981]: both terms denoting some sense of
necessary synthesis between of knowing and doing. In the field of linguistics,
philosophy of language, and communication studies, pragmatics are
well-encapsulated by the "language-as-action tradition," which harkens back to
the Oxford language philosophers like J.L. Austin, Paul Grice, and John Searle
[@trueswell_approaches_2005]. Austin's *How to Do Things with Words,* is
perhaps the paradigmatic formulation of the idea that words don't just mean
things, but that they enact change in the world.

[^ln-witt]: For more on the connection between Wittgenstein and James
see @goodman_james_2004.

[^ln-sweatshop]: See @freeman_high_2000 and @patel_working_2010.

[^ln-kittler2]: *Gramophone, Film, Typewriter* ends as follows: "And while
professors are still reluctantly trading in their typewriters for word
processors, the NSA is preparing for the future: from nursery school
mathematics, which continues to be fully sufficient for books, to
charge-coupled devices, surface-wave filters, digital signal processors
including the four basic forms of computation. Trenches, flashes of lightning,
stars---storage, transmission, *the laying of cables*
[@kittler_gramophone_1999, 263].

[^ln-determine]: I mean "determinism" as both (a) a belief in the intrinsic
agency of complex systems and (b) a practice of diminishing the scope of human
freedoms by technological means.

[^ln-capital]: Scholars like Alexander Galloway, David Golumbia, Bernard
Harcourt have advanced critique along similar lines. See
@galloway_protocol_2006, @golumbia_cultural_2009, and @harcourt_exposed:_2015.

[^ln1-brains]: For the first view see @putnam_minds_1960 and
@fodor_language_1975. For the second view see @deutsch_quantum_1985 and
@dyson_turings_2012.

[^ln-krsh]: The idea of reverse engineering is suggested also by Matthew
Kirschenbaum's idea of "forensic reading." See @kirschenbaum_mechanisms_2008.

[^ln-exposed]: For an extended exposition of this dynamic see
@harcourt_exposed:_2015.

[^ln-exposed2]: Again, a point that is given its full treatment in
@harcourt_exposed:_2015.

[^ln-dmca]: See @ku_critique_2004; @ginsburg_legal_2005; and @fry_circumventing_2009.

[^ln-translation]: Translations are mine unless source cited explicitly in
English.

[^ln-winner]: See for example: "Writers concerned with with problems of
technology-out-of-control have frequently echoed Hobbes in suggesting that
such an artifact---the Leviathan of interconnected technical systems---has a
soul of its own [...] A ghost appears in the network.  Unanticipated aspects
of technological structure endow the creation with an anticipated *telos*"
[@winner_autonomous_1978, 280].

