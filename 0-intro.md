# Simulated Text: an Introduction

As I write these words, a ceiling-mounted smoke detector in my kitchen emits a
loud noise every three minutes or so. And at every 15 minutes, a pleasant
female voice announces also "low battery." These precautions are stipulated by
US National Fire Alarm Code 72-108 11.6.6 (2013). The clause requiring a
"distinct audible signal before the battery is incapable of operating" is not
only required by law, it is *encoded into the device*. The device internalizes
legislation in its programming. Thus we obtain the condition where the two
meanings of code---as governance and machine instruction---coincide. Code
equals code.

I am not at home. But I know the alarm is happening because I receive
notifications of it on my phone---another small computational device, next to
my laptop. This mobile appliance also contains most of my book library. I pick
it up to read a book. But what I mean by "reading a book" obscures a metaphor
for series of odd actions. The "book" is a small, thin black rectangle: three
inches wide, five inches tall, and barely a few millimeters thick. A slab of
polished glass covers the front of the device, where the tiny eyes of a camera
and a light sensor also protrude. At the back, made of smooth soft plastic, we
find another, larger camera. At the foot of the device a grid of small
perforations indicate breathing room for a speaker and several microphones. To
"open" a book I touch the glass. The machine recognizes my fingerprint almost
instantly. I then tap and poke at the surface until I find a small image that
represents both my library and the book store, where I can "purchase books."
To "buy a book," I agree to a limited licence, granting me access to data
which the software then assembles into on-screen representation of books. I
tap again to begin reading. The screen dims to match room ambiance as words
fill the screen. One of the passages on the first page appears underlined: a
number of other readers in my social circle must have found the passage
notable. My finger slides along the glass surface to turn the page. The device
emits a muffled rustle, as if to remind me of the underlying analogy. The
image curls ever so slightly in a way that resembles a printed page as another
"page" slides into view. My tiny library metaphor holds uncounted thousands of
such page metaphors.

Despite the appearances, the electronic metaphor-making device next to my
computer has more in common with the smoke detector than it does with several
paper volumes also scattered across my desk. Both devices, the electronic book
and the smoke alarm, comprise printed circuit boards, capacitors, silicone
chips, and resistors. Both draw electric current. Both require firmware
updates and are governed by codes: political and computational. The smoke
alarm and the mobile phone connect to the network. They communicate with
remote data centers and with each other. And yet, I continue to read
electronic books as if they were familiar, immutable, and passive objects:
just books. I think of them as intimate artifacts---friends even---wholly
known to me, comforting, and warm. The electronic book is none of those
things. Besides prose, it keeps my memories, pictures, words, sounds, and
thoughts. It records my reading, sleeping, and consumption habits. It tries to
sell me things, showing me advertisements for cars, jewelry, and pills. It
comes with a manual and terms of service. It is my confidant, my dealer, my
spy.

## 0.1 Simulation

The aim of *Plain Text* is to dispel a pervasive illusion. The electronic book
is but a compelling metaphor that belies material realities of reading and
writing, transformed by the advent of computation. The device on my desk is
not a book, but a simulation of a book. And everything associated with
"reading" this metaphor must in itself be understood under the sign of
simulation. What does it mean to "study" a book metaphor? And what kind of a
metaphor is it? How did it come into being and how does it affect practices of
literary interpretation? In *Plain Text* I attempt to come to terms with the
conditions of simulated textuality.

The illusion of ephemerality that follows simulation comes at a price of
understanding. "Software's ghostly presence produces and defies apprehension,"
Wendy Chun wrote in her *Programmed Visions* [@chun_programmed_2011, 3]. But
what happens when all text is a type of software? Friedrich Kittler ends his
book on a similar note. In his vision, literature has finally been defeated by
military-grade encryption, secrecy, and obfuscation [@kittler_gramophone_1999,
263]. Attempts to silence print through book burning or censorship are
viscerally obvious and met with nearly universal disapproval. Unlike
censorship or the burning of books, hard-wired computational governance
proceeds by clandestine means. The simulation-producing nature of computed
text preserves the outward appearance of printed text, while concealing the
specifics of governance and control. I mean governance and control in the most
direct way possible, as a mode of physical regulation that erect barrier to
production, access, and distribution of knowledge. The challenge of *Plain
Text* will be in the description of such emerging but often occluded
technological contingencies.

A concern with the material conditions of simulated textuality leads us to a
rich archive of materials from the history of literary theory, semiotics,
telegraphy, and electrical engineering from the middle of the nineteenth to
the middle of the twentieth centuries. Reflecting on the development of Morse
Code in 1949 in the *Proceedings of the American Philosophical Society*, Frank
Halstead mentions the difficulty of finding a home field in the arts or
sciences for what he calls "code development." "It is a matter somewhat
related to the general art of cryptology," he writes, "yet it is not wholly
divorced from electrical engineering nor from general philology"
[@halstead_genesis_1949, 456]. As Halstead anticipated, research in the field
of code development and computational culture has led me to range of rare
primary materials: from the proceedings of the Association for Computing
Machinery (ACM) to the United States Patent and Trademark Office; from Bell
Labs to early Soviet publishing houses that heralded the advance of formalism;
from studies on animal communication behavior, to Unix manuals, to textbooks
on semiotics, and to foundational texts in the philosophy of aesthetics and
literary theory.

I deploy that archive to argue that extant theories of interpretation evolved
to function under the conditions tied to static print media. By contrast,
electronic texts are capable of changing dynamically to suit their readers,
political conditions, or geography.  Consequently, I argue for the development
of what I term *computational hermeneutics*, a strategy of interpretation
capable of reaching past the surface content to reveal also the software
platforms and the hardware infrastructures that contribute to the production
of meaning.  Drawing on a range of materials at the intersection of literary
thought and the history of modern computing, *Plain Text* examines a number of
key literary-theoretical constructs, recasting the paper book as a metaphor
machine and computational object.

I appeal to the idea of "plain text" in the title of this book to signal an
affinity with a particular mode of computational meaning making. In technical
terms, *plain text* identifies a file format and a frame of mind. As a file
format, it contains nothing but a "pure sequence of character codes." Plain
text stands in opposition to "fancy text," "text representation consisting of
plain text plus added information."[^ln-uni] In the tradition of American
textual criticism, "plain text" identifies an editorial method of text
transcription which is both "faithful to the source" and is "easier to read
than the original document" [@cook_time-bounded_1972]. Combining these two
traditions, I mean ultimately to build a case for a kind of a systematic
minimalism when it comes to our use of computers---a minimalism that
privileges legibility and human comprehension. I do so in contrast with other
available modes of human--computer interaction, which by contrast with minimal
computing privilege maximizing system--centric ideals like efficiency, speed,
performance, or security.

The use of plain text implies also an ethics of reading and writing. The title
therefore further identifies an interpretive stance one can assume in relation
to the making and the unmaking of literary artifacts. Besides visible content,
all contemporary documents carry with them a layer of hidden information.
Originally used for typesetting, the digital formatting layer affects more
than innocuous document attributes like "font size" or "line spacing."
Increasingly, devices that mediate literary activity encode forms of
governance. Such devices tacitly police intellectual property laws, censor,
and carry out surveillance operations. For example, the Digital Millennium
Copyright act, passed in the United States in 1996, goes beyond written
injunction to require the management of digital rights (DRM) at the level of
hardware. An electronic book governed by DRM may prevent the reader from
copying or sharing stored content, even for the purposes of academic
study.[^ln-dmca] Computational hermeneutics strives to make such control
structures available for critical reflection. Building on the recent work of
scholars like Wendy Kyong Chun, Tung-Hui Hu, and Lisa Gitelman I make the case
for an empowered poetics, able regain a measure of control over the material
contexts of our knowledge production [@chun_enduring_2008;
@gitelman_paper_2014; @hu_prehistory_2015].

## 0.2 Displacement

To come to terms with the conditions of simulated textuality *Plain Text*
embodies a reconciliation of vocabularies. It is a response to a particular
situation of a literary scholar encountering the field of software
engineering. For a long stretch of my professional life, these two areas of
activity remained separate. I worked at one and I studied the other. At the
time, I simply did not think that code had anything to do with poetry. The
idea for the book came to me in a moment of realization after I was asked one
of those naive but fundamental questions of the kind that can set research in
motion down a long and winding path. A childhood friend who loves books asked
about the difference between text in print and text on the screen. It was in
that struggle to articulate difference that I realized that some of my deepest
assumptions about literature relied on the centuries-long stability of print
media. Despite my professional experience as a programmer, I could not readily
explain the mechanisms by which keystrokes turned into pixels, pixels into
letters, and letters into words. I could recount technical detail on some
level, but my knowledge was also riddled with unexamined gaps.  It did not
amount to a coherent story. I was, despite my best efforts, surrounded by
magical lanterns that cast shadows of code and poetry.

Initially, at the point of contact, the two selves---the scholar and the
engineer---spoke different languages. It was and continues to be a
disconcerting process by which things dear and familiar to me, in both worlds,
grew strange and unfamiliar, showing themselves to be sometimes less than and
sometimes more than I comfortably expected. Nothing could be assumed from the
start. Field specific jargon, down to naive foundations, had to be examined
for hidden assumptions that prevented dialog. With time, I saw that code and
poetry have much to do with one another. Writing this book has taught me to
embrace the remaining incongruence.

The Czech-born philosopher Vilém Flusser understood the condition of unease
that comes with emigration, both physical and mental, to be a type of
information processing. I relate to his work as a former refugee fleeing the
dissolution of the Soviet Union, a transplant into Silicon Valley culture from
a strict literary education, and now a lapsed engineer among humanists. These
vantage points offer a singular view onto the material conditions of
contemporary intellectual life.

Technology offers us an irresistible compromise, by which we trade critical
understanding for comfort. Habit covers the various homes we make for
ourselves in the world "like a fluffy cotton wool blanket," Flusser
wrote---"it smoothes the sharp edges of all phenomena that it covers so that I
no longer bump against them, but I am able to make use of them blindly." "When
I sit at my desk," he went on to write, "I don't see the papers and the books
that are lying all about because I'm used to them" [@flusser_freedom_2003, 13
and 82]. Familiar objects pass no information to their users, according to
Flusser. Like water that surrounds fish, they pass into the background of
experience. Mediums become media. They disappear into the background, cease
producing meaning, become a stage for meaning-making, and like the stage
disappear from view.

The condition of exile, by contrast, allows the displaced to once again
transform habituated media into meaningful information. In exile, "everything
is unusual," Flusser writes [@flusser_does_2011, 81]. The migrant experiences
the world as an ex-perience [*er-fahrung*], or literally a driving out.
Discovery, he concludes "begins as soon as the blanket is pulled away," where
the familiar objects can pass into view again [@flusser_does_2011, 86-7].

Our challenge today is to uproot ourselves from the comfort that rapidly
descends on the dwellings of our intellectual life. Dulling the senses,
seemingly inconspicuous conduits of information---electronic books and
desks---acquire a sense of agency of their own. Devices that "watch," "hear,"
"see," and "think" give rise to object-oriented phenomenology and the internet
of things. A new generation of artificially intelligent objects, things like
smart phones, smart light bulbs, and smart watches, Intels and Idea Pads,
enter the networked public sphere in the role of seemingly independent
agents.[^ln-winner] Marx's "table that evolves grotesque ideas out of its
wooden brain" can now be re-branded into Microsoft Surface and PixelSense,
product names from the life cycle of an actual smart table
[@marx_captial_1906, 82; @wigdor_designing_2009].

I will argue here that if we hope to understand literature "under conditions
of high technology" (to echo Friedrich Kittler) we can only do so from the
position of humanism. One cannot at the same time lament the systematic
erasure of the human from the literary process and advocate for post- or
anti-humanism. Unlike Kittler, who once wrote that under conditions of high
technology "literature has nothing more to say," I believe that literature and
literary analysis continue to have a voice in contemporary life
[@kittler_gramophone_1999, 263]. Technology does not determine silence.
Rather, the grounds for all reflective textual activity slowly erode,
returning to their natural and meaningless state of entropy.

To prevent the slide into illiteracy outright, instruments of literary
analysis must evolve to elicit meaning under new technological contingencies.
Where texts are encrypted, we decrypt. If as Kittler writes, automated
discourse analysis threatens to take command, we will engineer automatons that
command on our behalf [@kittler_gramophone_1999, 263]. Such acts of
resistance, small and large, can recover a measure of agency from the ruling
determinism of---take your pick: markets, complex systems, unconscious drives,
monistic universes, *gaia* science, social networks, technology, the
singularity, bureaucracy, war...  Indeed, the possibility of human
erasure---nonsense---never strays far from reach, "like the face drawn in the
sand at the edge of the sea" [@foucault_order_1994, 387]. The fragility of
life compels not the Foucauldian wager on anti-humanism, but the need to
mobilize whatever modest means available to humans to persevere against great
odds.

By mistaking things for animate actors, we ourselves have become enmeshed in a
system of digital production that commodifies human experience. Objects that
surround us collect our reading habits, social interactions, and intimate
conversations.[^ln-exposed] But the actual living agents that benefit from the
trade in personal information are neither cyborgs nor post-human assemblages.
The bargain that trades critical understanding for comfort benefits specific
interests, like multinational corporations and government intelligence
agencies. The mythology surrounding smart objects shifts our attention from
the seats of power to things dumb, powerless, and indifferent to our
protestations.

The internal exile that we must undergo for smart books and smart desks to
come into view, to become foreground, cannot compare in difficulty to the
experience of physical displacement that follows natural disaster, war,
poverty, or political instability. Yet, our systematic reluctance to take on
even those small intellectual discomforts that could lead to acts of localized
dissent and disobedience---to write using free software, to build open
archives, to share memories in private---cannot be said to exist apart from
the complex systems that perpetuate inequity and violence globally. The
emotional affirmation that accompanies exuberant social networking brings with
it governing structures invoked widely in the name of law enforcement and
national security. Comfort and security constitute part and parcel of the same
ill-conceived bargain that leads to critical disempowerment. But where it is
difficult to imagine or to enact strategies of digital disobedience on a
universal scale, we can begin to address them through numerous minute
transactions that in aggregate brace everyday literary exchange.

To pick up an electronic book and to take it apart may be against the law in
some jurisdictions [@fry_circumventing_2009]. Given the extent to which
emergent thought-things like electronic books and personal communication
devices participate actively in the production of meaning, we can no longer
employ strategies of interpretation at the level of ideology alone. Close
reading, critical theory, and literary analysis must reach down to the silicon
bedrock that stages the very act of interpretation. Literary theory, a
discipline fundamentally engaged in the exegesis of all figurative tropes, is
therefore crucial to the understanding new computational environments, which
envelop intellectual life through metaphoric substitution.

I begin where Bernard Harcourt's recent book on digital disobedience ends:
with the possibility of localized dissent, limited for now to the immediate
physical contexts of reading, writing, and finding knowledge. Following
itinerant theorists of the metaphor like Vilém Flusser, Viktor Shklovsky, and
Hannah Arendt I propose to proceed through systematic estrangement of
computational metaphors, metaphors that fade into transparency as to escape
the critical gaze. A strategy of deliberate defamiliarization reclaims the
metaphor-device for analysis. To come to terms with the book as a device; to
begin to understand the nature of the text simulation; to perceive the
particularity of the computed sign: these are the aims of *Plain Text*.

## 0.3 Metaphor

Computation advances into everyday life though metaphor. Like all tropes, such
metaphors lose their evocative power with frequent use. The task of a literary
scholar becomes then to explicate and to renew the trope, in search for
parallelisms that, as George Lakoff and Mark Johnson explain, configure one
conceptual system in terms of another [@lakoff_metaphors_1980, 3-14]. Why do
we call some software programs "applications" for example? The application of
what to what? Apple's iconic *Human Interface Guidelines*, a manual of style
that heralded the era of "what you see is what you get" interfaces, contains
explicit echoes of Lakoff's thought. The manual urges the designer to "convey
meaning through representation" and to seek the metaphor appropriate to the
task [@apple_apple_1987, 11]. Do not ask the user to throw "documents" into
"jars," for example, the manual entreats: "dragging the document icon to the
Trash means the user wants to discard that document" [@apple_apple_1987, 229]
Simulated objects must "look like they do in real world" [@apple_apple_1987].
In the words of cognitive scientists John Carroll and John Thomas, whose work
the Apple design guidelines also reference extensively, "people employ
metaphors in learning about computer systems." Using the appropriate metaphors
therefore provides "wide-ranging improvements in learning ability and ease of
use" [@carroll_metaphor_1982, 108]. What users know about trashcans in the
real world can be used to guide behavior in the virtual one.

The rapid uptake of "what you see is what you get" interfaces pioneered by
Apple proved the effectiveness of metaphoric design. Through metaphor, users
are able to extend models of action from one domain into another. Empirical
studies have later shown the effectiveness of metaphor in computation
[@blackwell_does_1999; @sallam_use_2009]. Flusser would say that such
figurative habituation makes experience more "smooth." The user avoids
"bumping into the sharp corners" of new and complicated computational
environments. The metaphor can conserve mental energy. But, once internalized,
it also no longer attracts attention. The seminal work on figurative speech
undertaken by the Russian formalists at the turn of the twentieth century
reminds us that effective metaphoric shortcuts to learning have their
cognitive downside. Habituated experience passes into the unconscious
[@brik_poetika_1919, 104]. The thing "dries up" in Shklovsky's words, first in
perception and then in practice [@brik_poetika_1919, 38 and 104]. The figure
becomes so familiar that we cease thinking about it---an early insight that
also was confirmed experimentally almost a century later.[^ln-cog]

The formalists understood habituated metaphors to  diminish the vitality of
experience. Shklovsky quotes from the diaries of Lev Tolstoy, who, while
dusting his room, could not remember if he already dusted his sofa.  "Because
actions like these are habituated and unconscious, I could not remember and
knew that it was impossible to remember," wrote Tolstoy. "And so whether I
dusted and forgot or just did so without thinking, it as if the action never
happened [..] thus when life passes without conscious reflection, it passes as
if one has not lived at all." Life disappears into nothingness, when the
automatization of experience "consumes things, clothing, furniture, your
spouse, and the fear of war" [@brik_poetika_1919, 104].[^ln-translation] The
formalists rarely quoted Marx directly. Yet the echoes of Marx resonate
throughout. The dead metaphor marks the alienation of experience. The point at
which material artifact disappears from individual conciousness is the also
the point where it appears within the social sphere as fetish.

Carroll and Thomas wrote their recommendations with novice users in mind. The
metaphor was supposed to ease the transition into a virtual world. But once
inside, the virtual inhabitant also began to mistake metaphor for reality.
The *Apple Design Guidelines* assumed that learning would happen through
metaphor. But more than forty years of computing under the guidelines have
failed to produce a learned public. This is evidenced by the frequent and
urgent calls to digital literacy on the part of our leading intellectuals,
educators, and politicians.[^ln-digitalliteracy] Instead of educating, the
preference for easy figurative tropes forged a shortcut to knowledge. Once
established, such shortcuts continue to prevent users from literal engagement
with computational realities. Like Tolstoy's mindless dusting, life at the
keyboard passes without reflection. Worse yet, the trope obscures forces of
capital, governance, and control cloaked behind the innocuous figure.

Estrangement [*ostranenie*], sometimes also translated as "defamiliarization",
was the formalist answer to the ossified metaphor. Estrangement was meant to
recover the experience of the making of the thing, to name it, and to describe
it as if one encountered it for the first time [@shklovsky_voskreshenie_1914;
@shklovksy_sborniki_1917, 8]. The formalists found estrangement in art,
politics, and analysis alike [@jameson_prison-house_1972, 75-9;
@boym_estrangement_1996; @holquist_minding_2005]. In art, estrangement
proceeded through poetic experimentation: neologism, synecdoche, consonance,
or repetition, among other devices. Thus, the childlike incantation "a door
ajar" can evoke the etymological roots of the onomatopoetic "to jar" in the
sense of "to emit a harsh  sound"; the sense of the word as a vessel or a jar,
which comes from the Arabic *jarrah*; and to "ajar" which although relates to
the Old English *cyrr* or *cierr* in the meaning of "turn," as in "chore" or
"a turn of work", was in fact in the eighteenth century confused with jar in
the sense of "a harsh sound" to form the now rare "ajar" in the sense of "to
be in a jarring state; out of harmony; at odds; awry" [@_ajar_2015;
@_ajar_2015-1; @_jar_2015; @_jar_2015-1]. In this way the initial parallelism
between two usually unrelated words, a door and ajar, first creates a novel
analogy but then leads to renewed insight. Could the nonsensical *adoor* carry
the semantic weight of an adverb? Is it a chore to turn a jar? The
etymological excavation leads us to rediscover unexpected confluences in the
words' usage and origins.

Where estrangement in art could renew the word through suggestive, often
trans-rational experience, in scholarly practice the formalists adopted a
scientific stance towards the object of their study. Unlike the previous
generation of critics who concentrated on the historical, biographical, and
philosophical aspects of literary formation, the formalists wanted to
reconstruct how the thing was made. For example, in his influential essay on
"How Gogol's *Overcoat* Was Made" Boris Eikhenbaum identified *skaz*, or the
manner of speaking, as an organizing principle for Gogogl's satirical
narratives [@brik_poetika_1919, 151-67]. What readers lost in having the joke
explained to them they gained in understanding of the genre.

I take a similar approach in *Plain Text* by extending formal concerns with
the mechanics of literary "device" and "technique" to literary devices and
technics proper. I too am interested in how the literary thing is made.
Computers, I argue throughout, are essentially machines for universal symbolic
manipulation. This property makes them particularly suitable as objects for
figurative analysis, which aims to explicate concepts involved in the making
of the metaphor.[^ln-egturner] Influenced by the tradition of literary
exegesis in the work of scholars like Boris Eikhenbaum and Mark Turner, my
overarching aim in *Plain Text* is therefore to expose the illusion of
verisimilitude between text on paper and the simulated text on a screen.  The
words may look the same, but the underlying affordances of the medium differ
in significant detail. Consider for example a news report that alters its
content based on the reader's location. Imagine also an e-book reader that
highlights popular passages of a novel in real time, shortening the less
popular passages down to their algorithmically distilled summaries. For a
literary analyst, the instability of the digital medium means analysis cannot
be confined to reading for surface meaning alone. How can close reading
persist when reading devices reconfigure the text to fit individual tastes,
mood, or politics? How would we even agree on the fact that we are reading the
same text? The very possibility of interpretation comes into question.

Despite its power to recall the world anew, estrangement cannot be practiced
effectively in the mode of a monologue. To produce meaning, Flusser reminds
us, it must become dialogical practice. Perpetual exile is otherwise
uninhabitable [@flusser_freedom_2003, 81]. Estrangement thrusts the displaced
into the chaos of unsettled existence. With time, the displaced make a new
home, from which they can once again "receive noise as information" and
produce meaning. "I am embedded in the familiar," Flusser wrote, "so that I
can reach out toward the unfamiliar and create things yet unknown"
[@flusser_freedom_2003, 12]. The expellee and the settled inhabitant need each
other. The dialectics of exile can lead to "informed renewal" of shared space
through what Flusser called a "creative dialogue" between the settled and the
unsettled [@flusser_freedom_2003, 84]. Without the protection of one's home,
everything turns to noise. There can no information without a dwelling,
Flusser wrote, "and without information, in a chaotic world, one can neither
feel nor think nor act" [@flusser_does_2011, 12]. By this dynamic displacement
and habituation enter into a continuing conversation. But what is the "other"
to estrangement?

## 0.4 Computational hermeneutics

In *Plain Text*, I model the reciprocal movement to making strange on the
diverse practices of reverse engineering. Similar in method to what Matthew
Kirschenbaum calls "forensic argumentation," reverse engineering recalls the
formalist strategy of experiencing the making of the thing through careful,
case study-based reconstructions of the textual mechanism
[@kirschenbaum_mechanisms_2008, 15]. The function of a case study in an
engineer's education, as Henry Petroski explains in his *Invention by Design*,
is to understand the ways by which one gets "from thought to thing"
[@petroski_invention_1996, 3-7]. Along with an exposition of a metaphor, each
of my chapters therefore also contains at least one literary "thought thing."
Each chapter enacts a deconstruction---a literal taking apart---of that
device.

The shifting affordances of digital text challenge some of our deep-seated
intuitions about literature. The word processor, operating system, and the
electronic book are some of the sites that stage the encounter between
literary theory and practice today. In *Plain Text* I introduce a method of
computational hermeneutics, which is a form of textual analysis strongly
influenced by materialist critique. Where "distant reading" and cultural
analytics perceive patterns across large-scale copora, computational
hermeneutics breaks textuality down into its minute constituent components. It
is at this scale that I find readers and writers becoming fundamentally
alienated from the immediate material contexts of knowledge production. Mine
is not however a post-human materialism of the kind that privileges an
object's point of view. On the contrary, the book aims to remove the aura of
fetishism that attaches itself to literary--computational artifacts and to
complex systems that mediate the textual encounter.

The reverse engineering of literary devices reveals that not all texts are
created equal. In print, traditional distinctions between form and content lie
flat. The printing press firmly embeds ink into paper, leaving no space
between type and page. Media-minded critics like Johanna Drucker, Katherine
Hayles, and Jerome McGann have urged literary scholars to re-evaluate
textuality in its media-specific contexts [@drucker_digital_2001;
@mcgann_radiant_2001; @hayles_print_2004]. Their work reminds us that the
flatness of digital text endures only in the guise of an illusion. Low-level,
operational intuitions governing textuality---ideas about form, content,
style, letter, and word---change profoundly as text shifts its confines from
paper to pixel. A substantial gap separates the visible text from the source
code that produces it. Forces of capital and control exploit that gap,
obscuring the workings of the device.[^ln-capital]

Changing material conditions of textual transmission push against familiar
literary critical ideas. Easy digital reproduction of print weakens the
material basis for authorship attribution, for example. That is not say that
the author is dead. Authors continue to live and to collect
royalties.[^ln-dead] The weakening of the authorship function merely makes
certain ways of talking about things like "authorial intent" and "fidelity to
the original" difficult to sustain. The emergence of writer collectives like
Wikipedia and writing machines like spam bots further erodes ideas of
authorial production based on individual human agency. It would be a mistake
to believe that the trope of autopoiesis---literature that writes itself or
discourse that speaks---could ever displace the myth of the
author.[^ln-varela] A discipline of close attention to the atomic particulars
of encoding, transmission, storage, and the decoding of text at the site if
its application to the human condition ultimately reclaims a measure of
subjective agency in authorship.  This may seem strange at first: to recover
the subject in the physical minutiae of the textual--technological encounter.
Yet the point of contact between human, text, and device is significant
precisely because it is here that the subject seems to disappear into the
machine and the machine appears in the guise artificially intelligent actor.

Extant models of literary transmission assume movement through passive and
immutable media. Paper offers the document of record, which, once archived,
does not change its contents. Analytic techniques like genetic criticism and
forensic reading make it possible to reconstruct if not "authorial intent,"
then at least a trace of the author's hand. In some contexts---think
manuscripts and folios---we may even ascribe properties like "fidelity" to
"original" works of art. When media are immutable, one imagines a sort of a
causal custody chain between works and their creators, which at some point
must have occupied the same contiguous space and time. Mechanical reproduction
of print introduces a range of devices that mediate between readers and
authors.  Distance, time, and mediation subsequently weaken facile notions of
authorial fidelity. At the very least, we understand that such intermediary
structures as editors, publishing houses, and printing presses inject an
element of noise into the channel of communication.

The transition between the Gutenberg press and "Project Gutenberg," an online
library containing thousands of texts, has brought with it further as yet
unexamined possibilities. Unlike pen and paper which come in direct contact
with each other during writing, the contact between keyboard and screen passes
through another complex chain of mediation. Writing, in that sense, in itself
becomes an experience both programmed and simulated. We do not write in the
conventional sense of inscribing marks into a static host. Rather, we are
shown superimposed images of inscription and media. At best, the composite
trope attains some measure of correspondence to the physical realities of
typing, editing, and saving files. In the worst case, the connection between
keyboards and screens suffers from intractable "man-in-the-middle" attacks, by
which a third parties maliciously alter the content of intended communication
[@needham_using_1978].

In *Plain Text*, I will argue that some of the higher--level political
afflictions of the contemporary public sphere---mass surveillance and online
censorship, for example---relate to our failure as readers and writers to come
to terms with the changing conditions of digital textuality. A society that
cares about the long-term preservation of complex discursive formations like
free speech, privacy, or online deliberation, would do well to take heed of
the textual building blocks at their foundation. The structure of discursive
formation---documents and narratives---has long been at the center of both
computer science and literary theory. Using primary sources from both
disciplines, *Plain Text* uncovers the shared history of literary machines,
bringing computation closer to its humanistic roots, and the humanities closer
to its computational realities.

*Plain Text* makes a historical case for the recovery of textual thought
latent in the machinery of contemporary computing. Just as philology cannot
survive without awareness of its computational present, the design of
computational platforms cannot advance without greater awareness of its
cultural contexts. Much is at stake in the material affordances of the
literary artifact. The political struggle for meaning-making, the very
opportunity to engage in the act of interpretation, begins and ends with the
materiality of the textual artifact.

In the West, it is easy to forget the blunt effectiveness of physical control.
Books that are burned or redacted cannot be read at all. Elsewhere, global
inequities of access to knowledge compel readers to print their own books and
build their own libraries. Witness the so-called "shadow libraries" of Eastern
Europe, the street book vendors of India and Pakistan, and the gray market
presses of Nigeria arising form the country's "book
famine."[@mahmood_copyright_2005; @okiy_photocopying_2005; @liang_piracy_2009;
@bodo_short_2014; @nkiko_book_2014]. More than mere piracy, such
*samizdat*-like practices engage in the proactive preservation of the literary
sphere [@tenen_book_2014]. Informal book exchange networks create reading
publics that own the means of textual production and dissemination. Under
duress, readers build homemade knowledge infrastructures: they duplicate,
distribute, catalog, and archive. By contrast, in wealthier economies such
infrastructures are commodified. Consequently, the material contexts of
meaning making are received passively, taken for granted, and disappear from
view. For many readers, technologies that support reading, writing, and
interpretation pass from tools to fetish. We hold tender feelings for our
mobiles and laptops, grasp for them in times of trouble and cradle them in our
laps. No longer comprehensible by the way of the pen or the printing press we
imbue them with magical powers. Thus we exist in the state of profound
alienation from the material conditions closet to our mental activity.

The future of reading and writing is inexorably intertwined with the
development of computer science and software engineering. Even if you are not
reading these words on a screen, my message has reached you through a long
chain of machine-mediated transformations: from the mechanical action of the
keyboard on which I am now typing, to the arrangement of electrons on magnetic
storage media, to the modulation of fiber-optic signal, to the shimmer of the
flowing liquid crystal display rendering the text. Computation occupies the
space between the keyboard and the screen, which in turn gives rise to
higher-order cultural institutions: from the architecture of social media
platforms to the formation of massive shared archives. The "cultural
techniques" that guide our use of such technologies are formative of the
society as a whole [@leroi-gourhan_gesture_1993, 83-84;
@siegert_cultural_2015]. Therefore, daily choices like choosing a text editor,
a filing system, or a social networking platform cannot be addressed in
shallow instrumental system-centric ideals. Complex computational systems
cannot give rise to ideals any more than financial markets can. Among the many
available visions of human--computer interaction I argue for our choosing one
that confirms to a humanist ethos, whatever the reader's politics.

Computational hermeneutics encourages "users" to become active thinkers,
tinkerers, and makers of technology. It treats "binary" and "digital"
environments as fluid literary systems, amenable to the construction and the
deconstruction of meaning. I further encourage those who may have considered
themselves mere "users" to apply the same critical acuity they employ in the
close reading of prose and poetry to the understanding of code and machine.
For text to render on the screen properly it must be "encoded" or "translated"
from machine-transmittable code into human-readable shape. Encoding
constitutes a primitive field of textual activity, at the crossroads of
computer science and the study of literature. Encoding matters because how
text is encoded, transmitted, and stored decides who gets to decode, to
receive, and to access.

The advent of simulated text necessitates a computational hermeneutics, which
enables unfettered access to text, code, platform, and infrastructure.  For
now, commands like *xxd*, *pcap*, *ssh*, and *traceroute* resemble arcane
incantations that elicit hidden, symbolic action. Those who wield them gain
the metaphorical power to "hop" across, to "sniff" packets, to "survey," to
"traverse," and to "flood" network topographies. Computational hermeneutics
empower the reader to resist hard-wired models of machine-bound
interpretation. Yet today, resistance remains within the purview of the few.
Plain text channels itinerant streams of data back into the tidal pools of
human agency and comprehension for all. There, code can become intelligible
for the very subjects whose loss the Foucault and Kittler lament.[^ln-lament]
Only in such encrypted tunnels and secure shells can anything like the digital
humanities or new media studies take root.

## 0.4 Theory and Practice

My approach to writing *Plain Text* stems from the desire to enact theory
capable of addressing the grim picture Friedrich Kittler paints at the end of
his influential monograph.[^ln-kittler2] Kittler was neither a technological
romantic nor a Luddite. I read the concluding chapters of *Gramophone, Film,
Typewriter* as a call to action. When Kittler writes that "media determine our
situation," he challenges his reader to choose between complicity and defiance
[@kittler_gramophone_1999, xxxix]. What can we do to counteract technological
determinism highlighted by Kittler? In what follows, I outline several
intellectual lineages that frame my approach to the problem, leading to a kind
of a materialist critique that is both pragmatic and experimental.

Critical theory, at its best, aims to see "the human bottom of non-human
things" [@horkheimer_critical_1982, 143]. As such, it is one of our most
powerful tools for analysis and resistance against technological
determinism.[^ln-determine] But as Max Horkheimer wrote, "the issue is not
simply the theory of emancipation; it is the practice of it as well"
[@horkheimer_critical_1982, 233]. Recently, scholars like Kathleen
Fitzpatrick, Tiziana Terranova, and Trebor Scholz have began to turn the tools
of critical theory towards the instrumental contexts of knowledge production
[@scholz_digital_2013; @fitzpatrick_planned_2011; @terranova_network_2004]. I
join them to argue that in treating the instruments of intellectual production
and consumption uncritically, all of us---readers and writers---accumulate an
ethical debt.

For example, it is one thing to theorize about the free movement of literary
tropes across cultures and continents, and quite another to have that theory
appear in print behind paywalls inaccessible to most global reading
publics.[^ln-sarab] Similarly, a theoretical distinction between form and
content, when instantiated in specific file formats like Microsoft Word
(`.docx`) or Adobe Reader (`.pdf`), establishes divisions of labor between
editors, proofreaders, book sellers, and offshore typesetting
firms.[^ln-sweatshop] One group trades content in the economy of prestige,
another formatting in the economy of survival, and yet another controls
distribution in the market economy, for profit.

Distinctions of labor will remain in place as long as the conversation about
ideas like "form" and "content" persists in the abstract. Materialist critique
cannot achieve its stated aims without purchase on the material world. My hope
is that by grounding theory in practice, *Plain Text* can begin to repay the
debt accrued by materialism divorced from matter. Complex systems that support
the life of the mind today seem to lie beyond understanding or agency. In
almost a decade of teaching critical computing in the humanities, I routinely
encountered otherwise informed people who nevertheless felt hopelessly at odds
with the tools of their everyday intellectual activity. I suspect that much of
the metaphysical angst directed against digital culture in general is really a
symptom of that basic alienation. Contemporary knowledge workers stare into
rectangular black boxes for a considerable part of their days, suspecting, in
the absence of other feedback, that their gaze is met in bad faith.

Connecting theories of meaning--making to the practices of meaning--making
offers a way out of the bad faith conundrum. Bad faith identifies a
misalignment between thought and action.[^ln-sartre] The solution to connect
"meaning" with "operational meaning" belongs to a species of pragmatism.
William James articulated that view concisely when he wrote that "reality is
seen to be grounded in a perfect jungle of concrete expediencies"
[@james_pragmatisms_1907]. For James and other pragmatists, truth could not
exist in the abstract alone. It entailed also causes and effects that operate
in the world.[^ln-pragma-truth] In his essay "Pragmatism's Conception of
Truth," James asked: "How will the truth be realized? What concrete difference
will its being true make in anyone's actual life? What experiences will be
different from those which would obtain if the belief were false?" Frank
Ramsey, the young British philosopher close to Ludwig Wittgenstein would later
write in a similar vein about meaning as "defined by reference to the
actions."[^ln-pragmatism]

For the pragmatist, truth-carrying propositions of the shape "X is Y" (as in,
"the author is dead" or "art is transcendent") beg the questions of "Where?,"
"When?," "For whom?," and "What's at stake in maintaining that?" Following the
pragmatic insight of James and Ramsey, I will proceed with the conviction that
abstract categories like "literature," "computation," and "text" cannot
possibly be reduced to a number of essential, structural features. Rather, to
borrow from Wittgenstein's *Philosophic Investigations*, categories denote a
set of related practices that may or may not share in any given familial
characteristic.[^ln-witt] In our case, imagine a tree diagram where the
tangled branches of computation and textuality intersect and diverge in
beautiful and yet arbitrary ways.

As a consequence of my commitment to a pragmatic materialism, *Plain Text*
shares in the experimental turn affecting a wide range of humanistic
disciplines. When beginning the project, I was initially inspired by the
writings of a little-known but influential ninetieth century French physician,
who was one of the first researchers to incorporate experimentation into
medical practice.[^ln-bernard] Writing against the tradition of "inductive
generalizers," Bernard believed that medicine could only advance by "direct
and rigorous application of reasoning to the facts furnished us by observation
and experiment." "We cannot separate the two things," he wrote, "head and
hand." He went on to write that "the science of life is a superb and
dazzlingly lighted hall which may be reached only by passing through a long
and ghastly kitchen." "We shall reach really fruitful and luminous
generalizations about vital phenomena only in so far as we ourselves
experiment and, in hospitals, amphitheaters, or laboratories stir the fetid or
throbbing ground of life" [@bernard_introduction_1957, 3-15]. Today, the
lighted halls of literary and media theory lead to the scholar's workshop.

In an approach to "doing" theory, *Plain Text* joins the experimental turn
steering the academy toward critical practice, especially in fields
long-dominated by purely speculative reflection. The experimental turn
represents a generation's dissatisfaction with "armchair" philosophizing.
Recall the burning armchair, the symbol for the experimental philosophy
movement. Joshua Knobe and Shaun Nichols, some of the early proponents of the
movement, explain that "many of the deepest questions of philosophy can only
be properly addressed by immersing oneself in the messy, contingent, highly
variable truths about how human beings really are" [@knobe_experimental_2008,
3]. The emergence of spaces where research in the humanities is done
exemplifies the same trend.  In naming the locations of their practice
"laboratories,"  "studios," and "workshops," humanists reach for new metaphors
of labor. These metaphors aim to reorganize the relationship between body,
space, idea, artifact, instrument, and inscription.

As an example of what I have been calling here the "experimental turn" in the
field of early modern history consider the preface to a recent volume on *Ways
of Making and Knowing*, edited by Pamela Smith, Amy Meyers, and Harold Cook.
The editors write that the "history of science is not a history of concepts, or
at least not that alone, but a history of the making and using of objects to
understand the world" [@smith_ways_2014, 12]. Smith translates that insight in
the laboratory, where, together with her students, she bakes bread and smelts
iron to recreate long--lost artisanal techniques. For those who experiment,
"book knowledge" and "artifactual knowledge" relate in practice.

Artifactual knowledge---from typesetting software to e-book readers and word
processors---shapes our everyday encounter with literature. Such technologies
should not be taken as value-neutral conduits of information. I follow Lewis
Mumford and Langdon Winner to argue that technology affects the exercise of
textual politics in subtle and profound ways [@mumford_authoritarian_1964;
@winner_artifacts_1980]. Artifacts cannot hold beliefs about politics.
Political power is rather exercised through them. Stairs do not discriminate
against the mobility impaired, for example. The failure to enforce
accessibility through specific legal and architectural choices does.
Typesetting software, e-book readers, and word processors similarly incarnate
implicit communication models: ideas about deliberation, ethics of labor,
discursive values, and views about "natural" human aptitude for
interpretation. In this way, contemporary documents structure the literary
encounter according to license, location, or physical ability.

To what extent does the book in front of you permit or enable access?
Whatever the answer, a function of understanding the text must include the
explication of its physical affordances. Experimentalism enables the critic to
"lay bare" the device. A literary scholar's version of baking bread and
smelting iron is to make literal the figure suggested by the idea of media
"archeology" on the level of the mechanism. In *Plain Text* we will dig
through, unearth, and excavate textual machines. In practicing archeology I
contend that cardinal literary-theoretical concepts---such as word, text,
narrative, discourse, author, story, book, archive---are thoroughly enmeshed
in the underlying physical substratum of paper and pixel. It follows that any
attempt to articulate the idea cannot attain its full expressive potential
without a thick description of its base particulates.

Luckily for us, reading and writing are not esoteric activities. They are
readily available to phenomenological and physical introspection. I will
therefore occasionally encourage readers to encounter the immediate contexts
of their reading anew: to put down the book or to lean away from the screen
and to look at these textual artifacts with strange eyes. In this movement of
the body, I want to disrupt the mind's habituated intuitions, pitting them
against knowledge "at hand" and fingertip knowledge: as when ruffling through
the pages or typing at a keyboard. To what extent is electronic textually
ephemeral, for example? The pragmatic answer lies not in universal
propositions, but in technological affordances attached to specific reading
devices. What can a reader do with this text, here and now? Where is it
stored? Are readers able to copy and paste? Do they have legal permissions to
quote at length, to perform publicly, or to otherwise trans-mediate? Will the
text disappear when the reader closes the book's cover?

## 0.5 Plan of the Present Work

The tangled pathways of inscription winding its way through the device exist
in relation to specific communities of computational practice. A researcher
cannot therefore expect to discover a single theoretical framework that can
capture the complexity of the simulated textuality in motion. The inscription
goes by one name in one part of the system and by another elsewhere. What
counts for "code" and "poetry" in one domain, like computer science, may not
account for the same in another domain, like creative writing. An engineer's
use of the words "code" and "poetry" differs from that of a poet's. The
changing contexts evoke the corresponding shift in operational definitions.
Consequently, in *Plain Text*, I write neither a totalizing history of modern
computing nor a survey of literary theory. Rather, the argument progresses
from the action of the alphanumerical keyboard switch, through copper and
silicon, to liquid crystal and the floating gate, and on towards the reader
and the community. It is but a single possible pass through a cavernous black
box.  Each chapter reflects a waypoint along the journey.

The passage from keystroke to pixel runs a thread through the book. In the
chapters to follow, the electronic literary artifact---the thing next to my
laptop on my desk---will come fully into view as a computational device, a
kind of a universal metaphor engendering ubiquitous simulation. At the core of
the book's **first chapter** lies a single thought experiment about reading
and writing machines. Articulated first in the philosophy of Ludwig
Wittgenstein, the thought experiment ultimately led to the development of the
universal Turing machine. An important idea in the history modern computing,
the Turing machine embodies many of the paradoxes that continue to shape our
encounter with digital text. At once a machine and an algorithm, the Turing
machine blurs the boundaries between software and hardware, code and content.

Once we see the book as a kind of a Turing machine, we are able to confront
the particularity of the simulated sign. A number of significant consequences
follow. The **second chapter** describes the condition by which the simulated
sign splits to reside in at least two distinct locations, screen and storage
medium. The "illusion of the simulated text" will thereafter identify an
incongruity between the two sites of reading. Erasing an inscription on the
screen, for example, may not elicit the corresponding action on the disk.
Using archival materials from the history of telegraphy in the late nineteenth
and early twentieth centuries, I chart the gradual fracture and the ultimate
disappearance of the inscription. Early computers composited human-readable
text and machine instruction at the surface of the same storage media like
punch cards and ticker tape. Although difficult to read, these forms of
inscription were readily visible and therefore amenable to analysis. The
advent of magnetic storage forced the composite inscription into an opaque
medium.  Unable to perceive magnetic polarities without the aid of a machine,
readers often manipulated text blindly. In this way a typist would type
several sentences without seeing the printed output. The chapter identifies a
milestone in the history of human textuality: the moment at which the
inscription passes from view.

The **third chapter** charts the emergence of the screen which restored a
measure of visibility lost to magnetic inscription, with one major
side-effect. The fidelity of the visible word could no longer be guaranteed.
What the screen showed and what was stored on tape carried only a contingent
correlation. Even today, many readers struggle to answer a simple question:
Where does the text physically reside? Our inability to precisely locate the
site of the inscription belies its seemingly ephemeral quality. Words on the
screen appear conspicuous, but do not last once the device is powered off.
Device reading happens on screens that refresh themselves at a rate of around
60 cycles per second (Hertz). Screen textuality is therefore by definition
ephemeral. It is technically an animation. It moves even as it appears to
stand still. The third chapter unfolds at the site of the projection.  Modern
screens modulate the electrical signal to push light through a liquid crystal
medium. Here, I bring the mechanics of the medium to bear on a theoretical
conversation about digital aesthetics. Works by the philosophers Henri
Bergson, Jakob von Uexküll, and John Goodman help construct a phenomenology of
screen-based digital perception. The digital emerges ultimately not as
property of the medium, but as structure imposed onto matter from without. In
the extreme, that means that a censored electronic text is a perfectly analog
artifact. Conversely, texts in print are already "born digital," in the sense
that literary works like Shakespeare's *Hamlet* are amenable to "reliable
processes of copying and preservation" [@haugeland_analog_1981, 213-225]. But
such attributes are again neither universal nor essential to the medium. "The
reliability and preservation of textual copies" may mean one thing to a
literary scholar, another to a software engineer or a legal professional, and
yet something entirely different to a librarian.

The **fourth chapter** moves from the screen to machine internals. Somewhere
between sites of storage and projection, the computed sign underwent a series
of transformations. These transformations usually pass under the rubric of
"formatting." The development of the formatting layer commenced with several
"control characters" included in the machine alphabets that followed Morse
code. Initially, these characters were limited in function to actions like
"carriage return" or "stop transmission." But with time, the formatting layer
encompassed all manner of machine instruction. Thus a manufacturer's ability
to censor or to surveil electronic books is contained within the formatting
layer. Formatting is therefore crucial to our ability to interpret digital
text. Two rich histories collide on the pages of the fourth chapter: one, the
material history of formatting as a concept in computer science and two, the
intellectual history of form in literary theory.

The **fifth chapter** looks to the site of storage to find our media "homes"
that house the vast archives of our private media collections. The chapter
begins with a close reading of Beckett's *Krapp's Last Tape*. The title
character makes yearly audio recordings of himself, only to revisit them and
to enter into a sort of dialog with his own voice from the past. I posit this
archival encounter as Krapp's "media being" and suggest that such encounters
are commonplace. Writers and book collectors regularly deposit "snapshots" of
their consciousness into files, bookshelves, and folders. Jean-Paul Sartre's
idea of an "appointment with oneself" helps us see this external construction
of files, folders, and library furnishings as cognitive extension, in need of
delicate pruning and arrangement. In this light, I show that documents exist
not as completed works, but as "vectors" that mutate and move through time and
space.[^ln-wark] I ask: What is being externalized, communicated, and preserved?
And answer: It is not simply a message, but the subject itself. Finally, a
close reading of the "home" folder opens the way to the concluding chapter of
the book.

The **last chapter** returns to the question of ephemerality. Paradoxically,
despite its evanescent qualities, simulated text endures.  Embedded into solid
state drives and magnetic disks, inscription attains the quality of what Wendy
Hui Kyong Chun calls the "enduring ephemeral" [@chun_enduring_2008, 148].
Precisely because simulated text adheres lightly to its medium, it has a
tendency to multiply and to spread widely. Like the fine particles of glitter,
it is difficult to contain. The simulated text falls apart into bits and
pixels that replicate and tumble about the system.  In this way a deleted
email may still persist on the drive, be replicated to remote servers, and
proliferate across datasets used for machine learning, advertising, and
national intelligence. Consequently the structural insight from the first five
chapters can ultimately yield insight into the social implications of the
contemporary textual condition. In reading the letter of the law related to
the Digital Millennium Copyright act, I argue that the facility of copying
digital artifacts like books, code, or even object schematics for 3-D
printing, necessitates a new understanding of digital production. In obscuring
the material realities of the sign at the site of the inscription, the surface
simulation conceals an author's relationship to the products of his or her
labor. And despite the easy appearances, textual diffusion carries with it
palpable environmental costs.  The book ends with a stark image illustrating
the contrast between screen surface and the underlying bit structure. To
produce the image, I inject malicious code into an Adobe Acrobat file
(`.pdf`). The deformed text threatens to damage the literary device. A thick
description of the book, now as weapon and instrument, brings legibility to
the fore of poetics. The design and usage of literary devices must itself
become critical practice which can, in complement to critical theory, actively
engineer for human agency. A discussion of technological dissent through the
humanisms of Karl Marx, Hannah Arendt and Franz Fanon concludes the volume.

## 0.6 Scale

In the process of taking things and texts apart, I am under no illusion about
the possibility of finding perfect understanding. Technological systems that
give rise to complex social phenomena, from market trading to literary canon
formation, defy holistic comprehension. Writing in 1977, Langdon Winner warned
the scholar of science and technology to stay away from those "mystics" and
"crackpots" who offer vision of a new and totalizing synthesis. Quoting the
French poet and philosopher Paul Valéry, he wrote that "our means of
investigation and action have far outstripped our means of representation and
understanding" [@valery_collected_1962, 69; @winner_autonomous_1978, 290].
"Citizens of the modern age in this respect are less fortunate than children,"
he wrote. Children graduate from ignorance to understanding. But the adults
"never escape a fundamental bewilderment in the face of the complex world that
their senses report." They are not able to organize the vast body of
information that governs their lives into a "sensible whole." The best we can
do, he concludes, "is to master a few things in the immediate environment"
[@winner_autonomous_1978, 286].

Langdon Winner could scarcely anticipate the exponential growth in complexity
of computing systems that surround us today. The Unix operating system
announced on the pages of the Proceedings of the fourth ACM symposium on
Operating systems in 1972 comprised roughly 10,000 lines of code
[@ritchie_unix_1973; @ritchie_unix_1974; @mccandless_million_2015]. To give
the reader an idea of scale, consider the aggregate length of Shakespeare's
*Hamlet*, *Richard III*, and *Macbeth* put together. By contrast, *Google*
internet services comprise more than 2 billion lines of code, or roughly 700
million *Hamlets*.[^ln-twobil]

[^ln-twobil]: Code metrics from @mccandless_million_2015.

Similarly, the growth of the literary sphere has mirrored the rapid inflation
of the code base. On an average day in 2008, at home, an average American read
around 100,500 words a day [@bohn_how_2009]. At 250 words per page, that
amounts to reading around 402 printed pages or roughly a novel like Emily
Bronte's *Wuthering Heights* per day. Between the years of 1980 and 2008, the
consumption of information in bytes---a measure that would obviously privilege
storage-heavy content like sound and video---grew at a modest 5.4 percent per
year. Reading, in decline until the advent of the internet, has tripled in the
same period.  Reading in print accounted for 26 percent of verbal information
consumed in 1960. That number fell to 9 percent in 2008, but the consumption
of words digitally increased to 27 percent of total consumption, which means
that reading has increased its share of the overall household attention span
[@hilbert_worlds_2011; @hilbert_info_2012]. The first decade of the
twenty-first century saw a 20 percent increase in library visitation
[@u.s._institute_of_museum_and_library_services_public_2010]. By a
conservative estimate, the number of scientific publications grows at about
4.7 percent per year, which means that the amount of published research
roughly doubles every 15 years or so (and the numbers are much, much higher in
some fields) [@archambault_welcome_2005; @crespi_empirical_2008;
@larsen_rate_2010]. The number of books published in the United States almost
tripled from 2005 to 2009 and 37,450 poetry and drama titles were published in
the United States between 1993 and 2006 [@dworkin_seja_2008;
@bowker_u.s._2009]. All of the metrics point to an unprecedented growth of the
literary sphere. The expanding codebase further contributes to the expansion
of the textbase. It is likely that automated discourse generators like social
media bots, spam and automatic news generators will at some point soon outpace
humans, if they have not done so already.[^ln-spam]

Pointing to the sheer impossibility of processing the amount of published
information in nineteenth let alone the twenty-first century, the American
poet and critic Graig Dworkin urged his peers to abandon the dream of
comprehensive knowledge or the literary sphere altogether
[@dworkin_seja_2008]. Almost nothing could be said about it in aggregate, at
least not by conventional means. Unassisted strategies of interpretation like
philology or close reading were built in an environment of received literary
canons, naturally accessible to the human intellect. For the duration of the
"Gutenberg galaxy," the age of print, a well-educated person might have been
expected to internalize some several hundred or perhaps thousands of major
texts constituting the canon. Literary criticism was honed to operate on that
naturalized scale.

The expansion of the textual field has radically increased the cognitive
demands of literary engagement. The various practices of distant reading arise
from the condition in which canons are no longer accessible, in their
entirety, to the unaided (natural) human intellect. These include distant
reading and macroanalysis in literary studies [@jockers_macroanalysis_2013;
@moretti_distant_2013], culturomics in economy [@aiden_uncharted:_2014],
e-discovery in law [@scheindlin_electronic_2004; @scheindlin_electronic_2009],
automatic essay evaluation in education [@shermis_handbook_2013], and medical
informatics in medicine [@shortliffe_biomedical_2013], among others.  A shared
toolkit lies at the foundations of these nascent disciplines.  It includes
tools like statistical natural language processing [@manning_foundations_1999;
@jurafsky_speech_2008], automatic summarization [@radev_centroid-based_2004;
@nenkova_pyramid_2007], machine learning [@rasmussen_gaussian_2006;
@flach_machine_2012], network analysis [@opsahl_node_2010;
@szell_measuring_2010; @takhteyev_geography_2012], and topic modeling
[@wallach_topic_2006; @blei_probabilistic_2012].

The engines of search and recommendation rise to answer the needs for more
efficient filtration of data. Where a librarian or a book reviewer could sift
through thousands of volumes to distill the ones worth reading or purchasing,
the algorithmically-augmented search and discovery tool indexes billions.
Consider the task of finding an unknown (to me) factoid online, about
philosophy in the times of Andalusian Spain, for example. Where in the past I
might have started with a library subject catalog, today I construct a search
query, using resources that I believe contain a reasonably complete archive of
texts on the subject. The search engine in effect replace (or rather
complement) centuries-long processes of canon-formation. A near-instantaneous
list of query results now becomes my ephemeral, but nevertheless
authoritative, collection of literature relevant to any given topic of
interest. In this way the search engine supplants the catalog. And the
recommendation engine, of the kind commonly used by online book stores,
supplants the function of the book review.[^ln-bookreview] But as Frank
Pasquale has argued convincingly in his recent book, the problem with such
complex algorithmically-powered tools is that unlike public library catalogs,
proprietary search and recommendation engines are not necessarily open to
interpretation [@pasquale_black_2015]. They are under no obligation to reveal
their biases or motivations. The search results for "Andalusian Spain" may
well suppress elements of Muslim history or privilege a historical narrative
that privileges an English-centric view of the world.

[^ln-bookreview]: I write "supplants the function" because I do not mean to
imply that the book review disappears or becomes less vital. On massive market
platforms like Amazon Books, the book review passes from the domain of an
expert to the domain of the lay reader. Book reviews thus proliferate. Their
function changes from universal aesthetic judgement to instrumental reasoning.
In other words, we now find the book first then read the review. The review
ceases to serve as a tool for discovery--a function now addressed by the
search and recommendation engines.

Each text in the returned search results list requires the instrumentation of
close, analytical interpretation. However, the same discipline of critical and
reflective deliberation exercised on the level of an individual text needs to
also be exercised on the level of procedurally generated search engine
results: Where to search? Using what engine? How to construct the query? What
are the implicit biases of the system?  Whatever ideals motivate close reading
between "text" and "work" surely must drive the process on the level of
dynamic corpus composition. To make them available for scrutiny the
"macroscopes" themselves need to be taken apart. Thus where distant reading
perceives patterns across large-scale corpora, computational
hermeneutics---microanalysis---breaks literary systems down to their minute
constituent components.

![Micro, macro, and close reading.](images/micro.png)

Where distant reading and macroanalysis constitute the study of mediation
between readers and text aggregates (like canons, corpora, collections,
libraries, archives, and database) microanalysis examines mediation at the
level of physical minutiae otherwise not readily observed in cursory
exploration. The instruments of microanalysis may coincide with computational
tools, designed to find hidden patterns lurking above or beneath a given
document.[^ln-iarkho] The micro-instrumentation might also include a
screwdriver, a binding needle, or a soldering iron: sharp tools that help to
pry open and to scrutinize otherwise magical textual black boxes. My
methodology thus lies in the dialectical tension between making known and
making strange. Each chapter reveals the material realities of symbolic action
like "closing windows," "bookmarking a page," and "dragging and dropping
files." Where the metaphor is made strange, the mechanics of the device comes
into view. The metaphor and the machine help organize the book and each of its
chapters.

*Plain Text* does not require the reader become a computer scientist or a
software engineer to reflect on the contexts of knowledge production
critically. Rather, I ask only for the willingness to leave inhabited comfort.
The reader need only bring the tools of critical reflection available to their
discipline. More technically inclined readers will thus be challenged to
scrutinize their practice in its wider historical context. The chapter on form
and formatting, for example, ranges in material from Hegelian idealism to the
birth of the Document Object Model. The rhetorical strategy purposefully
crosses the wires of parallel intellectual traditions, that although speaking
different vocabularies, ultimately concern themselves with the same area of
human experience (the ideal shape of a document, in this example). The tension
between languages and rhetorical strategies makes known, as Winner wrote, only
"a few things in the immediate environment." Yet, I believe that complexity
cannot be addressed in any other way. My ultimate goal is not to reach the
total understanding of the system, but rather to create localized pockets of
clarity from which further critique and exploration can advance.

To understand something small about the physical realities of "turning"
virtual "pages" is to create a space capable of supporting agency, desire, or
dissent.[^ln-dissent] The digitally displaced hold on to the discomfort of
the encounter with the machine. Estrangement, always at the heart of immigrant
or queer poetics, reconciles without seeking wholeness or integration. I
dedicate this book then to queers and immigrants, literal and
figurative---spatial, literary, technological---to those being displaced
unwillingly, to those exiled within and without, to those who understand the
need for self-displacement, to those who transgress purposefully, and to those
willing to trespass.

<!--- NOTES  --->
<!--- NOTES  --->
<!--- NOTES  --->

[^ln-bernard]: On Bernard see @petit_claude_1987 and @sattar_aesthetics_2013.

[^ln-brains]: For the first view see @putnam_minds_1960 and
@fodor_language_1975. For the second view see @deutsch_quantum_1985 and
@dyson_turings_2012.

[^ln-capital]: Scholars like Alexander Galloway, David Golumbia, Bernard
Harcourt have advanced critique along similar lines. See
@galloway_protocol_2006, @golumbia_cultural_2009, and @harcourt_exposed:_2015.

[^ln-cog]: See for example @gibbs_categorization_1992; @blasko_effects_1993;
@gibbs_poetics_1994; @neal_role_1997, 441-463; @gentner_alignment_1997.

[^ln-dead]: See @barthes_death_1977; @foucault_what_1980; @nesbit_what_1987.

[^ln-determine]: I mean "determinism" as both (a) a belief in the intrinsic
agency of complex systems and (b) a practice of diminishing the scope of human
freedoms by technological means.

[^ln-digitalliteracy]: See for example @postman_technopoly:_1992;
@negroponte_being_1995; @davidson_now_2011.

[^ln-dissent]: See @harcourt_exposed:_2015, 251-80.

[^ln-dmca]: See @ku_critique_2004; @ginsburg_legal_2005; and @fry_circumventing_2009.

[^ln-egturner]: Mark Turner, whose work builds on Lakoff and Johnson is a
strong proponent of such an approach. See @turner_death_1987 or
@turner_language_1992.

[^ln-exposed]: For an extended exposition of this dynamic see
@harcourt_exposed:_2015.

[^ln-exposed2]: Again, a point that is given its full treatment in
@harcourt_exposed:_2015.

[^ln-flusser]: The work of @flusser_does_2011 has been similarly influential.

[^ln-hegel]: I discuss the topic at length in Chapter 3.

[^ln-iarkho]: I borrow the term "microanalysis" from the largely forgotten in
the West Russian literary scholar and member of the Moscow Linguistic Circle,
Boris Iarkho. In his *Methodologies of Exact Literary Study* (circa 1935-6) he
writes: "I understand 'atomism' as a sort of an ideal aspiration, as an
orientation toward the liminally small. But under no circumstances do I
advocate working with hypothetical quantities, like molecules, atoms,
positrons, and so on, which are located beyond the limits of perception. That
this applied mythology gave us such splendid results in chemistry, should not
conceal its true nature. Tomorrow, all such explanations of visible through
the invisible could give way to other hypotheses, as was the case with their
no less fertile predecessors (elemental spirits, phlogiston, and light ether).
But the cell, the nucleus, and the chromosome endure as lasting
accomplishments of microanalysis. I suggest to move as far as a microscope can
reach, and no further" [@iarkho_metodologia_2006, 363-364]. For Iarkho, the
most quantitatively inclined of the Russian Formalists, microanalysis involved
systematic application of statistical methods to the study of literature.

[^ln-kittler2]: *Gramophone, Film, Typewriter* ends as follows: "And while
professors are still reluctantly trading in their typewriters for word
processors, the NSA is preparing for the future: from nursery school
mathematics, which continues to be fully sufficient for books, to
charge-coupled devices, surface-wave filters, digital signal processors
including the four basic forms of computation. Trenches, flashes of lightning,
stars---storage, transmission, *the laying of cables*
[@kittler_gramophone_1999, 263].

[^ln-lacan]: The evanescent absence of life that Lacan mentions as "the sign
about which Robinson Crusoe would make no mistake" [@lacan_seminar_1997, 167].

[^ln-lament]: Or perhaps celebrate, depending on your understanding of their
post-humanism. For more a more extended discussion on Kittler, Foucault, and
post-humanism see for example @winthrop-young_silicon_2000; @wolfe_what_2010,
104-128; @siegert_cultural_2013.

[^ln-pragmatism]: @ramsey_foundations_2013, 155. The intellectual legacy of
pragmatism is wide-ranging and diffuse. It is perhaps most pronounced in the
teacher colleges, where James and Dewey are still read widely, which could
explain the ascendancy of such pedagogical terms as "situated
cognition"[@brown_situated_1988, @lave_situated_1991] and "experiential
learning"[@kolb_hegel_1981]: both terms denoting some sense of necessary
synthesis between of knowing and doing. In the field of linguistics,
philosophy of language, and communication studies, pragmatics are
well-encapsulated by the "language-as-action tradition," which harkens back to
the Oxford language philosophers like J.L. Austin, Paul Grice, and John Searle
[@trueswell_approaches_2005]. Austin's *How to Do Things with Words,* is
perhaps the paradigmatic formulation of the idea that words don't just mean
things, but that they enact change in the world.

[^ln-pragma-truth]: For a more thorough discussion on the topic see
@seigfried_william_1990, @pihlstrom_structuring_1996, and @putnam_jamess_1997.

[^ln-sarab]: See also @english_economy_2008; @brouillette_wither_2015 and
@brouillette_unesco_2015.

[^ln-sartre]: Sartre would write "transcendence" and "facticity." See
@sartre_being_1993, 86-119.

[^ln-spam]: For example, in 2004 researchers estimated that spam mail makes up
40%-80% of the total email volume @cournane_analysis_2004.

[^ln-sweatshop]: See @freeman_high_2000 and @patel_working_2010.

[^ln-translation]: Translations are mine unless source cited explicitly in
English.

[^ln-uni]: The Unicode Consortium. *The Unicode Standard: Worldwide Character
Encoding*, Version 1.0, Volume 1. Reading, Mass.: Addison-Wesley, 1990.

[^ln-varela]: See for example @varela_autopoiesis_1974; @barthes_rustle_1989,
5; @nuttall_new_2007, 6-25.

[^ln-wark]: I use the term "vector" in somewhat more limited sense than it
appears in the idea of "vectoralist class" coined by McKenzie Wark. By
"vectoralist class" Wark means something like the group of interests that
control the distribution of information. See for example
@wark_information_2006. In understanding the document as a vector I posit it
as a three-dimensional rather than a two-dimensional object, with the extra
dimension extending in time. Similar to Jerome McGann's idea of the "editorial
horizon." See @mcgann_textual_1991.

[^ln-winner]: See for example: "Writers concerned with with problems of
technology-out-of-control have frequently echoed Hobbes in suggesting that
such an artifact---the Leviathan of interconnected technical systems---has a
soul of its own [...] A ghost appears in the network.  Unanticipated aspects
of technological structure endow the creation with an anticipated *telos*"
[@winner_autonomous_1978, 280].

[^ln-witt]: @wittgenstein_philosophical_2001, 67-77. For more on the
connection between Wittgenstein and James see @goodman_james_2004.
