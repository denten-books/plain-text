# Simulated Text: an Introduction

As I write these words, a ceiling-mounted smoke detector in my kitchen emits a
loud noise every three minutes or so. And at every 15 minutes, a pleasant
female voice announces also "low battery." These precautions are stipulated by
US National Fire Alarm Code 72-108 11.6.6 (2013). The clause requiring a
"distinct audible signal before the battery is incapable of operating" is not
only required by law, it is *encoded into the device*. The device internalizes
legislation in its programming. Thus we obtain the condition where the two
meanings of code---as governance and machine instruction---coincide. Code
equals code.

I am not at home. But I know the alarm is happening because I receive
notifications of it on my phone---another small computational device, next to
my laptop. This mobile appliance also contains most of my book library. I pick
it up to read a book. But what I mean by "reading a book" obscures a metaphor
for series of odd actions. The "book" is a small, thin black rectangle: three
inches wide, five inches tall, and barely a few millimeters thick. A slab of
polished glass covers the front of the device, where the tiny eyes of a camera
and a light sensor also protrude. At the back, made of a smooth soft plastic,
we find another, larger camera. At the foot of the device a grid of small
perforations indicate breathing room for a speaker and several microphones. To
"open" the book I touch the glass. The machine recognizes my fingerprint
almost instantly. I then tap and poke at the surface until I find a small
image that represents both my library and the book store, where I can
"purchase" "books." To "buy a book," I agree to a limited licence, granting me
access to data which the software then assembles into on-screen representation
of books. I tap again to begin reading. The screen dims to match room ambiance
as words fill the screen. One of the passages on the first page appears
underlined: a number of other readers in my social circle must have found the
passage interesting. My finger slides along the glass surface to turn the
page. The device emits a muffled rustle, to remind me of the underlying
analogy. The image curls ever so slightly in a way that resembles a printed
page as another "page" slides into view. My tiny library metaphor contains
over a thousand such book metaphors.

Despite the appearances, the electronic metaphor-making device next to my
computer has more in common with the smoke detector than it does with several
paper volumes also scattered across my desk. Both devices comprise printed
circuit boards, capacitors, silicone chips, and resistors. Both draw electric
current. Both require firmware updates and are governed by codes: political
and computational. The smoke alarm and the mobile phone connect to the network
and both communicate with remote data centers and with each other. And yet, I
continue to read electronic books as if they were familiar, immutable, and
passive objects: just books. I think of them as intimate artifacts---friends
even---wholly known to me, comforting, and warm. The electronic book is much
less and much more than those things. Besides books, it keeps my memories,
pictures, words, sounds, and thoughts. It records my reading, sleeping, and
consumption habits. It tries to sell me things. It comes with a manual and
terms of service. It is my confidant, my dealer, my spy.

## 0.1 Simulated text

The aim of *Plain Text* is to dispel a pervasive illusion. The electronic book
is but a compelling metaphor that belies material realities of reading and
writing, transformed by the advent of computation. The device on my desk is
not a book, but a simulation of a book. And everything associated with
"reading" such a metaphor must in itself be understood as a simulation. What
does it mean to "study" a book metaphor? And what kind of a metaphor is it?
What is being compared to what? How did it come into being and how does it
affect practices of literary interpretation? In *Plain Text* I attempt to come
to terms with the conditions of simulated textuality.

To do that, *Plain Text* enacts a reconciliation of vocabularies. It is a
response to a particular situation of a literary scholar encountering the
field of software engineering. For a long stretch of my professional life,
these two areas of activity remained separate. I worked at one and I studied
the other. At the time, I simply did not think that code had anything to do
with poetry. The idea for the book came to me in a moment of realization after
I was asked one of those naive but fundamental questions of the kind that can
set research in motion down a long and winding path. A childhood friend who
loves books asked about the difference between text in print and text on the
screen. It was in that struggle to articulate difference that I realized that
some of my deepest assumptions about literature relied on the centuries-long
stability of print media. Despite my professional experience as a programmer,
I could not readily explain the mechanisms by which keystrokes turned into
pixels, pixels into letters, and letters into words. I could recount technical
detail on some level, but my knowledge also had huge gaps.  It did not amount
to a coherent story. I was, despite my best efforts, surrounded by magical
lanterns that cast shadows of code and poetry.

Initially, at the point of contact, the two selves spoke different languages.
It was and continues to be a disconcerting process by which things dear and
familiar to me, in both worlds, grew strange and unfamiliar, showing
themselves to be sometimes less than and sometimes more than I comfortably
expected. Nothing could be assumed from the start. Field specific jargon, down
to naive foundations, had to be examined for hidden assumptions that prevented
congruence. With time, I saw that code and poetry have much to do with one
another.

The Czech-born philosopher Vil√©m Flusser understood the condition of unease
that comes with emigration, both physical and mental, as a type of information
processing. Contemporary technology offers us an irresistible compromise, by
which we trade critical understanding for comfort. Habit covers the various
homes we make for ourselves in the world "like a fluffy cotton wool blanket,"
Flusser wrote---"it smoothes the sharp edges of all phenomena that it covers
so that I not longer bump against them, but I am able to make use of them
blindly." "When I sit at my desk," he went on to write, "I don't see the
papers and the books that are lying all about because I'm used to them"
[@flusser_freedom_2003, 13 and 82]. Familiar objects pass no information to
their users, according the Flusser. Like water for fish, they pass into the
background of experience. This is, we might say, the function of media: to
ultimately disappear into the background, not to produce meaning but to cease
production, to become a stage for meaning-making, and like the stage to
disappear from view.

By contrast, the condition of exile allows the displaced to once again
transform habituated media into meaningful information. In exile, "everything
is unusual," he wrote [@flusser_does_2011, 81]. The migrant experiences the
world as an ex-perience [*er-fahrung*], or literally a driving out. Discovery,
he concluded can "begin as soon as the blanket is pulled away," where the
familiar objects can pass into view again [@flusser_does_2011, 86-7].

Our challenge today is to uproot ourselves from the comfort that rapidly
descends on the dwellings of our intellectual life. Known quantities like
papers and books have long disappeared into the background. Mistaken for
neutral conduits of information, their electronic counterparts acquire a sense
of agency of their own. Devices that "watch," "hear," "see," and "think" give
rise to object phenomenology and the internet of things. Such seemingly smart
objects threaten to enter the network as independent agents. Marx's "table
that evolves grotesque ideas out of its wooden brain" can now be re-branded
into Microsoft *Surface* and PixelSense, product names form the life cycle of
an actual "smart" table [@marx_captial_1906, 82; @wigdor_designing_2009].

By mistaking things for animate actors, we ourselves have become enmeshed in a
system of digital production that commodifies human experience. Objects that
surround us collect our reading habits, social interactions, and intimate
conversations.[^ln-exposed] But the actual living agents that benefit from the
trade in personal information are neither cyborgs nor post-human assemblages.
The bargain that trades critical understanding for comfort benefits specific
interests, like multinational corporations and government intelligence
agencies. The rhetoric around smart objects shifts our attention from the
seats of power to things dumb, powerless, and indifferent to our
protestations.

The internal exile that we must undergo for the "smart" book and the desk to
come into view again cannot compare in difficulty to the experience of
physical displacement that follows natural disaster, war, poverty, and
political instability. Yet, our systematic reluctance to take on even those
small intellectual discomforts that could lead to acts of localized dissent
and disobedience---to write using free software or to publish in open-source
journals for example---cannot be said to exist apart from the complex systems
that perpetuate violence and inequity globally. The emotional affirmation that
accompanies exuberant social networking brings with it the governing
structures used widely in the name of law enforcement and national security.
Comfort and security in fact comprise part and parcel of the same
ill-conceived bargain that leads to critical disempowerment. But where it is
difficult to imagine or to enact strategies of digital disobedience on the
pandemic level, we can begin to address them through the numerous and
seemingly mundane series of micro transactions that ultimately comprise the
material foundations of intellectual life.

To pick up an electronic book and to take it apart may be against the law in
some jurisdictions.[^ln-dmca] However, given the extent to which emerging
thought-things like electronic books and personal communication devices
participate actively in the production of meaning, we can no longer employ
strategies of interpretation at the stratum of ideology alone. Close reading,
critical theory, and literary analysis must reach down to the silicon bedrock
that stages the very act of interpretation. Literary theory, a discipline
fundamentally engaged in the exegesis of all figurative tropes, is therefore
crucial to understanding new computational environments, which envelop
intellectual life through metaphoric substitution. I begin where Bernard
Harcourt's recent book on digital disobedience ends: with the possibility of
localized dissent, limited for now to the immediate physical contexts of
reading, writing, and finding knowledge. Following theorists of the metaphor
like Flusser and Viktor Shklovsky, I propose to proceed through systematic
revealment and estrangement of metaphors that have grown so transparent as to
escape the critical gaze.

The language of computation employs many vivid metaphors, which lose their
evocative power with frequent use. The task of a literary scholar becomes then
to renew the metaphor, in a process that involves close reading in search for
parallelisms that, as George Lakoff and Mark Johnson explain, configure one
conceptual system in terms of another [@lakoff_metaphors_1980, 3-14]. Why do
we call some software programs "applications" for example? The application of
what to what? Apple's iconic *Human Interface Guidelines*, a manual of style
that heralded the era of "what you see is what you get" interfaces, contains
echoes of Lakoff work and explicitly recommends *Metaphors we Live by* in the
bibliography section. The manual urges the designer to "convey meaning through
representation" and to seek the metaphor appropriate to the task
[@apple_apple_1987, 11]. Do not ask the user to throw "documents" into
"jars," for example, the manual entreats: "dragging the document icon to the
Trash means the user wants to discard that document" [@apple_apple_1987, 229]
Simulated objects must "look like they do in real world" it concludes
[@apple_apple_1987]. In the words of cognitive scientists John Carroll and
John Thomas, whose work was also references in the Apple design guidelines,
"people employ metaphors in learning about computer systems." Using the
appropriate metaphors therefore provides "wide-ranging improvements in
learning ability and ease of use" [@carroll_metaphor_1982, 108].

The seminal work on figurative speech undertaken by the Russian formalists at
the turn of the twentieth century reminds us that such metaphoric extension
also has its cognitive downside. Once habituated, experience passes into the
unconscious [@brik_poetika_1919, 104]. The thing "dries up" in Shklovsky's
words, first in perception and then in practice [@brik_poetika_1919, 38 and
104]. The metaphor, for the formalists, acts to conserve mental energy. Once
internalized, it does not appear to conciousness. The image becomes so
familiar that we cease thinking about it---an insight that was confirmed
experimentally almost a century later.[^ln-cog] Metaphoric conceptual
blending, a dynamic by which images and paradigms from one domain are extended
to another, improves learning and performance. Flusser would say it makes
experience more "smooth." The user avoids "bumping into the sharp corners" of
new and complicated computational environments. For formalists, the ossified
metaphor was also fraught with danger, as it threatened the vitality of life.
Shklovsky quotes from the diaries of Lev Tolstoy, who, while dusting his room,
could not remember if he already dusted his sofa. "Because actions like these
are habituated and unconscious, I could not remember and knew that it was
impossible to remember," wrote Tolstoy. "And so whether I dusted and forgot or
just did so without thinking, it as if the action never happened [..] thus
when life passes without conscious reflection, it passes as if one has not
lived at all." Life disappears into nothingness, when the "automatization of
experience "consumes things, clothing, furniture, your spouse, and the fear of
war" [@brik_poetika_1919, 104].

The formalists beloved that estrangement could "resurrect" the stale image and
provoke the "experience of the making of the
thing"[@shklovsky_voskreshenie_1914; @shklovksy_sborniki_1917, 7]. They
applied it in art and analysis alike. For example, in his influential essay on
"How Gogol's *Overcoat* Was Made" Boris Eikhenbaum analyzes the humor of Gogol
with clinical precision [@brik_poetika_1919, 151-67]. What readers lose in
having the joke explained to them, they gain in understanding of the genre. In
*Plain Text* I extend the formal concern with the literary device, to the
device proper. I too am interested in how the literary thing is made. Because
computers, as I will argue throughout, operate through symbolic and
metaphorical substitution, we can use the methodology of formal symbolic
analysis to our advantage. The challenge of such a strategy lies in the
relative paucity of our critical vocabulary. We have many theoretical models
to address literary phenomena on the macro scale: period, genre, style,
affect, world literature, etc. Yet the particulates of literary production and
dissemination on the micro scale go for the most part unnamed. To come to
terms with the book as a device; to begin to understand the nature of the text
simulation; to perceive the particularity of the computed sign: these are the
aims of *Plain Text*. A strategy of deliberate defamiliarization reclaims the
metaphor-device for analysis.

Yet despite its power to recall the world anew, estrangement cannot be
practiced effectively in the mode of a monologue. To produce meaning, it must
become dialogical and dialectical practice, as Flusser reminds us. Perpetual
exile is uninhabitable [@flusser_freedom_2003, 81]. Estrangement thrusts the
displaced into the chaos of unsettled existence. With time, the displaced make
a new home, from which they can once again "receive noise as information" and
produce meaning. "I am embedded in the familiar," Flusser writes, "so that I
can reach out toward the unfamiliar and create things yet unknown"
[@flusser_freedom_2003, 12]. The expellee and the settled inhabitant need each
other. Through what Flusser calls "creative dialogue," the dialectics of exile
can lead to "informed renewal" of shared space [@flusser_freedom_2003, 84].
Without the protection of one's home, everything turns to noise. There can no
information without a dwelling, Flusser writes, "and without information, in a
chaotic world, one can neither feel nor think nor act" [@flusser_does_2011,
12].

In *Plain Text*, I model the reciprocal movement to making strange on the
diverse practices of reverse engineering.[^ln-krsh] Unlike estrangement, the
reverse engineering of devices aims to bring the obscure to light. My
methodology thus lies in the dual complimentary motion between making known
and making strange. Each chapter unpacks metaphors like "closing windows,"
"bookmarking a page, "and "dragging and dropping files" to reveal the
internals of the device. The metaphor and the machine help organize the book
and each of its chapters. Each chapter unpacks a metaphor to its logical
conclusion.

The reciprocal motion to defamiliarization passes through a series of case
studies. The function of a case study in an engineer's education, as Henry
Petroski explains in his *Invention by Design*, is to understand the ways by
which one gets "from thought to thing" [@petroski_invention_1996, 3-7]. Along
with a metaphor, each of my chapters also contains at least one literary
"thought thing." Each also enacts a deconstruction---a literal taking
apart---of that device.

The digitally displaced hold on to the discomfort of the encounter with the
machine.  Estrangement---always at the heart of immigrant or queer
poetics---reconciles without seeking wholeness or integration. I dedicate this
book then to queers and immigrants, literal and figurative---spatial,
literary, technological---to those being displaced unwillingly, to those
exiled within and without, to those who understand the need for
self-displacement, to those who transgress purposefully, and to those willing
to trespass.

## 0.2 Computational hermeneutics

A concern with the contemporary conditions of simulated textuality leads us to
a rich archive from the history of literary theory, semiotics, telegraphy, and
electrical engineering from the middle of the nineteenth to the middle of the
twentieth centuries. Drawing on a range of archival materials at the
intersection of literary thought and the history of modern computing, *Plain
Text* examines a number of key literary-theoretical constructs, recasting the
paper book as a metaphor machine and computational object. I will argue that
extant theories of interpretation construct a notion of close reading based on
preexisting properties and assumptions attached to static print media. By
contrast, electronic text changes dynamically to suit its reader, cultural
context, and geography. Consequently, I argue for the development of what I
term *computational hermeneutics* capable of reaching past the surface content
to reveal also the software platforms and the hardware infrastructures that
contribute to the production of meaning.

I have selected "plain text" as the title of this book to signal an affinity
with a particular mode of interpretation. In technical terms, *plain text*
identifies a file format and a frame of mind. As a file format, it contains
nothing but a "pure sequence of character codes." Plain text stands in
opposition to "fancy text," "text representation consisting of plain text plus
added information."[^ln-uni] In the tradition of American textual criticism,
"plain text" identifies an editorial method of text transcription which is
both "faithful to the source" and is "easier to read than the original
document" [@cook_time-bounded_1972]. Combining these two traditions, I mean
ultimately to argue for a kind of a systematic minimalism when it comes to our
use of computers---a minimalism that privileges legibility and human
comprehension. I do so in contrast with modes of human--computer interaction
that may privilege other system--centric ideals like speed, security, or
efficiency.

The use of plain text implies an ethics of reading and writing. It is thus
also a frame of mind or a interpretive stance one takes towards the making and
the unmaking of literary artifacts. Besides the visible content, all
contemporary documents carry with them a layer of hidden information.
Originally used for typesetting, the formatting layer can affect much more
than innocuous document attributes like "font size" or "line spacing."
Increasingly, devices that mediate literary activity encode forms of
governance. Such devices can tacitly police intellectual property laws, carry
out surveillance operations, and force censorship. For example, the Digital
Millennium Copyright act, passed in the United States in 1996, goes beyond
legislature to require the "management" of digital rights (DRM) at the level
of hardware.<!-- cite --> An electronic book governed by DRM may prevent the
reader from copying or sharing stored content, even for the purposes of
academic study.  <!-- cite --> Computational hermeneutics strives to make such
control structures available for critical reflection. Building on the recent
work of scholars like Wendy Kyong Chun, Tung-Hui Hu, Matthew Kirschenbaum, and
Lisa Gitelman I make the case for an empowered poetics, able regain a measure
of control over the material contexts of our knowledge production
[@chun_enduring_2008; @kirschenbaum_mechanisms_2008; @manovich_software_2013;
@gitelman_paper_2014; @hu_prehistory_2015].

Today, it is becoming abundantly clear that the future of reading and writing
will always be in some way intertwined with the development of computer
science and software engineering. Even if you are not reading these words on
or through a screen, my message has reached you through a long chain of
machine-mediated transformations: from the mechanical action of the keyboard
(on which I am now typing), to the arrangement of electrons on magnetic
storage media, to the modulation of fiber-optic signal, to the shimmer of the
flowing liquid crystal display rendering the text. Computer science occupies
the space between the keyboard and the screen, which in turn habituates social
constructs, from the design of social media to the formation of massive shared
archives. Such "cultural techniques" are formative of our society as a whole
[@leroi-gourhan_gesture_1993; 83-84@siegert_cultural_2015]. Therefore, daily
choices like choosing a text editor, a filing system, or a social networking
platform cannot be addressed in shallow instrumental terms limited to
efficacy, speed, or performance.  Complex computational systems do not give
rise to ethics any more than financial markets do. Among the many available
visions of human--computer interaction, we can chose one that confirms to a
humanist ethos, whatever the reader's politics. Computational hermeneutics
encourages "users" to become active thinkers, tinkerers, and makers of
technology. It treats "binary" and "digital" environments as fluid literary
systems, amenable to the construction and the deconstruction of meaning. I
further encourage those who may have considered themselves mere "users" to
apply the same critical acuity they employ in the close reading of prose and
poetry to the understanding of code and machine.

*Plain Text* makes a historical case for the recovery of textual thought
latent in the machinery of contemporary computing. Just as philology cannot
survive without an understanding of its computational present, the design of
software and hardware systems that facilitate knowledge production cannot
advance without deep reflection about the cultural consequences of
infrastructure. In his now seminal essay on technological determinism, Langdon
Winner has argued that 

<!-- examples --> 
<! -- more on this -->
In the chapters to follow, the electronic literary artifact---the thing next
to my laptop on my desk---will come fully into view as a computational device.
I will insist on our ability to read such devices with the aim of developing
what I call the *hermeneutics of computational reading*. I will argue (a) that
the material history of computing belongs to the history of text processing as
it does to the history of machine control, (b) that the computer stands at the
root of *simulated textuality*, and (c) that the simulated sign splits to
reside in at least two distinct locations, screen and storage medium: the
first conspicuous but ephemeral, the second enduring but opaque. The "illusion
of the simulated text" identifies an incongruity between the two sites of
reading. The later chapters of the book will deal with the cultural
consequences of that illusion.

It will take us the rest of the book to come to terms with ambiguous textuality
in its bifurcated form. For now, I begin merely with the peculiarity of its
location. Computed text resides at multiple sites at the same time. Device
reading happens on screens that refresh themselves at a rate of around 60
cycles per second (Hertz). Screen textuality is therefore by definition
ephemeral. It is technically an animation. It moves even as it appears to stand
still. And it disappears when the device loses power. Yet paradoxically,
simulated text is also pervasive. Embedded into "solid state" drives and
magnetic "hard disks," inscription attains the quality of what Wendy Hui Kyong
Chun calls the "enduring ephemeral" [@chun_enduring_2008, 148]. Precisely
because simulated text adheres lightly to its medium, it has a tendency to
multiply and to spread widely. Like the fine particles of glitter, it is
difficult to contain. The simulated text falls apart into bits and pixels that
replicate and tumble about the system.

[^ln-flusser]: The work of @flusser_does_2011 has been similarly influential.

<!-- start here instead of the above few paras -->
<!-- consider taking out encoding entirely -->

<!-- come to the legibility question from the perspective of chapter 4 -->
<!-- legibility of what, of the whole thing -->
<!-- simulated text and how it obscures the material conditions -->

In drawing the history of character encoding, I want to describe gains and
losses accrued in the process of translation between machine and human
languages. And among a multitude of compromises, *legibility* stands as the
one that entails all others. I am going to repeat a version of the same thesis
for the duration of the manuscript: without legibility there can be no
understanding. Most attempts to articulate a humanist ethos, whether built on
consensus, narrative, discourse, or contract law, presuppose legibility.
Without legibility there can be no consensus, narrative, discourse or
contract. Yet, since advent the Gutenberg press and the vernacular reforms of
the Lutheran reformation, Western thought has taken legibility for granted.
<!-- footnote --> This is because until the advent of simulated text, print
had remained a remarkably stable medium.  Attempts to silence print through
book burning or censorship are viscerally obvious and met with nearly
universal disapproval. <!-- what the hell is that occludes, embeds ,
discretely--> Device textuality <!-- this needs a name --> threatens to return
us to pre-Lutheran times, where legibility was the domain of the select few
and interpretation the privilege of the chosen. <!-- we cannot read to some
extent anymore -->

<!-- this should go up -->

The illusion of ephemerality that follows simulation comes at a price of
legibility. "Software's ghostly presence produces and defies apprehension,"
Wendy Chun writes in her *Programmed Visions* [@chun_programmed_2011, 3]. But
what happens when all text is a type of software? Friedrich Kittler ends his
book on a similar note. In his vision, literature has finally been defeated by
military-grade encryption, secrecy, and obfuscation [@kittler_gramophone_1999,
263]. Unlike censorship or the burning of books, such modes of governance
*over* the sign are less obvious and more pervasive.  The simulation-producing
nature of computed text helps preserve the outward appearance of printed text,
while concealing the specifics of control. I mean control in the most direct
way possible, as a mode of physical regulation and barrier to access. The
difficulty of *Plain Text* will be in the description of such emerging but
often occluded technological contingencies.

The rise of critical practice in the humanities affirm the possibility of
interpretation under the sign of the simulated text. <!-- even print books are
born digital --> An ever increasing number of reading passes through some form
of a computational device.

Much is at stake in the material affordances of the literary artifact. The
political struggle for meaning-making---the very opportunity to engage in the
act of interpretation---begins with texts as material artifacts. In the West,
it is easy to forget the blunt effectiveness of physical control. Books that
are burned or redacted cannot be read at all. Elsewhere, global inequities of
access to knowledge compel readers to print their own books and build their
own libraries. Witness the so-called "shadow libraries" of Eastern Europe, the
street book vendors of India and Pakistan, and the gray market presses of
Nigeria arising form the country's "book famine."[@mahmood_copyright_2005;
@okiy_photocopying_2005; @liang_piracy_2009; @tenen_book_2014;
@bodo_short_2014; @nkiko_book_2014]. More than mere piracy, such
*samizdat*-like practices engage in the proactive preservation of the literary
sphere. Informal book exchange networks create reading publics that own the
means of textual production and dissemination. Under duress, readers build
homemade knowledge infrastructures: they duplicate, distribute, catalog, and
archive. In late-capitalist economies such infrastructures are commodified.
Consequently, they disappear from view. For many readers, technologies that
support reading, writing, and interpretation have passed from tools to fetish.
We have tender feelings for them and cradle them in our laps. No longer
comprehensible by the way of the pen or the printing press we imbue them with
magical powers and thus exist in the state of profound alienation from the
conditions closet to our mental activity.

<!-- consider explaining the term "affordances" in relation to mumford,
winner, and sts -->

The overarching aim of the book is therefore to expose the illusion of
verisimilitude between text on paper and the simulated text on a screen. The
words may look the same, but the underlying affordances of the medium differ
in significant detail. As an example, consider a news report that alters
important details based on the reader's location. Imagine an e-book reader
device that highlights popular passages of a novel in real time, shortening
the less popular passages down to their algorithmically distilled summaries.
For a literary analyst, the instability of the digital medium means analysis
cannot be confined to reading for surface meaning alone. How can close reading
persist when reading devices reconfigure the text to fit individual tastes,
mood, or politics? How would we even agree on the fact that we are reading the
same text? The very possibility of interpretation comes into question.

If we hope to trace the hidden flows of computed textuality, as Kittler wrote
"under conditions of high technology," we must do so from the position of
humanism. One cannot at the same time lament the systematic erasure of the
human from the literary process and advocate for post- or anti-humanism.
Unlike Kittler, who once wrote that in the face of technology "literature has
nothing more to say," I believe that literature and literary analysis continue
to have a voice in contemporary life.[@kittler_gramophone_1999, 263]
Technology does not necessarily silence---it subtly changes the nature of
textual phenomena. The instruments of literary analysis must consequently
evolve to deal with new technological contingencies. Where texts are
encrypted, we decrypt. If as Kittler writes, automated discourse analysis
threatens to take command, we will engineer automatons that command on our
behalf.[@kittler_gramophone_1999, 263] Such acts of resistance, small and
large, can recover a measure of agency from the ruling determinism of---take
your pick: markets, complex systems, unconscious drives, monistic universes,
*gaia* science, social networks, technology, the singularity, bureaucracy, or
war. Indeed, the possibility of human erasure never strays far from reach,
"like the face drawn in the sand at the edge of the sea"
[@foucault_order_1994, 387]. The fragility of life compels not the Foucauldian
wager on anti-humanism, but the need to mobilize whatever modest means
available for the human to persevere against great odds.

The advent of computational textuality necessitates a computational
hermeneutics, which enables unfettered access to text, code, platform, and
infrastructure. For now, commands like *xxd*, *pcap*, *ssh*, and *traceroute*
resemble arcane incantations that elicit hidden, symbolic action. Those who
wield them gain the metaphorical power to "hop" across, to "sniff" packets, to
"survey," to "traverse," and to "flood" network topographies. Computational
hermeneutics empower the reader to resist hard-wired models of machine-bound
interpretation. Yet today, resistance remains within the purview of the few.
Plain text channels itinerant streams of data back into the tidal pools of
human agency and comprehension for all. There, code becomes intelligible for
the very subjects whose loss Kittler laments. Only in such encrypted tunnels
and secure shells can anything like the digital humanities take root.

Computational hermeneutics reveal that not all texts are created equal. In
print, traditional distinctions between form and content lie flat. The
printing press firmly embeds ink into paper, leaving no space between type and
page. Media-minded critics like Johanna Drucker, Katherine Hayles, Matthew
Kirschenbaum, and Jerome McGann have urged literary scholars to re-evaluate
textuality in its media-specific contexts [@drucker_digital_2001;
@mcgann_radiant_2001; @hayles_print_2004].  Their work reminds us that the
flatness of digital text endures only in the guise of an illusion. Low-level,
operational intuitions governing textuality---ideas about form, content,
style, letter, and word---change profoundly as text shifts its confines from
paper to pixel. A substantial gap separates the visible text from the source
code that produces it. Forces of capital and control exploit that gap,
obscuring the workings of the device.[^ln-capital]

In *Plain Text*, I will argue that some of the higher--level political
afflictions of the contemporary public sphere---mass surveillance and online
censorship, for example---relate to our failure as readers and writers to come
to terms with the changing conditions of digital textuality. A society that
cares about the long-term preservation of complex discursive formations like
free speech, privacy, or online deliberation, would do well to take heed of the
textual building blocks at their foundation. The structure of discursive
formations---documents and narratives---has long been at the center of both
computer science and literary theory. Using primary sources from both
disciplines, *Plain Text* uncovers the shared history of literary machines,
bringing computation closer to its humanistic roots.

<!-- including this one -->


<!-- by this paragraph we need to explicate the simulated text -->
<!-- with historical anchoring -->

My challenge in this book lies in the relative paucity of our critical
vocabulary. We have many theoretical models to address literary phenomena on
the macro scale: period, genre, style, affect, world literature, etc. Yet the
particulates of literary production and dissemination on the micro scale go
for the most part unnamed. To come to terms with the book as a device; to
begin to understand the nature of the text simulation; to perceive the
particularity of the computed sign: these are the aims of *Plain Text*.

## 0.3 Theory and Practice

My approach to writing *Plain Text* stems from the desire to enact theory
capable of addressing the grim picture Friedrich Kittler paints at the end of
his influential monograph.[^ln-kittler2] Kittler was neither a technological
romantic nor a Luddite. I read the concluding chapters of *Gramophone, Film,
Typewriter* as a call to action. When Kittler writes that "media determine our
situation," he challenges his reader to choose between complicity and defiance
[@kittler_gramophone_1999, xxxix]. What can we do to counteract technological
determinism that Kittler warns about? In what follows, I outline several
intellectual lineages that frame my approach to the problem. A hermeneutics
capable of addressing Kittler's challenge must be grounded in materialism that
is both pragmatic and experimental.

The shifting affordances of digital text challenge some of our deep-seated
intuitions about literature. The word processor, operating system, and the
electronic book are some of the sites that stage the encounter between
literary theory and practice today. In *Plain Text* I introduce a method of
computational hermeneutics, which is a form of textual analysis strongly
influenced by materialist critique. Where "distant reading" and cultural
analytics perceive patterns across large-scale copora, computational
hermeneutics breaks textuality down into its minute constituent components. It
is at this scale that I find readers and writers becoming fundamentally
alienated from the immediate material contexts of knowledge production. Mine
is not however a post-human materialism of the kind that privileges an
object's point of view. On the contrary, the book aims to remove the aura of
fetishism that attaches itself to literary--computational artifacts and to
complex systems that mediate the textual encounter.

Critical theory, at its best, aims to see "the human bottom of non-human
things" [@horkheimer_critical_1982, 143]. As such, it is one of our most
powerful tools for analysis and resistance against technological
determinism.[^ln-determine] But as Max Horkheimer wrote, "the issue is not
simply the theory of emancipation; it is the practice of it as well"
[@horkheimer_critical_1982, 233]. Recently, scholars like Kathleen
Fitzpatrick, Tiziana Terranova, and Trebor Scholz have began to turn the tools
of critical theory towards the instrumental contexts of knowledge production
[@scholz_digital_2013; @fitzpatrick_planned_2011; @terranova_network_2004]. I
join them to argue that in treating the instruments of intellectual production
and consumption uncritically, all of us---readers and writers---accumulate an
ethical debt.

For example, it is one thing to theorize about the free movement of literary
tropes across cultures and continents, and quite another to have that theory
appear in print behind paywalls inaccessible to most global reading publics.
Similarly, a theoretical distinction between form and content, when
instantiated in specific file formats like Microsoft Word (`.docx`) or Adobe
Reader (`.pdf`), establishes divisions of labor between editors, proofreaders,
book sellers, and offshore typesetting firms.[^ln-sweatshop] One group trades
"content" in the economy of prestige, another "formatting" in the economy of
survival, and yet another controls distribution in the market economy, for
profit.

Distinctions of labor will remain in place as long as the conversation about
ideas like "form" and "content" persists in the abstract. Materialist critique
cannot achieve its stated aims without purchase on the material world. My hope
is that by grounding theory in practice, *Plain Text* can begin to repay the
debt accrued by materialism divorced from matter. Complex systems that support
the life of the mind today seem to lie beyond understanding or agency. In
almost a decade of teaching critical computing in the humanities, I routinely
encountered otherwise informed people who nevertheless felt hopelessly
estranged from the tools of their everyday intellectual activity. I suspect
that much of the metaphysical angst directed against computation in general is
really a symptom of that basic alienation. Contemporary knowledge workers
stare into rectangular black boxes for a considerable part of their days,
suspecting, in the absence of other feedback, that their gaze is met in bad
faith.

Connecting theories of meaning--making to the practices of meaning--making
offers a way out of the bad faith conundrum. Bad faith identifies a
misalignment between thought and action. The solution to connect "meaning"
with "operational meaning" belongs to a species of pragmatism. William James
articulated that view concisely when he wrote that "reality is seen to be
grounded in a perfect jungle of concrete expediencies"
[@james_pragmatisms_1907]. For James and other pragmatists, truth could not
exist in the abstract alone. It entailed also causes and effects that operate
in the world.[^ln-pragma-truth] In his essay "Pragmatism's Conception of
Truth," James asked: "How will the truth be realized? What concrete difference
will its being true make in anyone's actual life? What experiences will be
different from those which would obtain if the belief were false?" Frank
Ramsey, the young British philosopher close to Ludwig Wittgenstein would later
write in a similar vein about meaning as "defined by reference to the actions"
[@ramsey_foundations_2013, 155].[^ln-pragmatism]

For the pragmatist, truth-carrying propositions of the shape "X is Y" (as in,
"the author is dead" or "art is transcendent") beg the questions of "Where?,"
"When?," "For whom?," and "What's at stake in maintaining that?" Following the
pragmatic insight of James and Ramsey, I will proceed with the conviction that
abstract categories like "literature," "computation," and "text" cannot
possibly be (although they often are) reduced to a number of essential,
structural features. Rather, to borrow from Wittgenstein's *Philosophic
Investigations*, categories denote a set of "family" practices that may or may
not share in any given familial characteristic
[@wittgenstein_philosophical_2001, 67-77].[^ln-witt] To visualize the familial
model, imagine a tree diagram, where the tangled branches of computation and
textuality intersect and diverge in beautiful and yet arbitrary ways.

<!-- this section is way underdeveloped, bring in the citation into the text,
possibly dewey and technics and perhaps do bring this human-computer
interaction  connect to experimentalism-->

As a consequence of my commitment to a pragmatic materialism, *Plain Text*
shares in the experimental turn affecting a wide range of humanistic
disciplines. When beginning the project, I was initially inspired by the
writings of a little-known but influential ninetieth century French physician,
who was one of the first researchers to incorporate experimentation into
medical practice.[^ln-bernard] Writing against the tradition of "inductive
generalizers," Bernard believed that medicine could only advance by "direct
and rigorous application of reasoning to the facts furnished us by observation
and experiment." "We cannot separate the two things," he wrote, "head and
hand." He went on to write that "the science of life is a superb and
dazzlingly lighted hall which may be reached only by passing through a long
and ghastly kitchen." "We shall reach really fruitful and luminous
generalizations about vital phenomena only in so far as we ourselves
experiment and, in hospitals, amphitheaters, or laboratories stir the fetid or
throbbing ground of life" [@bernard_introduction_1957, 3-15]. Today, the
lighted halls of literary and media theory lead to the scholar's workshop.

[^ln-bernard]: On Bernard see @petit_claude_1987 and @sattar_aesthetics_2013.

In an approach to "doing" theory, *Plain Text* joins the experimental turn
steering the academy toward critical practice, especially in fields
long-dominated by purely speculative reflection. The experimental turn
represents a generation's dissatisfaction with "armchair" philosophizing.
Recall the burning armchair, the symbol for the experimental philosophy
movement. Joshua Knobe and Shaun Nichols, some of the early proponents of the
movement, explain that "many of the deepest questions of philosophy can only
be properly addressed by immersing oneself in the messy, contingent, highly
variable truths about how human beings really are" [@knobe_experimental_2008,
3]. The emergence of spaces where research in the humanities is done
exemplifies the same trend.  In naming the locations of their practice
"laboratories,"  "studios," and "workshops," humanists reach for new metaphors
of labor. These metaphors aim to reorganize the relationship between body,
space, idea, artifact, instrument, and inscription.

<!-- more on the experimental turn -->
<!-- highlight a specific project from pamela smith, describe pamela's project -->
<!-- expand -->

As an example of what I have been calling here the "experimental turn" in the
field of early modern history consider the preface to a recent volume on *Ways
of Making and Knowing*, edited by Pamela Smith, Amy Meyers, and Harold Cook.
The editors write that the "history of science is not a history of concepts, or
at least not that alone, but a history of the making and using of objects to
understand the world" [@smith_ways_2014, 12]. Smith translates that insight in
the laboratory, where, together with her students, she bakes bread and smelts
iron to recreate long--lost artisanal techniques. For those who experiment,
"book knowledge" and "artifactual knowledge" relate in practice.

Artifactual knowledge---from typesetting software to e-book readers and word
processors---shapes our everyday encounter with literature. Such technologies
should not be taken as value-neutral conduits of information. I follow Lewis
Mumford and Langdon Winner to argue that technology affects the exercise of
textual politics in subtle and profound ways. Artifacts cannot hold beliefs
about politics. Political power is rather exercised through them. Stairs do
not discriminate against the mobility impaired. The failure to enforce
accessibility through specific legal and architectural choices does.
Typesetting software, e-book readers, and word processors similarly embody
implicit communication models: ideas about deliberation, ethics of labor,
discursive values, and views about "natural" human aptitude for
interpretation. In this way, contemporary documents are capable of enforcing
limits to access by license, geography, or physical ability, for example.

To what extent does the book in front of you permit or enable access?
Whatever the answer, a function of understanding the text must include the
explication of its physical affordances. Experimentalism enables the critic to
"lay bare" the device. A literary scholar's version of baking bread and
smelting iron is to make literal the figure suggested by the idea of media
"archeology" on the level of the mechanism. In *Plain Text* we will dig
through, unearth, and excavate textual machines.

Similarly, it is my contention here that the cardinal literary-theoretical
concepts---such as word, text, narrative, discourse, author, story, book,
archive---are thoroughly enmeshed in the underlying physical substratum of
paper and pixel. It follows that any attempt to articulate the idea cannot
attain its full expressive potential without a thick description of its base
particulates.

Luckily for us, reading and writing are not esoteric activities. They are
readily available to phenomenological introspection. I will therefore
occasionally encourage readers to encounter the immediate contexts of their
reading anew: to put down the book or to lean away from the screen and to look
at these textual artifacts with strange eyes. In this movement of the body, I
want to disrupt the mind's habituated intuitions, pitting them against
knowledge "at hand" and fingertip knowledge: as when ruffling through the
pages or typing at a keyboard. To what extent is electronic textually
ephemeral, for example? (I discuss the question in full in chapter two.) The
idea of "ephemerality" can be made more palpable by asking: What can a reader
do with this text, here and now? Are readers able to copy and paste? Do they
have legal permissions to quote at length, to perform publicly, or to
otherwise trans-mediate?

## 0.4 Plan of the Present Work

<!-- needs more of a conclusion -->

<!-- new connector -->
<!-- rewrite -->

The tangled strains of thought and practice can exist only in relation to specific
communities of practice. A researcher cannot therefore expect to discover *the*
authoritative dictionary containing a neat summary of all terms needed for
analysis. What counts for "code" and "poetry" in one domain, like computer
science, may not account for the same in another domain, like creative writing.
An engineer's use of the words "code" and "poetry" differs from that of a
poet's. And yet an engineer by day can also become a poet by night. The
changing contexts evoke the corresponding shift in operational definitions.
Consequently, in *Plain Text*, I do not attempt to write a totalizing history
of modern computing nor a survey of literary theory. Rather, the argument
progresses from the action of the alphanumerical keyboard switch, through
copper and silicon, to liquid crystal and the floating gate, and on towards the
human and the community. It is but a single possible pass through a cavernous
black box. Each chapter reflects a waypoint along the journey.


The passage from keystroke to pixel runs a thread through the book. I begin
with a few simple questions that lay the grounds for digital textuality: What
exactly is a text? Where is it?  How will we find it?  My answers commence by
developing a theory of "microanalysis," the closest possible kind of reading
that pays attention to the material contexts of knowledge production.
Microanalysis opens the gap between text as it is embedded into storage media
and text as it is seen on the screen. I begin by laying out the case for
treating the contemporary "digital" book as a computational device. The nature
of computation

In the second chapter, I challenge conventional notions of digital media,
discovering properties at once "discrete" and "continuous." Both conventionally
threaten human capacity for comprehension. Which one is it, before or beyond
the human? A treatment of Liquid Crystal Displays (LCDs) illustrates that all
text is already in some sense "born digital," that is, by formal definitions,
"reproducible" and "differentiated" throughout. Furthermore, digitality depends
on "reliable processes of copying and preservation"---attributes that can mean
something different to a philosopher than to a librarian. From these insights I
take it that "being digital" is not an intrinsic ontological property, but
rather structure imposed from without. A thing is not digital in itself---one
makes use of something in a digital way. Materials from the history of
telegraphy in the late ninetieth and early twentieth centuries help narrate the
story of character encoding---a key hidden component of digital textuality.

I begin the book's third chapter by outlining a recent discussion on surface
reading. I ask: What lies beneath the text, literally? Case studies from the
early history of removable storage media, ticker tape and floppy disk,
elucidate the movement of text from human-legible inscription on the page and
punch card to magnetic inscription invisible to the naked human eye. In print,
form and content lie flat. On the screen, the two layers occupy physically
distinct strata. The rise of the pervasive Document Object Model (DOM) in the
twentieth century introduces a third layer which, in addition to form and
content, encodes device control. The material history of the DOM dispels the
illusion of flattened textuality. The control layer dynamically changes the
mechanics of the literary encounter. Texts now carry both a message and a set
instructions that alter the textual device. Consequently, I argue that literary
interpretation must include the exposition of the control layer.

The fourth chapter reaches beyond the device to the human. I begin with a close
reading of Beckett's *Krapp's Last Tape*. The title character makes yearly
audio recordings of himself, only to revisit them and to enter into a sort of
dialog with his own voice from the past. I posit this archival encounter as
Krapp's "media being" and suggest that such encounters are commonplace. Writers
and book collectors regularly deposit "snapshots" of their consciousness into
files, bookshelves, and folders. Jean-Paul Sartre's idea of an "appointment
with oneself" helps us see this external construction of files, folders, and
library furnishings as cognitive extension, in need of delicate pruning and
arrangement. In this light, I show that documents exist not as completed works,
but as "vectors" that mutate and move through time and space. Finally, I ask:
What is being externalized, communicated, and preserved? And answer: It is not
simply a message, but the subject itself.

In the last chapter of the book I tackle the apparent immateriality of digital
text. Ephemeral media brings promise of epistemological (social) and even
phenomenological (personal) transformation. But it also has a major practical
drawback. Inscription on magnetic tape cannot be assumed to correspond to the
composite screen image. Forms of governance like Digital Rights Management
(DRM) are embedded deep within the "data object" itself. A type of
steganography, or secret writing, hides the control layer from
view---precluding, and sometimes making illegal outright, the possibility of
interpretation. The chapter ends with a stark image illustrating the contrast
between screen surface and the underlying bit structure. To produce the image,
I inject malicious code into an Adobe Acrobat file (`.pdf`). The deformed text
threatens to damage the literary device. A thick description of the book, now
as weapon and instrument, brings legibility to the fore of reading ethics. The
design and usage of literary devices must itself become critical practice which
can, in complement to critical theory, actively engineer for human agency. A
discussion of technological dissent through the new humanisms of Hannah Arendt
and Franz Fanon concludes the volume.

In the process of textual production, printing and typesetting, it is certain
that my words were mixed with machine language, which in turn changed the
structure of the devices in your lap, in your hand, near to your eye,
embedded, or embodied. I could say that I bear no responsibility for extending
the reach of machine language so close to the reader. But that would be
factually incorrect. The choice of my writing implements and my channels of
communication affect deeply the contexts of interpretation.  Such choices, in
aggregate, define the shared ecosystem of knowledge that surrounds us.
Traditional strategies of close reading which limit interpretation to the
parsing of visible content risk missing the concealed machinations of naked
circuit control. It looks like you are reading a book, but this book may
change depending on the readers race, gender, ethnicity, geography, or
political affiliation. Who has agency program the device? Were a book also a
pill or fused with the neural circuitry of the brain, would you know what and
whom you were reading?


[^ln-cog]: See for example @gibbs_categorization_1992; @blasko_effects_1993;
@gibbs_poetics_1994; @neal_role_1997, 441-463; @gentner_alignment_1997.

[^ln-hegel]: I discuss the topic at length in Chapter 3.

[^ln-uni]: The Unicode Consortium. *The Unicode Standard: Worldwide Character
Encoding*, Version 1.0, Volume 1. Reading, Mass.: Addison-Wesley, 1990.

[^ln-lacan]: The evanescent absence of life that Lacan mentions as "the sign
about which Robinson Crusoe would make no mistake" [@lacan_seminar_1997, 167].

[^ln-pragma-truth]: For a more thorough discussion on the topic see
@seigfried_william_1990, @pihlstrom_structuring_1996, and @putnam_jamess_1997.

[^ln-pragmatism]: The intellectual legacy of pragmatism is wide-ranging and diffuse. It is
perhaps most pronounced in the teacher colleges, where James and Dewey are
still read widely, which could explain the ascendancy of such pedagogical terms
as "situated cognition"[@brown_situated_1988, @lave_situated_1991] and
"experiential learning"[@kolb_hegel_1981]: both terms denoting some sense of
necessary synthesis between of knowing and doing. In the field of linguistics,
philosophy of language, and communication studies, pragmatics are
well-encapsulated by the "language-as-action tradition," which harkens back to
the Oxford language philosophers like J.L. Austin, Paul Grice, and John Searle
[@trueswell_approaches_2005]. Austin's *How to Do Things with Words,* is
perhaps the paradigmatic formulation of the idea that words don't just mean
things, but that they enact change in the world.

[^ln-witt]: For more on the connection between Wittgenstein and James
see @goodman_james_2004.

[^ln-sweatshop]: See @freeman_high_2000 and @patel_working_2010.

[^ln-kittler2]: *Gramophone, Film, Typewriter* ends as follows: "And while
professors are still reluctantly trading in their typewriters for word
processors, the NSA is preparing for the future: from nursery school
mathematics, which continues to be fully sufficient for books, to
charge-coupled devices, surface-wave filters, digital signal processors
including the four basic forms of computation. Trenches, flashes of lightning,
stars---storage, transmission, *the laying of cables*
[@kittler_gramophone_1999, 263].

[^ln-determine]: I mean "determinism" as both (a) a belief in the intrinsic
agency of complex systems and (b) a practice of diminishing the scope of human
freedoms by technological means.



[^ln-capital]: Scholars like Alexander Galloway, David Golumbia, Bernard
Harcourt have advanced critique along similar lines. See
@galloway_protocol_2006, @golumbia_cultural_2009, and @harcourt_exposed:_2015.

[^ln1-brains]: For the first view see @putnam_minds_1960 and
@fodor_language_1975. For the second view see @deutsch_quantum_1985 and
@dyson_turings_2012.

[^ln-krsh]: The idea of reverse engineering is suggested also by Matthew
Kirschenbaum's idea of "forensic reading." See @kirschenbaum_mechanisms_2008.

[^ln-exposed]: For an extended exposition of this dynamic see
@harcourt_exposed:_2015.

[^ln-exposed2]: Again, a point that is given its full treatment in
@harcourt_exposed:_2015.

[^ln-dmca]: I will discuss the legals aspects of Digital Rights Management
technology and the consequences of the Digital Millennium Copyright Act in the
later chapters of the book.

## Cuts

The challenge of reading and writing such a book lies in its inherently
trans-disciplinary subject matter. Reflecting on the development of Morse Code
in 1949 in the *Proceedings of the American Philosophical Society*, Frank
Halstead mentions the difficulty of finding a home field in the arts or
sciences for what he calls "code development." "It is a matter somewhat
related to the general art of cryptology," he writes, "yet it is not wholly
divorced from electrical engineering nor from general philology"
[@halstead_genesis_1949, 456]. As Halstead anticipated, research for *Plain
Text*, a book about textual encoding, has led me to range of rare primary
archival materials: from the proceedings of the Association for Computing
Machinery (ACM) to the United States Patent and Trademark Office; from Bell
Labs to early Soviet publishing houses that heralded the advance of formalism;
from studies on animal communication behavior, to Unix manuals, to textbooks
on semiotics, and to foundational texts in the philosophy of aesthetics and
literary theory.

A skeptically minded reader may ask, what does all of that have to do with
literature? Surely, the domain of the digital belongs to computer science.
What can literary theory contribute to our understanding of digital
textuality?  My media archeological excavations reveal that the history of
software engineering shares common intellectual ancestry with literary theory.
Both converged on the separation of form and content at the beginning of the
twentieth century. Both subsequently articulated the idea of structure in
documents and narratives. The two fields often spoke the same language. But,
they did so in parallel, without much intersection. In *Plain Text* I attempt
to rebuild a vocabulary common to both fields.

For text to render on the screen properly it must be
"encoded" or "translated" from machine-transmittable code into human-readable
shape. Encoding constitutes a primitive field of textual activity, at the
crossroads of software engineering and the study of literature. Encoding
matters because how text is encoded, transmitted, and stored decides who gets
to decode, to receive, and to access.
