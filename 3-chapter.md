# Chapter 3: Literature Down to the Pixel (Screen Textuality, Simulated Text)

## 2.1 Metaphor Machine

Reading Jean Baudrillard at the end of the century prior, I understood his
writing on simulacra in relationship to ersatz entertainment: amusement parks,
virtual reality, reality television, and the like. But I think now that I then
failied to pecieve the pervasive nature of simulation. Computation, the
ultimate metaphor machine, mediates experience ubiquitously. It interjects in
simple everyday acts from drinking coffee in the morning, through brewing and
payment systems, to going to bed as alarm clock and night activity monitor.

Baudrillard writes:

> At the limit of an always increasing elimination of references and finalities,
an ever-increasing loss of resemblances and designations, we find the digital
and programmatic sign, whose "value" is purely *tactical*, at the intersection
of other signals ("bits" of information/tests) whose structure is that of a
micromolecular code of command and control [@baudrillard_selected_1988, 139-140].

The diffusion of simulated text will lead us to an archive of source materials
somewhat unusual for a work on literary theory. It will allow us to perceive
computation as a type of a literary activity, comprehensible in the lineage of
symbolism and book history. In search for the location of the symbol, we will
explore a number of symbol-manipulating devices that facilitate the production
of text. These will be found on the pages of mathematical journals and in
patent portfolios. Their ubiquity necessitates our attention. Three such
devices, which have fundamentally changed the nature of the literary encounter,
will emerge by the end of the chapter. Together, they tell the story of a
bargain that defines human--computer interaction: to trade facility for
comprehension, to complicate in return for the illusion of simplicity, and to
exist within that illusion without understanding how it is made.

### Ephemeral text


The bifurcated sign resides in two distinct locations, each entailing
drastically differing affordances for reading.

### Turing's *Computing Machinery and Intelligence*

"But if thinking consists only in writing or speaking, why shouldn't a machine
do it? Could a machine be in pain" [@wittgenstein_philosophical_1974, 17]?
Could it have internal states? "It is a travesty of the truth to say: thinking
is an activity of our mind, as writing is an activity of the
hand" [@wittgenstein_philosophical_1974, 17].

Wittgenstein's conversations with Turing about the ways by which physical
states pass into the symbolic ones and the other way around, prefigure the
operation of the universal Turing machine. The Turing machine occupies exactly
the described ambiguous state between symbol and mechanism.

This line of questioning was certainly already familiar to those students of
Wittgenstein who attended his earlier lectures at Cambridge, and who
undoubtedly have participated in his reading-related thought experiments.
Wittgenstein's reading and calculating machines were meant to challenge the
very notions of reading and calculating.

Turing returned to the problem of machine and human intelligence explicitly
[@turing_computing_1950]. Here, Turing proposes to reformulate Wittgenstein's
original question (Can machines think?) into what he calls the "imitation
game." The format would surely please his former professor: three people---a
man, a woman, and an interrogator of either sex---would communicate by
teletype. The object of the game for the interrogator is to determine the
gender of the individual behind the screen. The object of the game for the
other two participants is to fool the interrogator. And here is the twist: the
part of one of the participants (not the interrogator) would be played by a
machine.

The question "Can a machine think?" becomes then, "Will the interrogator decide
wrongly as often when the game is played like this as he does when the game is
played between a man and a woman?" [@turing_computing_1950, 433]. The striking
transposition of the cognitive activity (thinking) into its functional
discursive equivalent (misrepresenting one's identity in writing) echoes
Wittgenstein's playful thought experiments. The machine that thinks is a
machine that tells tall-tales. On this view, a convincing imitation of thought
is thought. We would simply not be able to prove otherwise, without peeking
behind the teletype screen to see who or what is doing the typing. In that
sense, Turing returns to Wittgenstein's refusal to locate the mark of the
cognitive within any given organ or activity. Thought is simply that which
looks (sounds, reads) like thought---it is a game that we play, and a game that
could conceivably be played by other entities, mechanical or otherwise. For our
purposes, it is interesting to note that Turing's chat-bot does not simply
compute or calculate.  It is a literary machine. It does not just imitate human
logic or speech.  Instead, it imitates (performs!) fibbing. It is able to
imagine a fictional story about an alternative identity, and ultimately to
convince its reader of the story's veracity.

In the shift from the cognitive to the discursive, Turing follows the
trajectory of Wittgenstein's thought experimentation. Moreover, the game is
tinged with distinctly sexual overtones, and in the context of an
interrogation. It is a game in which winning means transgressing either one's
gender or one's species, all the while maintaining a straight face. The
proposed language game is not simply conversational, it is suspenseful and
subversive, having the force of a mystery, a detective novel, or a legal
drama. At one point of the essay Turing answers what he calls "the argument
from consciousness," quoting one Professor Jefferson in saying that it is not
"until a machine can write a sonnet or compose a concerto because of thoughts
and emotions felt, and not by the chance fall of symbols, could we agree that
machine equals brain---that is, not only write it but know that it had written
it. No mechanism could feel (and not merely artificially signal, an easy
contrivance) pleasure at its successes, grieve when its valves fuse, be warmed
by flattery, be made miserable by its mistakes, be charmed by sex, be angry or
depressed when it cannot get what it wants" [@turing_computing_1950, 446].

In response Turing scripts the following conversation: 

> Interrogator: In the first line of your sonnet which reads 'Shall I compare
thee to a summer's day' would not 'a spring day' do as well or better?

> Witness: It would not scan.

> Interrogator: How about 'a winter's day.' That
would scan all right.

> Witness: Yes, but nobody want to be compared to a
winter's day.

> Interrogator: Would you say Mr.  Pickwick reminded you of Christmas?

> Witness: In a way.

> Interrogator: Yet Christmas is a winter's day, and I do not think
Mr. Pickwick would mind the comparison.

> Witness: I don't think you're serious.
By a winter's day one means a typical winter's day, rather than a special one
like Christmas [@turing_computing_1950, 447].

Add Plato. Reading is proper internalization. Interpretation is the scanned
symbol + state of the machine.

Literature as imitation game == mimesis, simulation.

[^ln1-chinese]: This line is a likely source for John Searle's famous "Chinese
Room" experiment [@searle_minds_1980].

[^ln1-enigma]: See the enigmatic fragment on @wittgenstein_remarks_1978, 372.

Tie to the first two chapters. Easy multiplicity of documents.  Digitality as
something imposed from without or imposed on us! Because the book is different
it fucks with authenticity.

The possibility of text that reconfigures itself.


Although much of contemporary popular discourse on computation speaks the
language of disruption, the history of computational symbolism, of the sort I
am suggesting here, must be seen as an evolutionary trajectory.

## 3.1 Digital Wake

What does the digital look like? It looks blue for one---not just any kind of
blue, but a particularly cool shade of pure blue, which passes from dark to
white, to translucent with starburst-like overtones. An online image search for
the word "digital" produces many sharp, sterile visuals of that kind.  Abstract
geometrical patterns predominate in the first dozen or so search results: often
forming curved, three-dimensional tunnels or lines that resemble circuit
boards. There are things too of course, cheap and expensive consumer things
like scanners, flash cards, circuit boards, and backup drives, but most of all
it is cameras---digital cameras, the very idea of which is meant to stand in
stark contrast to "traditional film photography." Few marketplace objects
exhibit such a strong sense of the opposition between the digital and the
analog as a consumer camera. These search pages also contain the outdated
clichés of yesterday's digital detritus: digital clocks, purple lightning
bolts, and the abstract chrome landscapes made widely available by graphic
editing software in the 1990s. There are numbers. The preferred arrangement is
in a torrential grid---the matrix---descending in the background, behind a
generic humanoid form, also translucent. Or better yet: alphanumeric characters
comprising a globe or a face. Ones and zeros are best arranged as an unending
string, which runs at a slight angle on the Z-axis and beyond the frame,
foregrounding whatever object that is meant to take on the digital as a
property: the digital wake.

When we constrict the image-search chronologically to the twenty-first century,
we see these cool-blue images give way to a more varied palette: bright prime
greens, yellows, and reds in retro-geometric pixelated shapes. Pixels take the
place of numbers here---not the small, invisible pixels of contemporary
computer screens, but the large and boxy pixels that by their very visible
boxiness flaunt the digital being of the image. Such an image alludes to the
time when pixels really did stand out as individual units---the technology not
being refined enough to produce the illusion of visual continuity. This faux
lo-fi aesthetic likely appeals to the romantic nostalgia many feel for the
early days of computing, and yet it also works as an assertion of independence
from that history. No longer shall the digital serve to emulate reality, nor be
judged merely by its degree of lifelike verisimilitude. The blocky world of
Markus Persson's *Minecraft* (2009) stands in antithesis to the magical realism
of Cyan's *Myst* (1995), the best-selling graphic-adventure PC game of the
twentieth century. Where the critics lauded the latter for its moody and
atmospheric photo-realistic environments, the former embraces the playful,
8-bit, low-fidelity aesthetic of gaming consoles from the 1980s. The
pseudo-pixelated world of Minecraft encourages its inhabitants to experiment
with deconstruction, producing cubist landscapes that foreground the
discontinuity of form and surface.

The iconography of the digital works in the other direction too, by approaching
and challenging the perceived continuity of the analog world. Liquid-metal
*Photoshop* font effects are thus supplanted by hyperrealistic renderings of
fire, smoke, and water: fluid elements that are by their very nature difficult
to render digitally, especially in movement. Digital alchemy approaches the
boundaries of technological possibility, but absent the constraints of realism
it pushes past reality, past nature, and past mere fidelity to the natural
world. A real-world explosion caught on film looks cheap in comparison to its
cinematic "special-effects" simulacrum. A blazing corporate logo rendered
digitally would lose much of its appeal were it produced by literally lighting
a logograph on fire. The burning digital logograph does not just say "We are
hot" or "We are on fire" (that would be too naive)---it says that our fire is
better than fire, more vivid and more lifelike. The digital fire is the ideal,
Platonic image of fire: an image that all fires should emulate.

I am eliciting these digital commonplaces to get at the conflicting popular
intuitions about "the digital" as something at once discrete, angular, and
therefore reductive and deterministic, but also shiny, smooth, perfect, and
liquid in the way that threatens to drown out and to transcend (instead of
reducing or determining) everything susceptible to the patina of time,
everything having distinct shape and anatomy. (Think here about the transition
between *The Terminator* and *Terminator 2*, where the "bad" robot that sets
out to destroy humanity is cast at first in the guise of an animatronic
mechanism, but then as an amorphous puddle of mercury that makes the original
mechanical Arnold look like a friend and savior. The first terminates with
shotguns and tire irons, where the second morphs into human shapes and pours
itself inside, threatening life from within.)

Like the terminators of yesteryear, digital sprites and bogeymen lose their
hold over popular imagination. Digital photography, digital clocks, and
digital humanities already ring archaic in their futuristic ambition, going the
way of e- or i- anything: the way of retro-suffixes like -bot, -mat, -lux, and
-tron.  The digital dissolves into the everyday as all clocks, all books, all
texts, and all human activity passes through some form of "digital being." My
aim here and throughout is to understand what is meant by its invocation: to
"come to terms," to make visible, to disturb it, to denature, and to make it
strange again.

What's at stake? Quant. of information. We are talking about synchronizing time.
Bring in Warren Brody's manifesto. TIME. And also digital is not about human.
Bring it the too much info from Kenny Goldsmith.
Digital text. What makes it digital? Because we have a device, does it mean
that we are not digital? It is structure form the outside.
What changes?

Literary devices are those machines that in themselves constitute a system for
textual diffusion. They facilitate symbolic exchange and they do so in diverse
textual ecosystems. The ecosystem of aesthetic discourse is one such market of
exchange. But I am interested in others as well. The film studio, the hospital,
the court house, and the university: these are are some of the locations of
distinct literary activity. Like literature in the aesthetic realm, these are
systems that propagate localized notions of document, style, authorship, value,
and canon.

## 3.2 Motion Blur

Televisions sold at the turn of 2010 started shipping with a feature called
"Motion-Compensated Frame Interpolation" (MCFI). The technology causes some
viewers to report what has been dubbed as the "soap opera effect": a condition
where even an expensively produced, big-budget film begins to look cheap and
unnatural. I am quite susceptible to this phenomenon: I can tell when MCFI is
in effect right away when watching something on someone else's television.
Everything in the dynamics of the shot looks somehow fake. The actors, even in
films that I've seen and love, appear to be acting. I no longer believe in
their characters as real people. I notice their makeup and their props and I
just know they are faking it. The sensation is akin to watching bad silent
cinema. The style of acting seems forced and over the top. One cannot imagine
people took *that* sort of thing seriously. The soap opera effect makes relics
of recent cinema.

The technology behind MCFI was developed in response to motion blur particular
to flat-panel liquid crystal displays (LCDs). LCDs work by passing light
through a liquid crystal medium, sandwiched between two polarized light filters
rotated at 90 degrees to each other. As light enters through one side, it
twists following the slightly curved molecular structure of the crystal,
allowing the beam to rotate and to pass through both filters. The structure
unwinds when it receives an electrical current, effectively blocking the light
from passing through the medium (into one, but not out of the other). By these
means, a series of small, pinhole-like pixels can be turned on and off to
create shapes, letters, and images on the screen.[^ln1-mishima]

[^ln1-mishima]: See @mishima_novel_2004.

![Liquid crystal in phase transition between polarizers. Schlieren texture
characteristic nematic phase to the left, and circular focal conic domains in
the smectic phase to the right. Image licensed under CC-BY-SA 3.0
[@polimerek_smectic_2004].](images/liquid.jpg)

Traditional film projectors advance from one frame to the next at a rate of 24
times each second to match the rate at which the film is captured by
traditional film cameras. The projector's light flickers two to three times per
frame to produce the effect of continuous motion, giving the projector a
"refresh rate" of 48–72 cycles per second, or Hertz (Hz). Without this strobing
effect, under continuous light, the succession of frames moving past the light
would appear as a blurry streak of undifferentiated images. An image persists
in the human visual field for about 16 milliseconds after cessation of a
stimulus, giving humans the temporal resolution of around 60 images per second.
Consequently, an average person begins to reach the state of "critical flicker
fusion," where the strobe light becomes unnoticeable at refresh rates higher
than 60Hz. At the limits of flicker fusion, the image begins to persist through
the gaps between frames, creating the appearance of continuous motion
[@efron_conservation_1973; @coltheart_iconic_1980; @brannan_applications_1992].
An LCD screen mimics the action of the analog projector by refreshing the
screen at intervals between 60 and 120Hz.

To complicate matters, human vision is more sensitive to rapid motion at the
periphery of vision, in a mechanism likely evolved to provide swift response to
predator attack [@burr_temporal_1981; @didyk_perceptually-motivated_2010]. As
humans move closer to their screens (as when sitting in front of a computer
monitor), more of the image enters the field of peripheral vision, making the
viewer more sensitive to screen flicker. Furthermore, being closer to the
screen increases the angular velocity of any depicted moving object. When
viewed from afar, the movement of several inches on the screen corresponds to a
change in the sight angle of several degrees, and likely a few millimeters of
iris movement. When viewed up close, the same several inches on the screen
represent a much larger angle, forcing the iris to move further laterally.

The human visual system is particularly adapt at tracking smooth, horizontal
movement, cognitively anticipating the perceived trajectory of a moving object,
while stabilizing the retinal image and keeping it at the center of the fovea
in what is called "smooth pursuit" lateral eye movement
[@rottach_comparison_1996; @purves_types_2001;
@didyk_perceptually-motivated_2010]. The "sample and hold" nature of the LCD
display frustrates the cognitive assumption of smooth movement. The object's
anticipated location does not correspond to its actual trajectory on the
screen, which appears to the eye as motion blur. To complicate matters further,
the phase transitions of the liquid crystal medium are not instantaneous. The
individual pixel "fades" instead of vanishing at once, leaving a physical (and
not merely a perceptual) trail of not-quite transitioned pixels in the wake of
object movement [@kurita_moving_2001; @pan_lcd_2005]. The motion blur is
exacerbated when viewing traditional film shot at 24 frames per second, since
even at high display sampling rates, the moving object appears at discrete
stationary locations punctuated by gaps in on-screen movement.

To compensate for motion blur, LCD manufacturers introduced MCFI, by which the
television itself inserts extra computed frames in between the original stock
footage. The extra frames should, in theory, make the motion appear more
natural, filling in the missing gaps that confuse the brain. Unfortunately for
the viewer, the effectively higher sample rates carry a major, unintended side
effect. Most viewers associate sampling rates of 40Hz and above with daytime
soap operas, which were, for a time, shot on lower quality video equipment as
opposed to expensive film equipment in use by the big-budget film industry.
Film is expensive because film cameras work by fixing the image onto the medium
through a photochemical process. Like traditional film photography, raw footage
must be developed and processed properly before it can be used for editing,
playback, and distribution. Video and more modern, digital recorders, by
contrast, transform light into fluctuations of the magnetic field, storing the
results on magnetic tape. The transducer (tape head) reads and writes directly
from and to tape, making magnetic storage significantly more compact and less
expensive than film, as it does not require chemical processing for playback
[@edgerton_columbia_2009]. Crucially for our story, video recorders operate at
60 frames per second, a recording resolution that together with a distinctive
color profile and audio aberration signatures give rise to what viewers
perceive as that "cheap video" look. By inserting extra frames into the moving
image, modern televisions in effect alter the expensive, slow format to match
its faster, cheaper video equivalent. The net improvement in sampling degrades
the perceived quality of the original. (It would be interesting to try to
create the reverse effect by reducing the fidelity of a "cheap" soap opera
video feed to 24 frames per second.)

The soap opera effect suggests a few questions for the media scholar.

First, it challenges the easy equivalence between "digital" and "discrete"
properties of the medium so often evoked in the critical literature. Another
mechanism must be in play here, since, according to accepted intuition, film
(the more discrete format of representation) is the one that more closely
approaches the analog (continuous) nature of observed phenomenon. Despite being
the more fragmentary medium, film is seen as the more analog format than video.
Under closer examination, film, usually discussed in the context of analog
formats, appears to be (in some aspects, as we will see) more digital than
digital video, particularly if "digital" is taken to mean "discrete" and
"differentiated." This incongruity points to a confusion in the common terms.

Second, it seems that the very material properties involved in our
understanding of "digital" and "analog" formats are also implicated in
higher-level functions of aesthetic judgment. Perception of quality in a given
recorded performance drifts with the vagrancies of encoding. "Good acting"
inexplicably falls apart with the introduction of extra frames.

Finally, where one would expect "artificial" digital discreteness to stand in
opposition to a measure of "natural" analog continuity, a thick description of
innate human perceptual apparatus reveals a complex patchwork of fragmentary
cognitive mechanisms, already to an extent digital and discrete in their
operation. At some level of analysis, gaps in the encoding format relate to
gaps in human visual processing. Neither can be said to represent reality with
perfect fidelity. Instead, brains and cameras stitch landscapes together from
an unevenly sampled landscape of variegated visual topographies, achieving a
measure of arbitrary synchronicity between human and machine.

Conventional notions of digital being further conflate the discrete nature of
digital media with human debasement, following the logic by which the perceived
material impoverishment in one sphere leads to the implied spiritual
impoverishment in the other.  The sentiment is everywhere in the popular press,
having deep intellectual roots in the history of thinking about technology.
Philosophers of technology from Heidegger to Kittler advance a powerful
"hermeneutics of suspicion" towards mechanization, digitization, and the
subsequent computability of human experience. To take that tradition seriously
is to direct hermeneutic suspicion to aspects of digital being that have
meaningful socio-political consequences.  If, as the case of motion blur
suggests, human experience is already and always "born digital," that is,
discrete and differentiated throughout, we must find ways of advancing critique
along theoretical distinctions that better capture the instrumental reality of
media practice. Nostalgia for "analog" oneness and continuity should itself
fall under the critical purview, examined alongside media marketing slogans
advertising gapless playback and lossless file formats.  Digitality, as I will
argue for the duration of the book, constitutes a part of the human condition
worthy of conservation. But, what I mean by "digital being" relates to the
practice of buying "digital cameras" or to promoting the "digital humanities"
in name only. To perceive what is at stake, we must do much more to
disambiguate digital cognates: electronic, binary, and computational.

## 3.3 We Have Always Been Digital

Let us start with digital representation expressed more formally, as done in
computer science, aesthetics, and the philosophy of information.

The difference between discrete and continuous quantities defines the
conversation about digital media.[^ln1-maley] For example, in a popular book
about what a "well-informed person should know about computers," Brian
Kernighan writes about "analog" as something meant to convey "the idea of
values that change smoothly as something else changes" [@kernighan_d_2011,
526]. Much of the world is analog, Kernighan explains. A water tap, a pen, or
a car steering wheel are all examples of analog interfaces. For example, when
riding a bicycle, turning the handlebars one way results in a corresponding
motion of the machine. This motion is smooth. Compare that with the action of a
light switch. A properly functioning light switch takes on two discrete states
only: on or off. A range of pressure applied to the switch does not correspond
to any mechanical action of the lever. But once a certain threshold is reached,
the switch "flips" to change states. "Digital systems deal with discrete
values," Kernighan writes. The switch contains a limited number of state
possibilities (2), where the bicycle handlebars could be rotated in an infinite
number of minutely differing gradations.

The philosopher Nelson Goodman (responsible for bringing the digital
conversation into the realm of aesthetic theory in the analytic tradition)
makes a further distinction in maintaining that digital systems must be more
than merely "discontinuous," but also "differentiated throughout." By contrast,
analog systems are "dense" and "undifferentiated to the extreme"
[@goodman_languages_1968, 159-164]. Written language and music notation, by
this definition, are digital systems par excellence, having the property of
reducing the undifferentiated, analog input (human thought) into discrete
semantic units (text or musical notation).[^ln1-goodman] Following Goodman's
logic one can reasonably maintain that the art of painting, unlike music or
language, cannot be reduced to the production of discrete semantic units, and
would be more of an analog system under the proposed definitions. In this
light, the language of cubist painting, which reconstructs shape into its
modular atomic components, can be viewed as an attempt to move painting from an
analog to a digital art form. The digital image, reduced to pixels of uniform
shape and size, takes the logic of cubism to its conclusion.

An interesting corollary to the continuous property of analog systems is our
inability to duplicate their states exactly. I can approximate the pressure
someone else puts on their bicycle handlebars with some arbitrary measure of
precision that can never reach perfect reproducibility. This means also that
while more digital art forms like literature are, in some sense, perfectly
reproducible, analog forms, like painting, are not. Following a similar chain
of reasoning, the late great American philosopher John Haugeland amends
Goodman's foundational definitions of digital representation to include
"flawless copying and preservation." This property further has the effect of
freeing digital representation from its medium. A Rembrandt is bound to its
canvas in a way that Shakespeare's *Hamlet* is not. Digital systems are media
independent. *Hamlet* can be reproduced on paper or stone, in Morse code, or
bound in liquid crystal, and still be *Hamlet* in some natural understanding of
the work. To the definition of Goodman's digital systems, Haugeland therefore
adds the notion of "feasible procedures" that can lead to "positive and
reliable" processes for reading and writing digital tokens
[@haugeland_analog_1981, 213-225]. In the case of the soap opera effect, we may
think of film as something like an "irreproducible series of paintings" and of
video as "perfectly copyable magnetic facsimiles." (This would at least restore
film to the conventional understanding of analog formats.)

Writing more than a quarter of a century later, Matthew Katz offers two further
important addenda to the discussion between Goodman and Haugeland. First, Katz
maintains that Goodman's notions of discreteness and differentiation properly
affect the format and not the medium of representation. Second, he maintains
that the distinction between digital and analog formats could sometimes relate
to the person involved, and not on the medium itself.  To illustrate these two
amendments, Katz gives the example of an approximate measuring system that uses
a supply of marbles in a large beaker.  The unit of measurement could be
something like a "number of handfuls." Even though marbles are a perfectly
discrete medium, the system is analog because no convention is established to
reproduce a "handful" with any sense of precision. My two handfuls will be
different from another's. A corollary to the distinction between medium and
format, then, lies in the user's ability to perceive quantities.  The reader
(viewer, listener) matters. Were humans endowed with the magical ability to
perceive the exact number of water molecules in a beaker, previously analog
systems (like unmarked beakers) would in effect become digital (under the
expanded definition). Similarly, were all humans endowed with hands of a
definite size and volume, "handfuls" would also be counted as discrete and
therefore digital quantities. From similar thought experiments Katz concludes
that the physical, perceptual, and cognitive capabilities of the "user"
(reader, audience, perceiver) affect the ontological status of the system
[@katz_analog_2008].

The movement from medium to user radically destabilizes Goodman's original
insight. Where Goodman begins with defining digitality in terms of physical
properties (divisible, or indivisible, dense, or continuous), Katz ends with it
as format imposed on matter from without. Undifferentiated matter like cake is
only analog until someone cuts it into pieces. Furthermore, the ontological
status of cake changes depending on the person doing the cutting. The cake is
more digital, in a sense, for someone armed with a laser cutter and a
microscope.  Similarly, painting can be thought of as a perfectly digital art
if we consider it at the scale of art gallery as a series of discrete,
replaceable, and reproducible frames(!) of canvas.[^ln1-art] What does it mean
to digitize text then when text is already, by some definitions, a digital
format?

These conundrums reveal a problem in the very formulation of the question: What
*is* digital representation? Under examination, properties related to the
digital--analog distinction cease constituting an ontological category. Rather
than a state of being, they indicate a systematic imposition of structure. The
more appropriate question then is: What *makes* for digital representation?

From the instrumental point of view, to make something digital is to liberate
representation from its medium. To digitize means also to allow for "copying
and preservation," actions that, when taken outside of their philosophic
context acquire an immediate practical implication (for a librarian, for
example). That is not to say that digitization makes thought wholly ephemeral
or immaterial; rather, that it allows for the continual advancement and
transmediation of thought---the ability to change specific material contexts as
afforded by the very property of "media independence." Digitization, in that
sense, is a necessary prerequisite for culture (in the common sense of
"collective intellectual achievement"). Without digital portability, all
representation (art and knowledge) attaches itself irrecoverably to
untranslatable and irreproducible conditions of its material production.

The digital being of representation, initially seen as a property of the
medium, arises then as a political property regulating the very transmission of
thought. Text on paper or screen remains digital only in so far as the medium
is not fixed. Truly digital text can be copied and placed into other hands and
minds, feasibly and reliably. The possibility of these procedures is what
ultimately gives representation its digital form. A document restricted or
classified in any way loses some of the necessary preconditions for being
digital. This also means that technologies like "digital rights management"
(DRM), which work to fix representation into a specific medium, transform
digital content back into its media-dependent, irreproducible, and analog
forms. Another way to put it would be to say that on "protected" devices (like
proprietary e-book readers) the content is digital for some users and analog
for those without permissions to copy, share, and transform.

The word, already a discrete quantity, comes into digital being as form when
coupled loosely to its material contexts. Ontologically, text is by nature a
digital format: first because it represents discrete units of information about
the world, and second, because it allows for some measure of "flawless copying
and preservation." Copying and preservation constitute the logic compelling the
historical advancement of writing. But, also note that "flawless copying and
preservation" are in themselves contingent (not fully determined) properties of
writing. For example, imagine a world in which ideas forever "adhere" to their
brain-bound media.[^ln1-brain] Imagine also a society that positively prohibits the
transmediation of thought, on paper or between brain cells. Envision extreme
forms of thought control that restrict the very basics of speech and literacy,
prohibiting the manufacture of pens, paper, computers, photocopiers, voice
recorders, and word processors. Such prohibitions would amount to total
censorship. A radically analog society would also be wholly mute.

Understanding digitality as a format imposed onto matter, a format that
facilitates transmediation, recasts the history of computing as something other
than simple "mathesis," the idea that computation reduces the world into more
discrete, and therefore computable, elements. That idea would be true if the
computer was simply a glorified calculator. But computers are not just that: in
practice, they reveal themselves as self-amending machines for universal
transmediation (not limited to numbers)---machines that, depending on the
user's acuity and dispensation to perceive text (and all information, really)
as deep structure, separate readers into those for whom texts exist as fixed
analog given, and those for whom they function as truly digital media, capable
of transformation affecting further texts, people, and machines. To paraphrase
a post-structuralist insight, the process of digital interpretation is in
itself a type of creativity, forever advancing rather than apprehending the
lateral movement of thought.

[^ln1-brain]: At the physical level, the process of textual remediation begins
at the brain, as when thoughts are initially transcribed onto paper.

[^ln1-art]: "Replaceable" and "reproducible" as in the sense that an art
gallery perseveres through the continual loss and rearrangement of its
constituent art works.

[^ln1-maley]: See for example @maley_analog_2011: "The received view is that
analog representations vary smoothly, while digital representations vary in
stepwise manner. In other words, 'digital' is synonymous with 'discrete,' while
'analog' is synonymous with 'continuous.'" See also @kittler_there_1995,
"Confronted as they are with a continuous environment of weather, waves, and
wars, digital computers can cope with this real number-avalanche only by adding
element to element."

[^ln1-goodman]: Goodman differentiates between "syntactic" and "semantic"
density. Some notational systems like writing and the decimal system are,
according to Goodman, "syntactically differentiated but semantically dense."
The key distinction for him seems to be a "limit on the length of message," by
which I think he means something related to infinite divisibility. The decimal
system as a whole can continue to approach a quantity indefinitely, reaching an
arbitrary point of precision. The computer limits decimal precision to some
arbitrary "depth" of approximation, making it semantically discrete. I find
this part of the argument unconvincing.

### 2.4 Binary Collapse of Space and Time

I submit another historical case study. Electrical engineers involved in the
development of the telegraph at the turn of the twentieth century fell into two
warring camps, in a conflict located precisely at the fault lines between
discrete and continuous representation.

Systems based on the neutral direct current design[^ln1-ndc] assumed current
flow for some elements of the encoding (like dashes) and absence of current
flow for others (like dots) [@welk_neutral_2000]. For this reason, it could be
said that although the Morse code family of alphabets amounts to a ternary
encoding system (using dots, dashes, and silences), under the conditions of
direct current it functions as a second-order binary encoding. Systems based on
the alternating current design converted Morse code (and other encoding
systems) into pulses of alternating positive and negative current, where
positive current could stand for "dashes" and negative for "dots." Where the
direct current designs preferred the use of non-sinusoidal, "square"
periodic waveforms to represent binary states, literature on alternating
current design often argued for the use of a sinusoidal signal, which could be
more easily modulated into a multitude of states beyond binary
[@crehore_practical_1900; @hausmann_telegraph_1915, 374]. Consequently,
alternating current designs tended towards the transmission of continuous
quantities like images and cursive handwriting. The very visual form of the
sinusoidal curve suggested a connection to natural, organic shapes, like
handwriting, in a visual analogy between text and signal.

The Pollak-Virag telegraph that Bernhard Siegert mentions in his seminal essay
on German media theory was one such device.[^ln1-virag] The original
Pollak-Virag machine, patented in 1900 (and filed in 1899), proposed to send
regular (ternary) Morse encoding by means of alternating current of two
different polarities and two different strengths, for a total of four types of
signals (see Figure 1).[^ln1-siegert] Other than the irregular signal shape and
the use of alternating current, the Pollak-Virag encoding differed little from
single-Morse systems. By 1901, however, the authors were issued an American
patent for the "Writing Telegraph," which builds on their previous design in an
interesting way. Rather than using a wave in four steps, the authors propose an
"automatic transmitter capable of sending current impulses over the line which
correspond to the direction and the size of a single letter element"
[@pollak_writing-telegraph_1901, 3]. In other words, they wanted to bend the
sinusoidal wave to correspond roughly to the shape of the letter (see Figure
2).

![Record obtained at the receiving station, illustration from "Means for
Rapidly Transmitting Telegrams," 1900
[@pollak_means_1900].](images/virag-wave.png)

On some level, then, the Pollak-Virag encoding constitutes a sort of an
unbroken, electrical alphabet. The electrical signal, in this case, makes for a
smoothly varying quality, intended to "trace in a substantially continuous
unbroken outline the written letters composing the matter transmitted"
[@pollak_writing-telegraph_1901, 1]. The perforated tape "driving" the signal
does, however, reduce notions of continuity into distinct perforations, which
break letters down into their individual constituents.

![Writing Telegraph, 1901 [@pollak_writing-telegraph_1901].](images/virag.png)

![Syphon Recorder Alphabet, Circa 1887--1935. From the personal
collection of Jim Linderman.](images/cyphon.jpg)

The metaphysics accompanying telegraph communication at the time display an
anxiety about the role of the human in this process, where notions of material
discreteness and continuity are extended from the underlying physical structure
of communication to implicate higher-order cultural (and even ethical)
categories. Arguing in opposition to the sine-wave camp, Donald Murray writes:

> Certainly if Smith wants to make Jones spin round like a dancing Dervish, the
best way might be for Smith to transmit sine waves […] but in practice Smith
always wants to make Jones perform an excessively complicated and irregular
series of motions, and for this purpose it is essential to transmit similar
motions by introducing upper harmonics in a fragmentary, non-periodic, and very
irregular way [@murray_setting_1905, 559].

Being periodic (and therefore, repetitive), the continuous sine wave is too
regular to answer to the diverse needs of human culture. Dance, in Murray's
articulation, can also consist of a series of irregular, fragmentary, and
non-periodic movements, much better captured by the discrete undulation of the
square wave.[^ln1-dervish] Writing for the Swedenborg-inspired *Spiritual
Telegraph*, an anonymous author unfolds normative neoplatonic rhetoric in a
similar argument on the importance of discrete differentiation:

> Whenever two persons are brought into sympathetic relations, either by
corporeal contact or through those refined media which pervade the Universe and
serve as the airy vehicles of thought, they mutually feel the presence of each
other, while the mind which is gifted with the greater degree of activity and
power at once becomes the proximate cause and fountain of inspiration to the
other. Thus from sources superior to ourselves, the very elements of life and
thought flow into us, and every living thing, according to its nature and
discrete degree, derives a kind of inspiration from that which is above. In
order to facilitate the transmission of impressions in this way, the recipient
must be willing to receive instruction and assume the passive or negative
relation of a learner; otherwise he will be likely to resist, unconsciously,
the infusion of foreign impressions and Divine ideas [@brittan_spiritual_1854,
169].

Following Swedenborg, the telegraph spiritualists conceive the discrete quality
of human experience as essential to the Christian hierarchy of the
universe.[^ln1-swedenborg] To deny step-wise ordering of "refined media"
pervading the universe is to close oneself off to the "transmission of
impressions," in a communication act (consisting of a "contact" and the
exchange of signifying "thought vehicles") between "source superior" and the
"receiver of instructions," placed in a binary and opposing relation
("negative" but without "resistance") to the idea of the divine. Continuity
mixes the sacred with the profane. For the spiritualist, a truly Christian
order must remain digital. To maintain otherwise would be Gnostic heresy
[@edwards_neglected_1990; @shaked_dualism_2005, 52-71].

Following Kant, it is common for contemporary cultural critics of computation
to associate the digital, discrete view of the universe with determinism and
its consequent threat to the idea of free will.[^ln1-golumbia] The telegraph
spiritualists instead associate determinism with continuity. Witness George
Dole writing in his Philosophy of Creation:

> Scientists have prosecuted research on the plane of continuous degrees of the
ultimates, and they have thereby failed to penetrate to interior things of
discrete degree. Consequently they derive life as not from the Lord, but from
nature, of which they have no other idea than that it is something mechanical
[@dole_philosophy_1906].

Neither camp can ground their claims in the material contexts of media
production (or in cognitive phenomenology). Nothing in the operation of the
Pollak-Virag telegraph gives us reason to posit either analog or digital
representation as somehow more or less essential to the human experience.
Instead, in reconstructing the long chain of transcoding and transmediation we
observe a hybrid system at once fluid and mixed with discrete meaning-carrying
entities, down to the level of underlying cognitive structure (of human
perception). Rapid phase shifts between discrete and continuous representation
in themselves constitute the instrumental purpose of the device.

It is tempting to think of the telegraph as a digital [*sic*] apparatus,
created to convert analog [*sic*] input into electric signal. Much technical
literature on the subject makes that simplification.[^ln1-tele] The innards of
the Pollak-Virag telegraph reveal a more complicated mechanism---one that involves
multiple shifts and transformations. On some level, language, already a
discrete and portable representation of mental states, which in themselves
stand in a complicated and fragmentary relationship to sensory phenomena,
undergoes a number of further material-phase transitions. In the writing
telegraph, this involves what I have been calling transmediation: from
notebook to paper tape, to the movement of an electromagnetic vibrator, to the
recording mirror galvanometer, into copper wire, to the receiving vibrator, and
into the machinations of the printing apparatus that once again produces ink
and alphabet. In changing material contexts, language also undergoes multiple
changes of encoding: here, from script to perforation, into vibration, to the
shape of the sinusoidal electrical impulse, and back out through a series of
transformations into script.

By "encoding," I mean a "controlled system of representation." Representation
in its unrestrained form (as in "pictorial representation") differs from
"encoding" in the size of its vocabulary. Painting and other forms of
uninhibited representational conventions are limitless in their expressive
possibility. The "language" of painting is infinite, in other words. Encoding,
by contrast, reduces the universe of possibilities to a limited set of salient
"codes," like the alphabet. These codes have a formal grammar and rules of
composition. Although the expressive potential of written language is
limitless, in some combinatorial sense of the word, language and other codes
can "break," that is, be reduced to nonsense, in a way that painting cannot. A
telegraph of the single-Morse type accepts alpha-numeric characters at first
and then translates them into a system of signals, expressed in time intervals
(between the synchronization tone and the activation of the key).

It becomes clear from the description of early telegraph encoding systems that
transcoding and transmediation relate to each other somehow. On one hand, all
changes between encoding systems necessitate material phase transition, if not
from one type of medium into another (from paper to magnetic storage, for
example), then from one specific material context into another (as when one
merely translates from French into English on the same piece of paper).
Transmediation, on the other hand, may or may not involve a change in encoding
formats, although it frequently does, as when language on paper is transformed
into the pulsations of the electrical signal in Morse code. In reading and
playing music, a musician furthermore transforms musical notation---a
controlled, paper-bound vocabulary---into sound representation, the movement of
molecules unlimited in its expressive potential.

Blueprints for the writing telegraph reveal a device that mixes discrete and
continuous modes of representation, through multiple acts of transcoding and
transmediation. Digitality, in that sense, is besides the point. The telegraph
is a device that pushes language, normally bound to air (as speech) and paper
(as a writing system), through water and metal. Even today, deep beneath the
sea surface, telegraph lines ferry messages from continent to continent. And at
each end of the telegraph cable is a transmediation device that makes media
transference possible. I use the word "media" here in the bluntest way
possible: as "material basis" or "substratum" for some observed, higher-order
phenomenon (in our case, communication).

As information was compressed and pushed through the wires, it underwent a
number of transformations. Donald Murray, the inventor of the Murray alphabet,
conceived of that transformation in terms of movement from space to time.
Unfolding something like the "metaphysics of telegraph signalling alphabets"
(his words), he describes spatial writing symbols that "appeal to the eye," and
temporal, "telephonic" signals that "appeal to the ear." Paradoxically, space
signals (like a signboard) occupy little space, but persist in time. "For
instance, a signboard may extend over 10 feet and 100 years," Murray writes.
Time signals are ephemeral by contrast: "a Morse signal in a wire may extend
over half a second and 500 miles" [@murray_setting_1905, 86].

The turn of the twentieth century represents a pivotal moment in the
development of universal Turing machines. The advent of removable storage
media---control cards and ticker tape---allowed for the automation of the
telegraph. Following the invention of Morse code in the first half of the
nineteenth century, a number of formats were proposed to encode human languages
into binary formats that rely on signal modulation for transmission. As
telegraph communication spread across national boundaries, agreements were
needed to standardize conventions for equipment and message encoding
[@international_telegraph_union_journal_1899, 82-91]. Such agreements were
handled on a regional, ad-hoc basis until 1865, with the creation of the
International Telegraph Union (ITU). The International Telegraph Conference in
Paris, held between March 1 and May 17 of 1865, adopted the use of a modified
Morse code character set, containing 33 Latin letters (including characters
from the French, German, and Spanish alphabets), 10 numbers (0--9), 14
punctuation marks (including a fraction bar), and 10 control codes (including
"end of service," "attention," and "error"). Significantly, the ITU also set
international tariffs related to telegraph communication
[@international_telegraph_union_documents_1865]. During the International
Telegraph Conference in Lisbon, held in the summer of 1908, the ITU specified
two additional formats: Hughes and Baudot.

![Table of Alphabets, 1901 [@vansize_new_1901, 23].](images/alphabets-vansize.png)

The Hughes telegraph, an 1855 design modification of an 1846 telegraph invented
by the American Royal E. House, was a capricious machine that relied on a
tuning mechanism to transmit individual characters. Inspired by the player
piano, the Hughes telegraph even looked like a piano, complete with a keyboard
and a rotating drum, commonly used in music boxes. The sending device struck a
tone which, when transmitted by electrical current, would initiate the rotation
of a similar drum in the receiving apparatus "at the pleasure of the distant
operator," in Hughes's words. The length of time between the initial
synchronization signal and the struck chord corresponded to a letter, which the
machine then printed to tape using a letter wheel. Hughes referred to the
device as a "Compound Magnetic and Vibrating Printing Instrument"
[@hughes_improvement_1856; @hayles_print_2004, 145-147; @noll_evolution_2007,
20-21], a name that already hints at the fragility of the device.

Morse code, co-developed between a number of American and British inventors in
the first half of the nineteenth century, had the advantage of an encoding
scheme that did not rely on device synchronization. The 1865 ITU rules require
a silence equal to three dots (same as one dash) to indicate the space between
two letters, and a silence equivalent to four dots (later changed to seven) to
indicate the space between words. Although Morse code can be expressed in terms
of ones and zeros (binary), it is technically a ternary code making use of at
least three elements: dots, dashes, and silences of various length.
Furthermore, note that Morse code is a "variable length" format, where
characters are encoded in sequences (of dots and dashes) that vary in length:
from one dash for "T" to six dots for "6." This property had made Morse code
ill-suited for automated telegraphy. In a long-chain of mediation between
message, telegraph operator, and machine, the human proved to pose a limiting
factor.

Writing in 1929 for the journal *American Speech*, Hervey Brackbill laments the
demise of a language associated with human-driven, Morse code telegraph
operation. "Morse telegraphy is commonly referred to as a 'game,' and the
operator 'works a wire.'" The operator tapping on a semi-automatic key "uses a
bug." Brackbill writes, "the first instruments, with their long slender levers
and springs, looked very much like a sprawling bug." They have trade names like
"Lighting Bug," "Gold Bug," and "Cootie" (for a small model)
[@brackbill_telegraphers_1929, 287-288]. Operators using "straight keys"
achieved speeds upwards of 25--30 words per minute, limited by the shortest
possible length of the smallest transmitted unit (a dot), fixed by the American
and the International Morse Code conventions to 1/24 of a second in duration.
The physical limits of the code suggest a theoretical upper limit for
transmission speed of around 49 words per minute---a rate further limited by
the operator's manual dexterity [@mcnicol_american_1913, 207;
@u.s._bureau_of_labor_statistics_displacement_1932; @halstead_genesis_1949].
Telegraph operators working a "hand sender" commonly developed partial
paralysis of wrist or arm, commonly known as a "glass arm"
[@brackbill_telegraphers_1929;
@u.s._bureau_of_labor_statistics_displacement_1932]. In his 1949 paper on "The
Genesis and Speed of Telegraph Codes," Frank Halstead notes that "the practical
upper limits of speed will also be limited by the ability of some human beings
to operate a keyboard, until such time as electrical connection be made direct
with the receiver's central nervous system" [@halstead_genesis_1949, 451].

A sender is a "ham" or a "bum" when he "falls down" or makes errors in sending.
To "put someone under the table" in sender's lingo is to transmit faster that
the receiver can transcribe. The "reader" is "burnt up" when he falls behind.
He has to "break," or interrupt the sender to ask for repetition. To "paste"
someone meant to transmit at high speeds to deliberately burn him up or put him
under the table [@brackbill_telegraphers_1929]. Around the turn of the
twentieth century, companies like Western Electric, Mecograph, and Vibroplex
began manufacturing semi-automatic keys, which made use of a horizontal switch
capable of emitting a rapid succession of dots to one side of the action, and
dashes to the other [@martin_telegraphic_1904; @boyd_telegraph-key_1916]. A bug
was said to "run away" when adjusted for too high of a speed. The Vibroplex
keys would significantly alter the “fist,” or the transmission style of the
operator, allowing for speeds that approached 50 words per minute (and above,
if not following the minimum signal length specifications)
[@mcnicol_american_1913, 209; @halstead_genesis_1949].

By the 1930s, devices variously known as "printer telegraphs,"
"teletypewriters," and "teletypes" displaced Morse code telegraphy as the
dominant mode of commercial communication. The monthly "Labor Review" report
published by the U.S. Bureau of Labor Statistics in March of 1932 estimates
more than a 50 percent drop in the number of Morse code operators between the
years of 1915 and 1931. Morse operators referred to the tele-typists on the
sending side as "punchers" and on the receiving side as "printer
men."[^ln1-printermen] The printer men responsible for assembling pages from
ticker tape were called "pasters" and sometimes, derisively, as "paperhangers"
[@brackbill_telegraphers_1929]. Where, in the Morse code machine, a
human-operator was responsible for translating language into code, teletype
automated the process completely. The human operator could simply enter printed
characters into the machine directly, using a keyboard. The teletype would then
transcode the input into transmitted signal and then back from the signal onto
paper on the receiving end.

[^ln1-printermen]: According to the U.S. Department of Labor statistics, women
comprised 24 percent of Morse operators in 1915 (before the wide-spread advent
of automated telegraphy). By 1931 women comprised 64 percent of printer and
Morse manual operators [@u.s._bureau_of_labor_statistics_displacement_1932,
514].

The automation of the telegraph necessitated new encoding formats, chief among
them the Baudot code, standardized in 1908 by the ITU. The significant
difference between Hughes and Morse encodings on the one hand and Baudot on the
other remains the fact that Baudot constitutes a fixed-length code. The
invention of fixed-length ciphers lies in the sixth book of Francis Bacon's *De
augmentis scientiarum* (*Division of the Sciences*),[^ln1-bacon] an encyclopedic
treatise on the "partition of sciences" written in the form of a letter to King
James in 1623. Book Six of Bacon's systematic account of the sciences is
dedicated to speech, speaking, writing, grammar (literary and philosophical),
poetry (meter and verse), and, most relevant to our discussion, "the knowledge
of ciphers." Here Bacon boasts of inventing a "highest degree of cipher” that
can signify "all in all" (omnia per omnia). Bacon proceeds to describe a
"fivefold," "bi-literarie” alphabet, which encodes each letter of the English
language using a string of As and Bs five-letters long. The letter A, for
example, becomes "aaaaa." B becomes "aaaab," C "aaaba," and so on to Z,
represented as "babbb." "Neither is this a small matter," Bacon writes:

> These Cypher-Characters have, and may performe: For by this *Art* a way is
opened, whereby a man may expresse and signifie the intentions of his minde, at
any distance of place, by objects which may be presented to the eye, an
accommodated to the eare: provided those objects be capable of a twofold
difference only; as by Bells, by Trumpets, by Lights and Torches, by the
report of Muskets, an any instruments of like nature [@bacon_advancement_1987,
266].

Although the discovery of binary arithmetic rightly belongs to Gottfried
Leibniz, who, influenced by hexagrams in the *I Ching*, articulated his own
system in his *Explication de l'Arithmétique Binaire* (written in 1679 and
published in 1703) [@leibnitz_explication_1703], Bacon should take the credit
for articulating a system for fixed-length binary encoding. His "two-fold
difference" did have the effect of enabling humans to "express and signify"
thoughts at virtually "any distance and place." The fixed-length property of
Bacon's cipher, later implemented in the 5-bit Baudot code, signaled the
beginning of the modern era of serial communications
[@jennings_annotated_2004]. Baudot and the related Murray alphabets
[^ln1-murray] were designed specifically with automation in mind. Both being
fixed-length alphabets, the Baudot in particular did away with the "unison"
signal that separated letters in Morse, since the signals could be divided into
letters by count (every five codes representing a single character).
Additionally, the Murray code was more compact than Morse and especially more
economical than Hughes, which used up to 54 measures of silence to send a
signal representing double quotes.[^ln1-zero] The signal for "zero" in Morse
code occupied 22 measures. By contrast, all Baudot and Murray characters were a
mere five units in length, with the maximum of ten used to switch the receiving
device into "figure" or "capital letter" states (for the total of ten units)
[@murray_setting_1905; @beauchamp_history_2001 380-397].

![Murray Keyboard Perforator, 1905. Note the QWERTY arrangement of the keys
[@murray_setting_1905].](images/murray-keyboard.png)

As the volume of transmissions increased, human operators were simply not fast
enough to keep up with the demand of encoding and decoding messages into and
from Morse code. The rise of printing telegraphs answered the demand of
increased information exchange. The great variety of printing telegraph designs
at the turn of the twentieth century all attempted to solve the problem of
automating the encoding and decoding mechanisms of transmission, with the goal
of speeding up and increasing the volume of possible communication.[^ln-cables]
Murray writes: "It is the object of machine telegraphy not only to increase the
saving of telegraph wire […] but also to reduce the labor cost of translation
and writing by the use of suitable machines" [@murray_setting_1905, 557].
Baudot's and Murray's codes were not only shorter, they were simpler and less
error-prone, resulting in less complicated and more durable devices. By the end
of the nineteenth century a number of devices were in use internationally,
making it possible to decouple the real-time encoding of text into code from
the operation of the machine. A tape containing the message could be prepared
in advance, fed into the telegraph, and printed in human-legible form on the
receiving end.

Several engineering solutions emerged to transcend the limits of human
information processing. The automated printing telegraph decoupled encoding
from transmission. It then became possible to prepare messages in advance, in
volume, and to run the messages through a machine without human assistance. At
that point, telegraph diverged from the telephone to become a device of
asynchronous communication, displacing the act of communication in time as it
did in space. But even with the advent of removable storage media, the
bandwidth, or the "information density," of the system was limited by the
number and the size of available cables. Independent developments in
communication technology led to a range of techniques known as
"multiplexing"---ways of sharing the same wire to send multiple messages.
Multiplexing at the time fell into two broad approaches: time-division and
pulse-amplitude modulation [@rowland_multiplex_1901; @hausmann_telegraph_1915].
Pulse-amplitude modulation involved filling the available space (bandwidth)
with simultaneous signals of different types: sending multiple signals at
differing frequencies that shared the same channel.[^ln1-multi] Pulse-amplitude
modulation was difficult to implement initially. Cross-channel noise and device
sensitivity hampered reliable reception and decoding across multiple
simultaneous wave frequencies.

Emile Baudot was one of the first engineers to notice that the prevailing Morse
and Hughes telegraph systems failed to make full use of the time allotted for
message transmission. The Hughes code in particular made extensive use of long
silences, which could be condensed to extract more utility from the channel.
Baudot-type multiplexers made use of synchronized rotating mechanisms both of
the sending and the receiving end, to distribute units of time among multiple
operators. Figure 6 shows a scheme for a duplex printing telegraph, for
example. A single rotation of the synchronization device (known as the
"distributor") was thereby divided into two units of five segments each, each
corresponding to a distinct channel of communication. Two operators could
therefore share the same channel to transmit two different messages.

![Multiple Printing Telegraph, 1905
[@murray_setting_1905].](images/multiplex.png)

The maintaining of unison became of paramount importance in such multiplexed
systems. The operators of a time-shared device needed to know when it was their
turn to type. The Baudot multiplexer made use of a time- or cadence- "tapper"
mechanism to indicate turns to individual operators, and in some devices,
locking the keyboard to prevent out-of-turn input. In a quadruplex system up to
four operators would engage in an intricate dance of fingers, keys, tappers,
and signals, synchronized by the rotation of the telegraph distributor.

Synchronization was also needed for Morse code devices, to distinguish
non-meaning carrying silence (the receiver is turned off) from the
meaning-carrying one (the receiver pauses to indicate a dot). The receiving
device needed a measure (duration) of silence to differentiate between "dots"
and spaces between words, both indicated by silences of different length. But
if the devices went out of sync, or if communication lagged for some reason,
the coherence of the message faltered. Sending and receiving machines had to be
tuned to a cadence of common information exchange. The system of operators,
transmitters, and receivers, was, in aggregate, tuned to a specific but
arbitrary rhythm by which certain messages made sense only in particular (also
arbitrary) units of time. In early telegraphy, these units of time were slower
than natural human time, enough so for the operator to remain idle. Later
systems increased the pace to a rhythm beyond natural human abilities of
comprehension, to a point where human operators could no longer decipher the
signal without machine assistance. In the language of wiremen, the bugs were
"running away with" the whole "game" [@brackbill_telegraphers_1929, 288].

[^ln1-siegert]: Siegert takes the Virag as an "apocryphal emblem" of a
"systematic logic in the dominant cultural technique," representative of the
"order of digital signals" [@siegert_cacography_2011, 41]. Although not
concerned with the history or philosophy of digital media explicitly, Siegert
suggests that the writing telegraph symbolizes the cleansing of "the noise of
all graphic form." His theory of cultural techniques instead creates "an
awareness for the plenitude of a world of as yet undistinguished things, that,
as an inexhaustible reservoir of possibilities, remain the basic point of
reference for every type of culture" [@siegert_cacography_2011, 35].

[^ln1-virag]: The Pollak-Virag device also proposed an "electromagnetic
vibrator" coupled to a recording "mirror-galvanometer." The authors further
specify for "the vibrator on the receiving instrument [to] vibrate in harmony
with these impulses, vibrating in one direction or the other, according to
their sign and to an extent depending on their intensity" [@pollak_means_1900].

[^ln1-dervish]: One could make more of the Dervish being used here as a
negative example, in an instance of colonial gaze. But Murray himself was an
empire outsider, born in the small, remote town of Invercargill, New Zealand (a
town that also happens to be one of the southern-most cities in world). "I am a
child of the Southern Cross," he writes in *Philosophy of Power*, "and I have
no preordained respect for geniuses born under the Northern constellations" [@murray_philosophy_1939, 51].

[^ln1-swedenborg]: See @swedenborg_treatise_1778, regarding the "gross error of
those who assert the materiality of the soul, affirming it to be homogeneous,
and continuous with the body; whereas it is heterogeneous, and discrete" (24).
Swedenborg gives the neoplatonic, monistic articulation of the matter to the
angels, in the "Angelic Idea of Creation," writing "The question was asked,
'Whence, then, is hell?' They said, 'From man's freedom, without which a man
would not be a man,' because man, from that freedom, broke the continuity in
himself; and this being broken, separation took place; and the continuity,
which was in him from creation, became like a chain, or a piece of linked
work, which falls to pieces when the hooks above are broken or torn out, and
then hangs down from small threads. Separation or rupture was brought about,
and is brought about, by the denial of God" [@swedenborg_apocalypse_1901, 290].

[^ln1-golumbia]: See @golumbia_cultural_2009: "Following a line of criticism
that extends at least as farm back as Kant [...] (loc. 63)." Paraphrasing
@deleuze_postscript_1992, Golumbia writes "To submit a phenomenon to computation is to striate
otherwise-smooth details, among details, to push upwards towards the sovereign,
to make only high-level control available to the user and then only those
aspects of control that are deemed appropriate to the sovereign (loc. 159)."

### 2.5 What Is It Like to Be a Telegraph

I am tempted to believe, like others have before me, that I am on my way to
enacting something like "machine phenomenology" or "systems phenomenology" in a
system which encompasses people, texts, techniques, and technologies. In such
cases, astute readers often reach for Jakob von Uexküll's *A Foray into the
Worlds of Animals and Humans*, to borrow from his notion of perceptual worlds
(umwelten).[^ln1-umwelten] In his forays, Uexküll imagines a multitude of alien
ways of seeing the world, from that of a sea urchin to those of a pea weevil
and the ichneumon wasp. For Uexküll, the task of the biologist is not to
encounter such living things as unfamiliar "machines" in the human world, but,
rather, to occupy the alien life form as another subject. The tick should not
be looked at as yet another machine, Uexküll writes, but as an "operator"
[@uexkull_foray_2010, 44-53]. Consequent to this point of view is Uexküll's
insistence on the subjectivity of perceptual experience. The tick carries with
it its own sense of time and space, which the biologist recovers through the
description of its perceptual apparatus (What can this organism perceive?) and
therefore by reconstructing its projected world view (What does a stimulus mean
for this organism?). In this way, the subject establishes what Uexküll calls a
"circuit of meaning." The canopy of an oak tree, for example, acts in unison
with material properties of rain to capture and distribute liquid down to the
roots of the tree. Capturing and distributing liquid to the roots constitutes,
in that sense, the "meaning" of the oak--rain circuit. It is a peculiar notion
of "meaning" to be sure, but for Uexküll it is enough to maintain that meaning
arises out of the subject's interaction with its environment. The subject and
object are brought into what Uexküll calls harmony:

> If the flower were not bee-like  
> If the bee were not flower-like  
> The harmony would never succeed [@uexkull_foray_2010, 198].

Similarly, the pea and the pea weevil achieve harmony through "meaning
transfer" between the pea and the weevil larvae, which allows the larvae to
prepare an escape hatch through vegetative matter in advance of its pupation
[@uexkull_foray_2010, 161].

[^ln1-umwelten]: See for example @agamben_open_2003, 39-49; @hayles_print_2004,
16-17.

The literature of post-humanism privileges these moments of alien subjectivity
because they destabilize an anthropocentric view of the world. What gets lost
in the shifting of perspectives is the apparent contradiction at the very core
of what the writer Dorion Sagan (rightly) calls "Gaia sciences"
[@uexkull_foray_2010, 12], the idea ascendant in diverse fields from literary
studies to information theory and free-market economics, relegating meaning
creation from the level of the subject to the level of complex system.
Paradoxically, because Uexküll's model of communication makes meaning available
only at the system level ("oak plus rain" or "bee plus flower"), it is never
available to the subject itself (bees or flowers). The quest for alien
semiotics leads only to, in Uexküll's own words, "mute interaction"
[@uexkull_foray_2010, 148], meaningful only from some vantage point outside of
the system. The forest always "understands" more than the tree, the planet more
than the forest, and so on, in a regression that can only conclude with
whole-universe monism: the One.

In his essay on the subjective experience of bats, Thomas Nagel concludes that
there must be something about the experience of a bat *for* a bat
[@nagel_what_1974, 439] that is not fully accessible to human description. The
attempt to describe things in-themselves for-themselves, in that light, seems
like yet further human hubris. Accepting the meta-human "systems" view of
meaning formation would mean also to acknowledge the limits of the human to
comment on the proceedings---lest the organ speak for the organism. And to
encounter the bat (a dog, a tree, a machine) from the perspective of the human
is to respect and to retain a measure of the other's ineffable alienness. There
is a quiet humanism in Nagel's suggestion that we should confine our remarks to
objective phenomenology, not dependent on the self-deceiving acts of the
imagination [@nagel_what_1974, 449]. After all, one can never truly become a
bat or a tick.  We can only imagine what it is like to be a bat *for* a human.
"Certainly it appears unlikely," Nagel writes, "that we will get closer to the
real nature of human experience by leaving behind the particularity of our
human point of view and striving for a description in terms accessible to
beings that could not imagine what it was like to be us" [@nagel_what_1974,
444]. Nagel suggests: "Though presumably it would not capture
everything, its goal would be to describe, at least in part, the subjective
character of experiences in a form comprehensible to beings incapable of having
those experiences" [@nagel_what_1974, 449].

I do not know what a truly objective phenomenology would look like, but I would
like to think that it would lie, at least in part, in the underlying material
conditions of perceptual media. I am able to advance an equitable description
of the system from the only perspective available to me. These blueprints and
diagrams are all we have to understand the ways in which the system affects its
constituents. Paying attention to the details of the schematics is the only way
to "dig" ourselves into something like an alien phenomenology. As the
humanities move in search of the digital, I find that they need to regain a
sense of the human. No perspective is available otherwise from which to enact
analysis or critique. Once recovered, that Archimedean point of leverage can
serve as a starting point for the work of de-naturalizing comfortable (if
calcified) notions of the digital. It is in this de-familiarization (in a sense
used by Viktor Shklovsky and Svetlana Boym) of the human that I see Uexküll's
contribution to the study of media phenomenology. The human disappears from
view not by some inevitable property of technological progress, but through
automation of experience. In refracting ourselves through the gaze of the
technological other we see the human world anew.

From this perspective, electricity---the "language" of wires---is not a media
format intended for human consumption. One could learn to understand telegraph
intercourse through electrocution, by applying the wire directly to the skin.
Direct brain--computer interfaces promise to do more or less exactly that and
are common enough today to be turned into a toy.[^ln1-mindflex] The early
efforts in this space implanted electrodes into the brain directly to capture
single-neuron activity, or relied on imprecise "noisy" electroencephalographic
(EEG) scalp activity. In 2004 a team of scientists developed a way of
controlling "a one-dimensional computer cursor rapidly and accurately" using
electrocorticographic (ECoG) activity recorded from the surface of the brain
[@leuthardt_braincomputer_2004; @miller_spectral_2007]. And in 2015, a
quadriplegic woman piloted an F-35 Joint Strike Fighter using her brain in a
simulation developed by the University of Pittsburgh's Human Engineering
Research Laboratories in collaboration with the Defense Advanced Research
Projects Agency (DARPA) [@collinger_collaborative_2014; @prabhakar_how_2015].
Advances in brain--computer interfaces suggest also the not-all-too-distant
possibility of computer--brain interfaces (in a reversal of the direction),
completing the loop between human and machine. This may also imply the
possibility of communicating brain-states directly, without the mediation of
text or sound. However futuristic that possibility may seem to us today, it is
merely a small evolutionary step in a long history of brain-to-brain
interfaces, already mediated by paper, ink, code, and silicon. The digital
world of paper and ink holds no less magic.

I return, then, to the subject of time, found throughout in the background of
this chapter. The popular imagination of digital machines belies a deep sense
of anxiety about the rhythm of the system as a whole, in which humans play only
a small part (as bug operators, users, or receivers). Telegraphs and
subsequently computers force an atomization of experience into discrete bits,
along with the opposite movement toward continuity fluid and rapid enough as to
move beyond the boundaries of human perception. Attention to the material
conditions of removable storage media reveals a delicate negotiation of
synchronicity between human and machine time, in a complex chain of encoding
and re-mediation that connects differing and incongruent perceptual apparatus.
Under these conditions, it would be a mistake to conflate digital and discrete,
just as it would be a mistake to reduce the human to the continuous.[^ln1-cont]
Homeostasis is found at some arbitrary point, contingently connected to human
biology and to current technological capability. The question of "What
constitutes a moment?" when watching television or reading a book, can become
then, "What *should* constitute a moment?" For the tick lying in wait for its
next warm meal, a moment can last for decades (Uexküll reports up to 18
years). "Time, which frames all events," Uexküll writes, "seemed to us to be
the only objectively consistent factor […], but now we see that the subject
controls the time of its environment" [@uexkull_foray_2010, 52]. The
computational environment before us constitutes the grounds for all
higher-level textual activity, from record keeping to poetry and software
development. From the textual grounds of human--machine interaction it then
becomes possible to derive poetics, in the sense of a study and an art of
creating one's own time: taking control of prosody, rhythm, meter, and cadence.

[^ln1-ndc]: Also known as the "single current" or "single Morse" system.

[^ln1-bacon]: This volume is also commonly translated as "Of the Dignity and
Advancement of Learning," following the Spedding edition. The first two books
of *The Advancement* appeared first in 1605. Together with books 6-9 published
in Latin in 1623 they are sometimes referred to as "Of the Advancement and
Proficience of Learning or the Partitions of Sciences," following the Oxford
1640 edition. I will follow the 1640 English edition here. Volume 10, in the new
Oxford Collected Works, containing *The Advancement*, is not out at the time of
my writing this chapter (2015).

[^ln1-murray]: The Australian Donald Murray improved on the Baudot system to
minimize the amount of "designed to punch as few holes as possible," allotting
fewer perforations to common English letters (@murray_setting_1905, 567).

[^ln1-zero]: Twenty-eight measures to indicate the numerical "figure space" and
26 to indicate double quotes (which shared the encoding length with the letter
"z").

[^ln1-current]: ITA-2 could also be adopted to work with "double current"
devices, in which case 0 would represent "negative current" and 1 "positive
current" [@itu_telegraph_1932, 36].

[^ln1-kittler]: This along with the ominous "laying of cables" that concludes
Friedrich Kittler's *Gramophone, Film, Typewriter*.

[^ln1-multi]: Technical literature makes a distinction between space- and
frequency- division multiplexing. On some level, space-division multiplexing
simply involves the splitting of a signal into multiple physical channels
(wires). Frequency-division better "fills" the space of a single channel.

[^ln1-tele]: See for example @angell_pro_2009, 233:  "The telegraph is a
digital device sending only high and low pulses through the wire;"
@jepsen_my_2001, 195: "The telegraph was a digital device that used dots and
dashes in a manner similar to the ones and zeroes of digital logic;"
@comer_internet_2006, 32: "The telegraph is a digital device because instead of
sending a continuous signal that is an exact analog of the input, the telegraph
clicks to send the individual characters."

[^ln1-mindflex]: The American toy giant Mattel makes a game called "Mindflex."
The Frequently Asked Questions page includes the following prompt: "Have you
ever dreamed of moving an object with the power of your mind? Mindflex Duel™
makes that dream a reality! Utilizing advanced Mindflex Duel™ technology, the
wireless headset reads your brainwave activity. Concentrate...and the ball
rises on a cushion of air! Relax...and the ball descends. It's literally mind
over matter!" (@mindflex_mindflex:_2015)

[^ln1-cont]: Gregory Hickok, a prominent cognitive scientists working out of
University of California, Irvine writes: "The brain samples the world in
rhythmic pulses, perhaps even discrete time chunks, much like the individual
frames of a movie. From the brain's perspective, experience is not continuous
but quantized [...] This is not to say that the brain dances to its own beat,
dragging perception along for the ride. In fact, it seems to work the other way
around: Rhythms in the environment, such as those in music or speech, can draw
neural oscillations into their tempo, effectively synchronizing the brain's
rhythms with those of the world around us" [@hickok_its_2015]. His study on the
topic is forthcoming in *Psychological Science* in 2015.


